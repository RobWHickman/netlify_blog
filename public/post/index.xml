<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Robert Hickman</title>
    <link>/post/</link>
    <description>Recent content in Posts on Robert Hickman</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Scraping Dynamic Websites with PhantomJS</title>
      <link>/post/dynamic_web_scraping/</link>
      <pubDate>Sat, 06 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/dynamic_web_scraping/</guid>
      <description>


&lt;p&gt;For &lt;a href=&#34;http://www.robert-hickman.eu/post/guardian_knowledge_june&#34;&gt;a recent blogpost&lt;/a&gt;, I required data on the ELO ratings of national football teams over time. Such a list exists online at &lt;a href=&#34;https://eloratings.net/&#34;&gt;eloratings.net&lt;/a&gt; and so in theory this was just a simple task for &lt;a href=&#34;https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/&#34;&gt;rvest&lt;/a&gt; to read the html pages on that site and then fish out the data I wanted. However, while this works for the static websites which make up the vast majority of sites containing tables of data, it struggles with websites that use JavaScript to dynamically generate pages.&lt;/p&gt;
&lt;p&gt;Eloratings.net is one such website which rvest is unable to scrape. E.g.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(rvest)

# url to data on Brazil&amp;#39;s ELO rating over time
url &amp;lt;- &amp;quot;https://eloratings.net/Brazil&amp;quot;

read &amp;lt;- read_html(url) %&amp;gt;%
  # this is the CSS selector for the page title
  html_nodes(&amp;quot;#mainheader&amp;quot;)

read&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_nodeset (1)}
## [1] &amp;lt;h1 id=&amp;quot;mainheader&amp;quot; class=&amp;quot;mainheader&amp;quot;&amp;gt;&amp;lt;/h1&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;does not manage to capture the data displayed in the page mainheader (it ‘should’ return “World Football Elo Ratings: Brazil” from the title of that page).&lt;/p&gt;
&lt;p&gt;Instead, what we want to do is save a copy of the generated page as a .html file and then read that into R using read_html(). Luckily, a way exists to do just that, using the (now deprecated, but still working) &lt;a href=&#34;http://phantomjs.org/&#34;&gt;PhantomJS headless browser&lt;/a&gt;. Much of the code I used to get going with this is adapted from a tutorial &lt;a href=&#34;https://velaco.github.io/how-to-scrape-data-from-javascript-websites-with-R/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First you want to install PhantomJS from the above website and run through it’s &lt;a href=&#34;http://phantomjs.org/quick-start.html&#34;&gt;quick start guide&lt;/a&gt;. This is a pretty thorough guide, I would say that there are really only three steps from installation to getting going:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://www.howtogeek.com/118594/how-to-edit-your-system-path-for-easy-command-line-access/&#34;&gt;Add phantomjs to the system PATH&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Open a text editor and save one of the &lt;a href=&#34;https://phantomjs.org/quick-start.html&#34;&gt;tutorial scripts&lt;/a&gt; as filename.js&lt;/li&gt;
&lt;li&gt;run &amp;gt; phantomjs C:/Users/usr/path/to/file.js
in a command line console&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The file we’re going to use to render the js pages and then save the html is below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;// scrapes a given url (for eloratings.net)

// create a webpage object
var page = require(&amp;#39;webpage&amp;#39;).create(),
  system = require(&amp;#39;system&amp;#39;)

// the url for each country provided as an argument
country= system.args[1];

// include the File System module for writing to files
var fs = require(&amp;#39;fs&amp;#39;);

// specify source and path to output file
// we&amp;#39;ll just overwirte iteratively to a page in the same directory
var path = &amp;#39;elopage.html&amp;#39;

page.open(country, function (status) {
  var content = page.content;
  fs.write(path,content,&amp;#39;w&amp;#39;)
  phantom.exit();
});&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(which, again, is stolen and adapted from &lt;a href=&#34;https://velaco.github.io/how-to-scrape-data-from-javascript-websites-with-R/&#34;&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;This is saved as scrape_ELO.js in the static directory of my blog folder.&lt;/p&gt;
&lt;p&gt;To keep everything in R, we can use the system() family of functions, which provides access to the OS command line. Though the referenced tutorial uses system(), it relies on scraping a single referenced page. To iteratively scrape every country, we’ll need to provide an argument (country) which will contain the link to the page on eloratings.net for that country.&lt;/p&gt;
&lt;p&gt;E.g. for Brazil we will provide “&lt;a href=&#34;https://www.eloratings.net/Brazil&#34; class=&#34;uri&#34;&gt;https://www.eloratings.net/Brazil&lt;/a&gt;” as the country argument&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;phantom_dir &amp;lt;- &amp;quot;C:/Users/path/to/scrape_ELO/&amp;quot;
country_url &amp;lt;- &amp;quot;https://www.eloratings.net/Brazil&amp;quot;

# use system2 to invoke phantomjs via it&amp;#39;s executable
system2(&amp;quot;C:/Users/path/to/phantomjs-2.1.1-windows/bin/phantomjs.exe&amp;quot;,
        #provide the path to the scraping script and the country url as argument
        args = c(file.path(phantom_dir, &amp;quot;scrape_ELO.js&amp;quot;), country_url))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then read in this saved html page using rvest as per usual and recover the information therein.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# read in the saved html file
page &amp;lt;- read_html(&amp;quot;elopage.html&amp;quot;)

# scrape with rvest as normal
country_name &amp;lt;- page %&amp;gt;%
  html_nodes(&amp;quot;#mainheader&amp;quot;) %&amp;gt;%
  html_text() %&amp;gt;%
  gsub(&amp;quot;Elo Ratings: &amp;quot;, &amp;quot;&amp;quot;, .)

country_name&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Brazil&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m not going to include my full script for scraping eloratings.net as usually a reason for doing this obscuring of the data is to prevent exactly what I’m doing. Instead I’ll give a skeleton function of the one I use. If you are having problems with setting up phantomjs to scrape pages, my contact details are listed on my &lt;a href=&#34;http://www.robert-hickman.eu/&#34;&gt;blog homepage&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scrape_nation &amp;lt;- function(country) {
  # download the page
  url &amp;lt;- paste0(&amp;quot;https://eloratings.net/&amp;quot;, country)
  system2(&amp;quot;C:/Users/path/to/phantomjs-2.1.1-windows/bin/phantomjs.exe&amp;quot;, 
          args = c(file.path(phantom_dir, &amp;quot;scrape_ELO.js&amp;quot;), url))
  
  # read in downloaded page
  page &amp;lt;- read_html(&amp;quot;elopage.html&amp;quot;)
  
  # recover information
  country_name &amp;lt;- page %&amp;gt;%
    html_nodes(&amp;quot;#mainheader&amp;quot;) %&amp;gt;%
    html_text() %&amp;gt;%
    gsub(&amp;quot;Elo Ratings: &amp;quot;, &amp;quot;&amp;quot;, .)
  
  opposing &amp;lt;- page %&amp;gt;%
      html_nodes(&amp;quot;.r1 a&amp;quot;) %&amp;gt;%
      html_text()
  
  teams &amp;lt;- page %&amp;gt;%
      html_nodes(&amp;quot;.r1&amp;quot;)
  
  fixtures &amp;lt;- map2_df(teams, opposing, split_teams)

  ratings &amp;lt;- page %&amp;gt;%
    html_nodes(&amp;quot;.r4&amp;quot;) %&amp;gt;%
    html_text() %&amp;gt;%
    map_df(., split_ratings)
  
  rankings &amp;lt;- page %&amp;gt;%
    html_nodes(&amp;quot;.r6&amp;quot;) %&amp;gt;%
    map_df(., split_rankings)

  dates &amp;lt;- page %&amp;gt;%
    html_nodes(&amp;quot;.r0&amp;quot;) %&amp;gt;%
    html_text() %&amp;gt;%
    map_df(., convert_date)

  # bind into a data frame
  df &amp;lt;- fixtures %&amp;gt;%
    cbind(., ratings) %&amp;gt;%
    cbind(., rankings) %&amp;gt;%
    cbind(., dates) %&amp;gt;%
    mutate(table_country = country_name)
}

elO_data &amp;lt;- map_df(country_links, scrape_nation)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we want to convert this to long format. We have two observations per country and any point in time- the rating, and the ranking. For &lt;a href=&#34;http://www.robert-hickman.eu/post/guardian_knowledge_june&#34;&gt;the blogpost I needed the data for&lt;/a&gt; I took just the ranking data in the end. Here, I’m going to do the opposite and take only the rating data to make a nice little plot of national teams ratings over time&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elo_data %&amp;lt;&amp;gt;%
  mutate(date = as.Date(date)) %&amp;gt;%
  # rename and select variables
  select(
    date,
    home, away,
    rating_home = r1, rating_away = r2,
    ranking_home = ranking1, ranking_away = ranking2
  ) %&amp;gt;%
  # melt twice to convert to long format
  gather(
    &amp;quot;location&amp;quot;, &amp;quot;nation&amp;quot;,
    -rating_home, -rating_away, -ranking_home, -ranking_away, -date
  ) %&amp;gt;%
  gather(&amp;quot;measure&amp;quot;, &amp;quot;value&amp;quot;, -nation, -date, -location) %&amp;gt;%
  # take only relevant information
  filter(
    (location == &amp;quot;home&amp;quot; &amp;amp; measure %in% c(&amp;quot;rating_home&amp;quot;, &amp;quot;ranking_home&amp;quot;)) |
      (location == &amp;quot;away&amp;quot; &amp;amp; measure %in% c(&amp;quot;rating_away&amp;quot;, &amp;quot;ranking_away&amp;quot;))
  ) %&amp;gt;%
  separate(measure, into = c(&amp;quot;measure&amp;quot;, &amp;quot;location&amp;quot;), &amp;quot;_&amp;quot;) %&amp;gt;%
  # filter out relevant data
  filter(!duplicated(.)) %&amp;gt;%
  filter(date &amp;gt; &amp;quot;1950-01-01&amp;quot;) %&amp;gt;%
  filter(measure == &amp;quot;rating&amp;quot;) %&amp;gt;%
  select(-measure, rating = value, -location)

# print the df
head(elo_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         date nation rating
## 1 1950-05-06 Brazil   1957
## 2 1950-05-07 Brazil   1969
## 3 1950-05-13 Brazil   1961
## 4 1950-05-14 Brazil   1965
## 5 1950-05-18 Brazil   1969
## 6 1950-06-24 Brazil   1991&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To cap off this little post, I decided to use &lt;a href=&#34;https://github.com/thomasp85/gganimate&#34;&gt;gganimate&lt;/a&gt; to show how the ratings of some nations have changed over time. It’s a nice little sanity test that we’ve scraped data correctly, but also, as a football nerd, I enjoy seeing how nations have risen and fallen over the years&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gganimate)

p &amp;lt;- elo_data %&amp;gt;%
  # select out a few nations
  filter(nation %in% c(
    &amp;quot;Brazil&amp;quot;,
    &amp;quot;England&amp;quot;,
    &amp;quot;Canada&amp;quot;,
    &amp;quot;Hungary&amp;quot;,
    &amp;quot;Nigeria&amp;quot;,
    &amp;quot;Japan&amp;quot;
  )) %&amp;gt;%
  # going to take the average over every 4 months
  # could use zoo::rollmean but also want to cut down plotting
  mutate(month = as.numeric(format(date, &amp;quot;%m&amp;quot;)),
         year = as.numeric(format(date, &amp;quot;%Y&amp;quot;))) %&amp;gt;%
  mutate(third = case_when(
    month &amp;lt; 5 ~ 0,
    month &amp;lt; 9 ~ 33,
    TRUE ~ 66
  )) %&amp;gt;%
  mutate(year = as.numeric(paste0(year, &amp;quot;.&amp;quot;, third))) %&amp;gt;%
  group_by(nation, year) %&amp;gt;%
  summarise(rating_av = mean(rating)) %&amp;gt;%
  ungroup() %&amp;gt;%
  # pipe into ggplot
  ggplot(aes(x = year, y = rating_av, group = nation)) +
  # coloured line per nations
  geom_line(size = 1.5, aes(colour = nation)) +
  scale_colour_manual(values = c(&amp;quot;goldenrod&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;grey60&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;darkblue&amp;quot;, &amp;quot;forestgreen&amp;quot;)) +
  labs(title = &amp;quot;ELO Rating of Selected Nations over Time&amp;quot;,
       subtitle = &amp;quot;date from eloratings.net&amp;quot;,
       x = &amp;quot;year&amp;quot;,
       y = &amp;quot;ELO rating&amp;quot;) +
  theme_minimal() +
  theme(legend.position=&amp;quot;bottom&amp;quot;) +
  # gganimate reveal
  transition_reveal(year)

# save the gif
gif &amp;lt;- animate(p, nframes = 200)
anim_save(&amp;quot;../../static/files/nation_animation.gif&amp;quot;, gif)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which if we render gives us&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../static/files/nation_animation.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And the data looks good! The &lt;a href=&#34;https://en.wikipedia.org/wiki/Golden_Team&#34;&gt;Mighty Magyar&lt;/a&gt; Hungary team of the 1950s can be seen to peak before the nations long decline, whereas the opposite is true for Japan. Overall, I’m pretty happy with the result. It could surely be cleaned up using rolling means and more careful plotting, but for a small example to plot the output from the scraping (the real point for this post) it serves a purpose.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Guardian Knowledge June 2019</title>
      <link>/post/guardian_knowledge_june/</link>
      <pubDate>Sat, 06 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/guardian_knowledge_june/</guid>
      <description>


&lt;p&gt;Most Wednesday’s I enjoy reading &lt;a href=&#34;https://www.theguardian.com/football/series/theknowledge&#34;&gt;The Knowledge&lt;/a&gt; blog on the Guardian’s website and reading the football trivia therein. When time (and questions) allow, I like to answer some of the questions posed, example of which are &lt;a href=&#34;http://www.robert-hickman.eu/post/the-knowledge-4th-august-2018/&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://www.robert-hickman.eu/post/counties_league_points/&#34;&gt;here&lt;/a&gt;, and &lt;a href=&#34;http://www.robert-hickman.eu/post/the-knowledge-7th-february-2019/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;league-of-nations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;League of Nations&lt;/h1&gt;
&lt;p&gt;The first question comes from&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Which player had the nationality with the lowest FIFA World Ranking at the time of him winning the Premier League?
&lt;/p&gt;
— The Tin Boonie (&lt;span class=&#34;citation&#34;&gt;@TheTinBoonie&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/TheTinBoonie/status/1140936272862691328?ref_src=twsrc%5Etfw&#34;&gt;June 18, 2019&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;a similar question is also answered in this weeks column:&lt;/p&gt;
&lt;p&gt;‘&lt;em&gt;“Fulham defender Zesh Rehman made his debut for Pakistan, who are ranked 168 by Fifa. Is that the lowest-ranked country a Premier League player has played for?”&lt;/em&gt; wondered Zulfiqar Shah in January 2006.’&lt;/p&gt;
&lt;p&gt;I thought I’d answer both of these using R.&lt;/p&gt;
&lt;p&gt;First some libraries we’ll need, and also set a seed for reproducibility.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(magrittr)
library(rvest)

set.seed(3459)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first thing we need to answer these is the nationality of EPL players. For this two good sources are &lt;a href=&#34;https://www.transfermarkt.co.uk/&#34;&gt;transfermarkt.co.uk&lt;/a&gt; and &lt;a href=&#34;https://www.11v11.com/&#34;&gt;11v11.com&lt;/a&gt;. I’m going to opt for the later, just because the tables are a little easier to scrape.&lt;/p&gt;
&lt;p&gt;To start, we need to get the links to every team to have competed in the premier league&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get years of EPL seasons
years &amp;lt;- 1993:2019
# base url we&amp;#39;ll scrape from
base_url &amp;lt;- &amp;quot;https://www.11v11.com&amp;quot;

# cat together
tables &amp;lt;- paste0(base_url, &amp;quot;/league-tables/premier-league/01-june-&amp;quot;, years)

competing_teams &amp;lt;- tables %&amp;gt;%
  # get a list of the links to every teams squad page
  map(., function(x) {
    x %&amp;gt;%
      read_html() %&amp;gt;%
      html_nodes(&amp;quot;#table-league &amp;gt; tbody:nth-child(2) &amp;gt; tr &amp;gt; td:nth-child(2) &amp;gt; a:nth-child(1)&amp;quot;) %&amp;gt;%
      html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
      # paste into working link for year and competition (EPL)
      paste0(base_url, ., &amp;quot;tab/players/season/&amp;quot;, gsub(&amp;quot;.*01-june-&amp;quot;, &amp;quot;&amp;quot;, x), &amp;quot;/comp/1/&amp;quot;)
  }) %&amp;gt;%
  unlist()

head(competing_teams, n = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then scrape the squads for these teams in that specific season. We want the players, their nationality and also the number of appearances they made in the league that season&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;squads &amp;lt;- competing_teams %&amp;gt;%
  # get the players/appearances/nationalities
  map_df(., function(y) {
    # read once to save server calls
    read &amp;lt;- y %&amp;gt;%
      read_html() 
    
    # get the squad info
    squad &amp;lt;- read %&amp;gt;%
      html_nodes(&amp;quot;.squad&amp;quot;) %&amp;gt;%
      html_table(fill = TRUE) %&amp;gt;%
      as.data.frame() %&amp;gt;%
      # get rid of rows without player info
      filter(!is.na(Player))
    
    # get the listed nationalities
    flags &amp;lt;- read %&amp;gt;%
      html_nodes(&amp;quot;.squad &amp;gt; tbody:nth-child(2) &amp;gt; tr &amp;gt; td:nth-child(3)&amp;quot;)
    
    # from here get the actual nationalities per player
    nations &amp;lt;- flags %&amp;gt;%
      html_nodes(&amp;quot;img&amp;quot;) %&amp;gt;%
      html_attr(&amp;quot;title&amp;quot;)
    
    # these might mismatch in length
    # in which case append NA
    if(length(flags) != length(nations)) {
      missing &amp;lt;- which(!grepl(&amp;quot;img&amp;quot;, flags))
      
      nations &amp;lt;- c(
        nations[1:(missing-1)],
        NA,
        nations[missing:length(nations)]
      )
    }
    
    # mutate nationality and team and season
    squad %&amp;gt;%
      mutate(
        nation = nations,
        year = gsub(&amp;quot;.*season\\/&amp;quot;, &amp;quot;&amp;quot;, gsub(&amp;quot;\\/comp.*&amp;quot;, &amp;quot;&amp;quot;, y)),
        team = gsub(&amp;quot;\\/tab\\/players.*&amp;quot;, &amp;quot;&amp;quot;, gsub(&amp;quot;.*teams\\/&amp;quot;, &amp;quot;&amp;quot;, y))
      ) %&amp;gt;%
      # select useful appearance information
      select(player = Player, position = Position,
             appearances = A, sub_appearances = S, 
             nation, year, team)
  }) %&amp;gt;%
  # manually add in some missing nationalities
  mutate(nation = case_when(
    grepl(&amp;quot;Steffen Karl&amp;quot;, player) ~ &amp;quot;Germany&amp;quot;,
    grepl(&amp;quot;Marc Muniesa&amp;quot;, player) ~ &amp;quot;Spain&amp;quot;,
    grepl(&amp;quot;Oriol Romeu&amp;quot;, player) ~ &amp;quot;Spain&amp;quot;,
    grepl(&amp;quot;Aleix García&amp;quot;, player) ~ &amp;quot;Spain&amp;quot;,
    grepl(&amp;quot;Martín Montoya&amp;quot;, player) ~ &amp;quot;Spain&amp;quot;,
    TRUE ~ nation
  ))

head(squads, n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To answer the question as asked, we’d want historical &lt;a href=&#34;https://www.fifa.com/fifa-world-ranking/ranking-table/men/&#34;&gt;FIFA ranking&lt;/a&gt; data. While it does exist going back to 2007, I’d prefer to have the full data set back to 1993, and in any case, there are also &lt;a href=&#34;https://en.wikipedia.org/wiki/World_Football_Elo_Ratings&#34;&gt;some problems&lt;/a&gt; with the historical calculation FIFA used for it’s ratings.&lt;/p&gt;
&lt;p&gt;Instead, we can use the ELO method of rating teams (most commonly used to rank chess players). There are two ways to do this- we can calculate the ratings ourselves using a dataframe of international results, or we can take the accepted ratings at &lt;a href=&#34;https://www.eloratings.net&#34;&gt;eloratings.net&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ll outline the first method here and then use the data from the second further below &lt;a href=&#34;&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;calculating-elo-ratings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Calculating ELO ratings&lt;/h2&gt;
&lt;p&gt;To calculate our ratings, first we need to to load up a dataframe of international football results. The one I’m using comes from &lt;a href=&#34;https://www.kaggle.com/martj42/international-football-results-from-1872-to-2017&#34;&gt;kaggle&lt;/a&gt; and has 40k matches listed since the start of international football in 1872:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# https://www.kaggle.com/martj42/international-football-results-from-1872-to-2017
international_results &amp;lt;- readRDS(&amp;quot;../../static/files/international_results.rds&amp;quot;)

head(international_results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         date home_team away_team home_score away_score tournament    city
## 1 1872-11-30  Scotland   England          0          0   Friendly Glasgow
## 2 1873-03-08   England  Scotland          4          2   Friendly  London
## 3 1874-03-07  Scotland   England          2          1   Friendly Glasgow
## 4 1875-03-06   England  Scotland          2          2   Friendly  London
## 5 1876-03-04  Scotland   England          3          0   Friendly Glasgow
## 6 1876-03-25  Scotland     Wales          4          0   Friendly Glasgow
##    country neutral
## 1 Scotland   FALSE
## 2  England   FALSE
## 3 Scotland   FALSE
## 4  England   FALSE
## 5 Scotland   FALSE
## 6 Scotland   FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We only need a few of these variables- enough to know who wins each match and when/where it was played. We can then use this data to initialise several parameters to be used in our ELO calculation. For team i (in a match of teams i and j), this is &lt;a href=&#34;https://eloratings.net/about&#34;&gt;calculated as&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ rating_{i_{t}} = rating_{i_{t-1}} + K \cdot G \cdot (R - E(R)) \]&lt;/span&gt;
The rating of team i is their old rating plus the difference between the actual result (R = 1 for a win, 0.5 for a draw, 0 for a loss) and the expected result (where 1 means certain win for team i).&lt;/p&gt;
&lt;p&gt;The unexpectedness of the result is then multiplied by two parameters. The first K, is to account for the importance of the match, with more important matches having a higher K factor, and a greater influence of team rating.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
K = 
\begin{cases} 
60 &amp;amp; \text{if World Cup Final} \\
50 &amp;amp; \text{if World Cup/ Major Intercontinental Matches} \\
40 &amp;amp; \text{if World Cup/Continental Competition Qualifiers} \\
30 &amp;amp; \text{if Other Tournaments}\\
20 &amp;amp; \text{if Friendly} \\
\end{cases}

\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The second parameter, G is controlled by the strength of the result&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
G = 
\begin{cases} 
1 &amp;amp; \text{if } N &amp;lt; 2 \\
1.5 &amp;amp; \text{if } N = 2 \\
1.75 &amp;amp; \text{if } N = 3 \\
1.75 + \frac{N-3}{8} &amp;amp; \text{if N &amp;gt; 3} \\
\end{cases}

\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where N is the goal difference:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ N = Goals_{i} - Goals_{j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The expected result is calculated based on the rankings of both teams going into the match&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ E(result) = \frac{1}{10 ^ \frac{-dr_{i,j}}{400} + 1} \]&lt;/span&gt;
where the difference in rankings (dr) is calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
dr_{i,j} = 
\begin{cases} 
rating_{i, t-1} + 100 - rating_{j, t-1} &amp;amp; \text{if i at home} \\
rating_{i, t-1} - 100 - rating_{j, t-1} &amp;amp; \text{if j at home} \\
rating_{i, t-1} - rating_{j, t-1} &amp;amp; \text{if neutral} \\
\end{cases}

\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We then add K, G and R to each row of the data frame to make our calculations easier down the line. Unfortunately, my dataset doesn’t give the context of each game, so I’ve set K to 40 for every match. In theory this shouldn’t make a difference, but will affect the ratings of teams who over/under perform in big matches.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;international_results %&amp;lt;&amp;gt;%
  # select relevant columns
  select(
    date,
    home = home_team, away = away_team,
    hgoal = home_score, agoal = away_score,
    neutral
  ) %&amp;gt;%
  # convert date to date format
  mutate(date = as.Date(date)) %&amp;gt;%
  # K = match importance
  # don&amp;#39;t have competition data in this dataset so just set to 40
  mutate(K =  40) %&amp;gt;%
  # G = goal difference factor
  # takes into account how much a team is beaten by
  mutate(G = case_when(
    abs(hgoal-agoal) &amp;lt; 2 ~ 1,
    abs(hgoal-agoal) &amp;lt; 3 ~ 1.5,
    abs(hgoal-agoal) &amp;gt;= 3 ~ 1.75 + (abs(hgoal-agoal)-3)/8
  )) %&amp;gt;%
  # results = 1 for win and 0.5 for a draw
  mutate(result = case_when(
    hgoal &amp;gt; agoal ~ 1,
    hgoal &amp;lt; agoal ~ 0,
    hgoal == agoal ~ 0.5
  )) %&amp;gt;%
  # arrange by date so ELO can be updated sequentially
  arrange(date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We still need to initialise our rating (R) for each team, which for simplicity I’ve set to 1200 to start with. That is, every team starts with the same rating and will gradually tend towards their ‘natural’ rating. Given There’s probably at least 50 years of data for most teams before the Premier League begins in 1992, hopefully it should be enough for this to level out.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;team_ratings &amp;lt;- international_results %&amp;gt;%
  # select date and teams
  select(date, home, away) %&amp;gt;%
  # melt
  gather(., &amp;quot;location&amp;quot;, &amp;quot;nation&amp;quot;, home, away) %&amp;gt;%
  select(-location) %&amp;gt;%
  arrange(date) %&amp;gt;%
  # set out unique teams with a rating of 1200
  filter(!duplicated(nation)) %&amp;gt;%
  mutate(rating = 1200) %&amp;gt;%
  select(-date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, using the very first international fixture between England and Scotland in 1872 we have parameters of&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(international_results, n = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         date     home    away hgoal agoal neutral  K G result
## 1 1872-11-30 Scotland England     0     0   FALSE 40 1    0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ratings(t-1) = 1200 for both England and Scotland
K = 40
G = 1 for a draw
R = 0.5 for a draw&lt;/p&gt;
&lt;p&gt;the equal ratings, but home location for Scotland mean that for England:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ dr_{i,j} = 1200 + 100 - 1200  = 100 \]&lt;/span&gt;
and so an expected result (1- the expected home result)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ E(R) = 1 - \frac{1}{10 ^ {100/400} + 1} = 1 - \frac{1}{2.78}  =  0.36\]&lt;/span&gt;
and so England will get a post match rating of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ rating_{j} = 1200 + (40 \cdot 1 \cdot (0.5 - 0.36)) = 1207.2 \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_ELO &amp;lt;- function(date, home, away, K, G, result) {
  #get the difference in ratings
  hr &amp;lt;- team_ratings$rating[which(team_ratings$nation == home)]
  vr &amp;lt;- team_ratings$rating[which(team_ratings$nation == away)]
  dr &amp;lt;- vr - (hr + 100)
  
  # calculate expected results
  e_result &amp;lt;- 1/ ((10^(dr/400))+1)
  
  # calculate new ratings
  new_hr &amp;lt;- hr + ((K*G) * (result - e_result))
  new_vr &amp;lt;- vr + ((K*G) * ((1-result) - (1-e_result)))
  
  # pipe these back into a df of team ratings to sample from
  team_ratings$rating[which(team_ratings$nation == home)] &amp;lt;&amp;lt;- new_hr
  team_ratings$rating[which(team_ratings$nation == away)] &amp;lt;&amp;lt;- new_vr
  
  # return new ratings
  return(list(h_rating = new_hr, v_rating = new_vr))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which can be applied to the dataframe of match information using pmap_df from the &lt;a href=&#34;&#34;&gt;purrr&lt;/a&gt; package, which allows for some pleasing conciseness. It allows for the speed of applying a function, without needing to split the data frame by row and pass into lapply and rebind together.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elo_data &amp;lt;- international_results %&amp;gt;%
  # select relevant variable
  # keep date so we know a teams ELO at specific date
  select(date, home, away, K, G, result) %&amp;gt;%
  bind_cols(pmap_df(., calc_ELO)) %&amp;gt;%
  # get rid of ELO parameters
  select(date, home, away, h_rating, v_rating) %&amp;gt;%
  # gather twice to get a long df of teams ratings after matches
  gather(&amp;quot;location&amp;quot;, &amp;quot;nation&amp;quot;, -date, -h_rating, -v_rating) %&amp;gt;%
  gather(&amp;quot;rating&amp;quot;, &amp;quot;value&amp;quot;, -date, -location, -nation) %&amp;gt;%
  # filter for home rating for teams at home and vice versa
  filter((location == &amp;quot;home&amp;quot; &amp;amp; rating == &amp;quot;h_rating&amp;quot;) |
           (location == &amp;quot;away&amp;quot; &amp;amp; rating == &amp;quot;v_rating&amp;quot;)) %&amp;gt;%
  select(date, nation, rating = value) %&amp;gt;%
  # we only care about ratings from August 1992
  filter(date &amp;gt; &amp;quot;1992-07-31&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re left with a dataframe of 46992 observations for 3 variables: date, nation and the ranking of the nation at that time. We can plot a random selection of 5 teams just to sanity check and see that teams we know have historically been stronger (e.g. Argentina) show consistently higher rankings than weaker nations (e.g. Greece).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# randomly choose 5 teams with max ELOs &amp;gt; 1600
teams &amp;lt;- elo_data %&amp;gt;%
  filter(rating &amp;gt; 1600) %&amp;gt;%
  .$nation %&amp;gt;%
  unique() %&amp;gt;%
  .[sample(length(.), 5)]

# plot rating over time for these 5 teams
p1 &amp;lt;- elo_data %&amp;gt;%
  filter(nation %in% teams) %&amp;gt;%
  ggplot(aes(x = date, y = rating, colour = nation, group = nation)) +
  geom_point() +
  geom_line() +
  # colour by football shirt colour
  scale_colour_manual(values = c(&amp;quot;skyblue&amp;quot;, 
                                 &amp;quot;darkblue&amp;quot;,
                                 &amp;quot;darkorange&amp;quot;,
                                 &amp;quot;darkgreen&amp;quot;,
                                 &amp;quot;red&amp;quot;)) +
  labs(title = &amp;quot;ELO Ratings of Select Countries over Time&amp;quot;,
       subtitle = &amp;quot;ratings calculated using homebrew script&amp;quot;,
       x = &amp;quot;date&amp;quot;,
       y = &amp;quot;rating&amp;quot;) +
  theme_minimal()

plot(p1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-06-20-The_Knowledge_4_files/figure-html/plot_teams_elo-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’m pretty happy with how the code works. With a more complete dataset of matches, and also the time to properly filter countries in and out as they are formed/dissolved, I think it would make a pretty viable answer, however, I wanted to be as accurate as possible, and I can’t compete with the official-unofficial ratings of &lt;a href=&#34;https://www.eloratings.net&#34;&gt;eloratings.net&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dynamic-scraping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dynamic Scraping&lt;/h2&gt;
&lt;p&gt;When scraping data for blog posts, I typically rely on &lt;a href=&#34;&#34;&gt;rvest&lt;/a&gt; and it’s read_html(url) function. However, while this works for the static websites which make up the vast majority of sites containing tables of data, it struggles with websites that use JavaScript to dynamically generate pages.&lt;/p&gt;
&lt;p&gt;Eloratings.net is one such website which rvest is unable to scrape. E.g.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# url to data on Brazil&amp;#39;s ELO rating over time
url &amp;lt;- &amp;quot;https://eloratings.net/Brazil&amp;quot;

read &amp;lt;- read_html(url) %&amp;gt;%
  # this is the CSS selector for the page title
  html_nodes(&amp;quot;#mainheader&amp;quot;)

read&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_nodeset (1)}
## [1] &amp;lt;h1 id=&amp;quot;mainheader&amp;quot; class=&amp;quot;mainheader&amp;quot;&amp;gt;&amp;lt;/h1&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;does not manage to capture the data displayed in the page mainheader (it ‘should’ return “World Football Elo Ratings: Brazil” from the title of that page).&lt;/p&gt;
&lt;p&gt;Instead, what we want to do is save a copy of the generated page as a .html file and then read that into R using read_html(). Luckily, a way exists to do just that, using the (now deprecated, but still working) &lt;a href=&#34;http://phantomjs.org/&#34;&gt;PhantomJS headless browser&lt;/a&gt;. Much of the code I used to get going with this is adapted from a tutorial &lt;a href=&#34;https://velaco.github.io/how-to-scrape-data-from-javascript-websites-with-R/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First you want to install PhantomJS from the above website and run through it’s &lt;a href=&#34;http://phantomjs.org/quick-start.html&#34;&gt;quick start guide&lt;/a&gt;. This is a pretty thorough guide, I would say that there are really only three steps from installation to getting going:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://www.howtogeek.com/118594/how-to-edit-your-system-path-for-easy-command-line-access/&#34;&gt;Add phantomjs to the system PATH&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Open a text editor and save one of the &lt;a href=&#34;https://phantomjs.org/quick-start.html&#34;&gt;tutorial scripts&lt;/a&gt; as filename.js&lt;/li&gt;
&lt;li&gt;run &amp;gt; phantomjs C:/Users/usr/path/to/file.js
in a command line console&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The file we’re going to use to render the js pages and then save the html is below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;// scrapes a given url (for eloratings.net)

// create a webpage object
var page = require(&amp;#39;webpage&amp;#39;).create(),
  system = require(&amp;#39;system&amp;#39;)

// the url for each country provided as an argument
country= system.args[1];

// include the File System module for writing to files
var fs = require(&amp;#39;fs&amp;#39;);

// specify source and path to output file
// we&amp;#39;ll just overwirte iteratively to a page in the same directory
var path = &amp;#39;elopage.html&amp;#39;

page.open(country, function (status) {
  var content = page.content;
  fs.write(path,content,&amp;#39;w&amp;#39;)
  phantom.exit();
});&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(which, again, is stolen and adapted from &lt;a href=&#34;https://velaco.github.io/how-to-scrape-data-from-javascript-websites-with-R/&#34;&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;This is saved as scrape_ELO.js in the static directory of my blog folder.&lt;/p&gt;
&lt;p&gt;To keep everything in R, we can use the system() family of functions, which provides access to the OS command line. Though the referenced tutorial uses system(), it relies on scraping a single referenced page. To iteratively scrape every country, we’ll need to provide an argument (country) which will contain the link to the page on eloratings.net for that country.&lt;/p&gt;
&lt;p&gt;E.g. for Brazil we will provide “&lt;a href=&#34;https://www.eloratings.net/Brazil&#34; class=&#34;uri&#34;&gt;https://www.eloratings.net/Brazil&lt;/a&gt;” as the country argument&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;phantom_dir &amp;lt;- &amp;quot;C:/Users/path/to/scrape_ELO/&amp;quot;
country_url &amp;lt;- &amp;quot;https://www.eloratings.net/Brazil&amp;quot;

# use system2 to invoke phantomjs via it&amp;#39;s executable
system2(&amp;quot;C:/Users/path/to/phantomjs-2.1.1-windows/bin/phantomjs.exe&amp;quot;,
        #provide the path to the scraping script and the country url as argument
        args = c(file.path(phantom_dir, &amp;quot;scrape_ELO.js&amp;quot;), country_url))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then read in this saved html page using rvest as per usual and recover the information therein.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# read in the saved html file
page &amp;lt;- read_html(&amp;quot;elopage.html&amp;quot;)

# scrape with rvest as normal
country_name &amp;lt;- page %&amp;gt;%
  html_nodes(&amp;quot;#mainheader&amp;quot;) %&amp;gt;%
  html_text() %&amp;gt;%
  gsub(&amp;quot;Elo Ratings: &amp;quot;, &amp;quot;&amp;quot;, .)

country_name&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Brazil&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m not going to include my full script for scraping eloratings.net as usually a reason for doing this obscuring of the data is to prevent exactly what I’m doing. Instead I’ll give a skeleton function of the one I use. If you are having problems with setting up phantomjs to scrape pages, my contact details are listed on my &lt;a href=&#34;http://www.robert-hickman.eu/&#34;&gt;blog homepage&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scrape_nation &amp;lt;- function(country) {
  # download the page
  url &amp;lt;- paste0(&amp;quot;https://eloratings.net/&amp;quot;, country)
  system2(&amp;quot;C:/Users/path/to/phantomjs-2.1.1-windows/bin/phantomjs.exe&amp;quot;, 
          args = c(file.path(phantom_dir, &amp;quot;scrape_ELO.js&amp;quot;), url))
  
  # read in downloaded page
  page &amp;lt;- read_html(&amp;quot;elopage.html&amp;quot;)
  
  # recover information
  country_name &amp;lt;- page %&amp;gt;%
    html_nodes(&amp;quot;#mainheader&amp;quot;) %&amp;gt;%
    html_text() %&amp;gt;%
    gsub(&amp;quot;Elo Ratings: &amp;quot;, &amp;quot;&amp;quot;, .)
  
  opposing &amp;lt;- page %&amp;gt;%
      html_nodes(&amp;quot;.r1 a&amp;quot;) %&amp;gt;%
      html_text()
  
  teams &amp;lt;- page %&amp;gt;%
      html_nodes(&amp;quot;.r1&amp;quot;)
  
  fixtures &amp;lt;- map2_df(teams, opposing, split_teams)

  ratings &amp;lt;- page %&amp;gt;%
    html_nodes(&amp;quot;.r4&amp;quot;) %&amp;gt;%
    html_text() %&amp;gt;%
    map_df(., split_ratings)
  
  rankings &amp;lt;- page %&amp;gt;%
    html_nodes(&amp;quot;.r6&amp;quot;) %&amp;gt;%
    map_df(., split_rankings)

  dates &amp;lt;- page %&amp;gt;%
    html_nodes(&amp;quot;.r0&amp;quot;) %&amp;gt;%
    html_text() %&amp;gt;%
    map_df(., convert_date)

  # bind into a data frame
  df &amp;lt;- fixtures %&amp;gt;%
    cbind(., ratings) %&amp;gt;%
    cbind(., rankings) %&amp;gt;%
    cbind(., dates) %&amp;gt;%
    mutate(table_country = country_name)
}

elO_data &amp;lt;- map_df(country_links, scrape_nation)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we want to convert this to long format. We have two observations per country and any point in time- the rating, and the ranking. I’m going to filter out just the ranking, as that’s what the questions ask, but if anything there’s possibly more information in the rating data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elo_data %&amp;lt;&amp;gt;%
  # rename and select variables
  select(
    date,
    home, away,
    rating_home = r1, rating_away = r2,
    ranking_home = ranking1, ranking_away = ranking2
  ) %&amp;gt;%
  # melt twice to convert to long format
  gather(
    &amp;quot;location&amp;quot;, &amp;quot;nation&amp;quot;,
    -rating_home, -rating_away, -ranking_home, -ranking_away, -date
  ) %&amp;gt;%
  gather(&amp;quot;measure&amp;quot;, &amp;quot;value&amp;quot;, -nation, -date, -location) %&amp;gt;%
  # take only relevant information
  filter(
    (location == &amp;quot;home&amp;quot; &amp;amp; measure %in% c(&amp;quot;rating_home&amp;quot;, &amp;quot;ranking_home&amp;quot;)) |
      (location == &amp;quot;away&amp;quot; &amp;amp; measure %in% c(&amp;quot;rating_away&amp;quot;, &amp;quot;ranking_away&amp;quot;))
  ) %&amp;gt;%
  separate(measure, into = c(&amp;quot;measure&amp;quot;, &amp;quot;location&amp;quot;), &amp;quot;_&amp;quot;) %&amp;gt;%
  # filter out relevant data
  filter(!duplicated(.)) %&amp;gt;%
  filter(date &amp;gt; &amp;quot;1992-01-01&amp;quot;) %&amp;gt;%
  filter(measure == &amp;quot;ranking&amp;quot;) %&amp;gt;%
  select(-measure, ranking = value, -location)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;answering-the-question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Answering the Question&lt;/h2&gt;
&lt;p&gt;So now we have a database of Premier League players’ nationalities, and also of the ELO rankings of countries since 1992, we can answer the original questions.&lt;/p&gt;
&lt;p&gt;First we need to make sure that the data can join to each other, which means making sure that the nation names are common between the two data sets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get unique names for countries in both data sets
squad_teams &amp;lt;- unique(squads$nation)
rating_teams &amp;lt;- unique(elo_data$nation)

# find non joining country names
squad_teams[!squad_teams %in% rating_teams]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Ireland Republic&amp;quot;       &amp;quot;Trinidad and Tobago&amp;quot;   
##  [3] &amp;quot;Czech Republic&amp;quot;         &amp;quot;Macedonia FYR&amp;quot;         
##  [5] &amp;quot;St. Kitts and Nevis&amp;quot;    &amp;quot;Bosnia and Herzegovina&amp;quot;
##  [7] &amp;quot;Congo DR&amp;quot;               &amp;quot;Antigua and Barbuda&amp;quot;   
##  [9] &amp;quot;Korea Republic&amp;quot;         &amp;quot;Curacao&amp;quot;               
## [11] &amp;quot;Cape Verde Islands&amp;quot;     &amp;quot;Equatorial Guinea&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some are missing where country names follow different convention- e.g. the Democratic Republic of Congo is named DR Congo in one, and Congo DR in the other. We can quickly convert these odd countries and join the two data sets together using dplyr. Then we can get an idea of the national rankings of the nationality of Premier League players since it’s inception&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# rename mismatching nations
elo_data %&amp;lt;&amp;gt;%
  mutate(nation = case_when(
    grepl(&amp;quot;^Ireland&amp;quot;, nation) ~ &amp;quot;Ireland Republic&amp;quot;,
    grepl(&amp;quot;^Czechia&amp;quot;, nation) ~ &amp;quot;Czech Republic&amp;quot;,
    grepl(&amp;quot;Trinidad/Tobago&amp;quot;, nation) ~ &amp;quot;Trinidad and Tobago&amp;quot;,
    grepl(&amp;quot;Macedonia&amp;quot;, nation) ~ &amp;quot;Macedonia FYR&amp;quot;,
    grepl(&amp;quot;St Kitts and Nevis&amp;quot;, nation) ~ &amp;quot;St. Kitts and Nevis&amp;quot;,
    grepl(&amp;quot;Bosnia/Herzeg&amp;quot;, nation) ~ &amp;quot;Bosnia and Herzegovina&amp;quot;,
    grepl(&amp;quot;DR Congo&amp;quot;, nation) ~ &amp;quot;Congo DR&amp;quot;,
    grepl(&amp;quot;Antigua/Barbuda&amp;quot;, nation) ~ &amp;quot;Antigua and Barbuda&amp;quot;,
    grepl(&amp;quot;South Korea&amp;quot;, nation) ~ &amp;quot;Korea Republic&amp;quot;,
    grepl(&amp;quot;Curaçao&amp;quot;, nation) ~ &amp;quot;Curacao&amp;quot;,
    grepl(&amp;quot;Cape Verde&amp;quot;, nation) ~ &amp;quot;Cape Verde Islands&amp;quot;,
    grepl(&amp;quot;Equat Guinea&amp;quot;, nation) ~ &amp;quot;Equatorial Guinea&amp;quot;,
    TRUE ~ nation
  ))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;players_national_elo &amp;lt;- squads %&amp;gt;%
  # convert dates
  mutate(year = as.numeric(year)) %&amp;gt;%
  # join elo rating data
  left_join(., elo_data, by = &amp;quot;nation&amp;quot;) %&amp;gt;%
  # take only relevant data per season
  filter(date &amp;lt; as.Date(paste0(year, &amp;quot;-06-30&amp;quot;)) &amp;amp;
           date &amp;gt; as.Date(paste0(year-1, &amp;quot;-07-01&amp;quot;))) %&amp;gt;%
  # rename for concise printing
  rename(apps = appearances, sub_apps = sub_appearances)

# histogram of players national team lowest ratings
p2 &amp;lt;- players_national_elo %&amp;gt;%
  arrange(ranking) %&amp;gt;%
  # take only lowest ranked observations
  filter(!duplicated(paste(player, team, year), fromLast = TRUE)) %&amp;gt;%
  # group by decade
  mutate(decade = case_when(
    year &amp;lt; 2000 ~ &amp;quot;1990&amp;quot;,
    year &amp;lt; 2010 ~ &amp;quot;2000&amp;quot;,
    year &amp;lt; 2020 ~ &amp;quot;2010&amp;quot;
  )) %&amp;gt;%
  ggplot(aes(ranking)) +
  geom_histogram() +
  labs(title = &amp;quot;Distribution of EPL Player&amp;#39;s Nation&amp;#39;s Ranking&amp;quot;,
       subtitle = &amp;quot;(taking lowest point of ranking data&amp;quot;,
       x = &amp;quot;lowest national team ELO rating&amp;quot;,
       y = &amp;quot;player count&amp;quot;) +
  theme_minimal() +
  facet_wrap(~decade)

plot(p2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-06-20-The_Knowledge_4_files/figure-html/combine_datasets-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It surprised me that the distribution hasn’t obviously changed over the year. There’s maybe a few more players from ‘mid-level’ nations (ranking 30-50) this decade but I’d doubt it’s significantly more. The majority of players come from nations in the top 20 worldwide consistently since the Premier League’s inception.&lt;/p&gt;
&lt;p&gt;We can easily then take the players with the worst national team ranking by arranging by the ranking of national teams (and removing duplicate players so it isn’t just filled with the same 2/3 names)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;worst_national_team_players &amp;lt;- players_national_elo %&amp;gt;%
  # arrange rating from low to high
  arrange(-ranking) %&amp;gt;%
  # remove duplicated players
  filter(!duplicated(player)) %&amp;gt;%
  # select only relevant info
  select(year, player, team, apps, sub_apps, nation, ranking)

# show the 25 players with the worst ranking their nation had during that time
head(worst_national_team_players, n = 25)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    year             player                    team apps sub_apps
## 1  2015     Brandon Comley     queens-park-rangers    0        1
## 2  2004        Zesh Rehman                  fulham    0        1
## 3  2012     George Elokobi wolverhampton-wanderers    3        6
## 4  2014     Leandro Bacuna             aston-villa   28        7
## 5  2013      Kemy Agustien            swansea-city    4       14
## 6  2015        Kenji Gorré            swansea-city    0        1
## 7  1998 Danny Higginbotham       manchester-united    0        1
## 8  1999          Carl Cort               wimbledon    6       10
## 9  2007 Mikele Leigertwood        sheffield-united   16        3
## 10 2007     Moses Ashikodi                 watford    0        2
## 11 2016       Cuco Martina             southampton   11        4
## 12 2003         Neil Danns        blackburn-rovers    1        1
## 13 2005  Dexter Blackstock             southampton    8        1
## 14 2019     Neil Etheridge            cardiff-city   38        0
## 15 2013     Emmerson Boyce          wigan-athletic   36        0
## 16 1997          Mart Poom            derby-county    4        0
## 17 2007     Matthew Briggs                  fulham    0        1
## 18 2018        Nahki Wells                 burnley    0        9
## 19 2012      Jason Roberts        blackburn-rovers    5        5
## 20 1998 Sagi Burton-Godwin          crystal-palace    1        1
## 21 2010     Gunnar Nielsen         manchester-city    0        1
## 22 2009          Leon Cort              stoke-city    9        2
## 23 2000        Adam Newton         west-ham-united    0        2
## 24 2004       Delroy Facey        bolton-wanderers    0        1
## 25 2013    Gaël Bigirimana        newcastle-united    3       10
##                 nation ranking
## 1           Montserrat     227
## 2             Pakistan     204
## 3              Somalia     199
## 4              Curacao     188
## 5              Curacao     186
## 6              Curacao     186
## 7            Gibraltar     181
## 8               Guyana     179
## 9  Antigua and Barbuda     179
## 10 Antigua and Barbuda     179
## 11             Curacao     179
## 12              Guyana     175
## 13 Antigua and Barbuda     174
## 14         Philippines     174
## 15            Barbados     173
## 16             Estonia     169
## 17              Guyana     169
## 18             Bermuda     167
## 19             Grenada     166
## 20 St. Kitts and Nevis     160
## 21       Faroe Islands     160
## 22              Guyana     153
## 23 St. Kitts and Nevis     147
## 24             Grenada     145
## 25             Burundi     143&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So Zesh Rehman as the question supposes, is one of the players from extremely low ranked nations. However, he is beaten by Brandon Comley at QPR who represents Montserrat internationally, who fell as low as 227 in the ELO world rankings. The 204 for Zesh Rehman is different to the 168 listed in the question for three reasons:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;As earlier discussed we are using ELO rankings, not the official FIFA rankings (and are probably right to)&lt;/li&gt;
&lt;li&gt;This lists the &lt;em&gt;lowest&lt;/em&gt; that nation fell in the time that player was in the Premier League, not just at the time Zesh Rehman (or any other player) made his debut for Pakistan (/other nation)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Indeed, Brandon Comley did not start to play for Montserrat until September 2018, by which time he was playing for Colchester in League Two.&lt;/p&gt;
&lt;p&gt;What is interesting though, is that there are 19 instances of players of nationalities ranked lower than 164- many more than I would have imagined.&lt;/p&gt;
&lt;p&gt;To truly answer the question, that is, to only count rankings in games Premier League players played in, we can scrape international match data from &lt;a href=&#34;https://www.national-football-teams.com/&#34;&gt;national_football-teams.com&lt;/a&gt;. 11v11.com which I’ve used thus far does list international appearances, but tends to have thinner data (and vice versa for national-football-teams.com with regards to Premier League squad data).&lt;/p&gt;
&lt;p&gt;First, we’ll manually add the links to the profiles of players who are contenders for having the lowest ranked appearance (the lowest 20 in the above dataframe), then we can use this to scrape the international matches they’ve played in. This is then matched by data to the ELO ranking data to find the national ranking of the country they represented. Finally, we take the lowest ranked appearance for each player and see if any can beat Zesh Rehman.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(players_intl_links)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           player
## 1 Brandon Comley
## 2    Zesh Rehman
## 3 George Elokobi
## 4 Leandro Bacuna
## 5  Kemy Agustien
## 6    Kenji Gorré
##                                                                        url
## 1 https://www.national-football-teams.com/player/71845/Brandon_Comley.html
## 2    https://www.national-football-teams.com/player/12929/Zesh_Rehman.html
## 3                                                                     &amp;lt;NA&amp;gt;
## 4 https://www.national-football-teams.com/player/63672/Leandro_Bacuna.html
## 5  https://www.national-football-teams.com/player/59413/Kemy_Agustien.html
## 6    https://www.national-football-teams.com/player/74671/Kenji_Gorre.html&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_international_matches &amp;lt;- function(player, url) {
  if(!is.na(url)) {
    df &amp;lt;- url %&amp;gt;%
      read_html() %&amp;gt;%
      html_nodes(&amp;quot;#games &amp;gt; table&amp;quot;) %&amp;gt;%
      html_table(fill = TRUE) %&amp;gt;%
      as.data.frame() %&amp;gt;%
      .[c(1:2, 5:7)] %&amp;gt;%
      filter(Date != &amp;quot;&amp;quot;) %&amp;gt;%
      mutate(result = gsub(&amp;quot;\n.*&amp;quot;, &amp;quot;&amp;quot;, Result), player = player, match_date = as.Date(Date)) %&amp;gt;%
      select(player, match_date, home = Home.Team, away = Away.Team.1, result, event = Event)
  }
}

low_ranked_appearances &amp;lt;- players_intl_links %&amp;gt;%
  pmap_df(., get_international_matches) %&amp;gt;%
  left_join(., players_national_elo, by = &amp;quot;player&amp;quot;) %&amp;gt;%
  filter(match_date == date) 

lowest_ranked_appearances &amp;lt;- low_ranked_appearances %&amp;gt;%
  arrange(-ranking) %&amp;gt;%
  filter(!duplicated(player, fromLast = TRUE)) %&amp;gt;%
  mutate(match = paste(home, &amp;quot;vs&amp;quot;, away)) %&amp;gt;%
  select(date, player, team, nation, ranking, match, result)

lowest_ranked_appearances&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          date         player             team            nation ranking
## 1  2005-12-07    Zesh Rehman           fulham          Pakistan     197
## 2  2018-11-13 Neil Etheridge     cardiff-city       Philippines     165
## 3  2018-03-26   Cuco Martina          everton           Curacao     136
## 4  2004-05-20  Jason Roberts       portsmouth           Grenada     131
## 5  2008-03-26 Emmerson Boyce   wigan-athletic          Barbados     126
## 6  2019-03-22   Pedro Obiang  west-ham-united Equatorial Guinea     125
## 7  2015-06-13   Modou Barrow     swansea-city            Gambia     122
## 8  2006-11-21     Paul Ifill sheffield-united          Barbados     119
## 9  2019-06-21 Leandro Bacuna     cardiff-city           Curacao     118
## 10 2003-06-07      Mart Poom       sunderland           Estonia      85
##                                     match result
## 1                   Pakistan vs Sri Lanka    1:0
## 2                Philippines vs Singapore    1:0
## 3                      Curaçao vs Bolivia    1:0
## 4                         Cuba vs Grenada    2:2
## 5                    Barbados vs Dominica    1:0
## 6              Sudan vs Equatorial Guinea    1:4
## 7                  South Africa vs Gambia    0:0
## 8  Barbados vs Saint Vincent &amp;amp; Grenadines    3:0
## 9                     Honduras vs Curaçao    0:1
## 10                     Estonia vs Andorra    2:0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And it appears not! It’s quite incredible that in the 13.5 years since the question was answered noone has even got close. This is probably due to the fact that though Pakistan were ranked 168th by FIFA at the time, using the ELO system, they come out as the 197th best team at the end of 2005.&lt;/p&gt;
&lt;p&gt;Perhaps more surprising is that it seems that until 2003 the record was 85th, held by Mart Poom playing for Estonia (though this might also be because data on low ranking international matches gets worse as you go back past this point- Mart Poom probably played for Estonia in).&lt;/p&gt;
&lt;p&gt;We can graph the appearances by Premier League players in matches involving low ranked nations to see how close people have gotten pretty easily. The below shows the rankings for the countries in the above printed data frame. Matches where Premier League players made an appearance are highlighted in black boxes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3 &amp;lt;- elo_data %&amp;gt;%
  filter(nation %in% low_ranked_appearances$nation) %&amp;gt;%
  mutate(date = as.Date(date)) %&amp;gt;%
  ggplot(aes(x = date, y = ranking, colour = nation, group = nation)) +
  geom_point(alpha = 0.5, size = 2) +
  geom_line(alpha = 0.5, size = 2) +
  geom_point(data = low_ranked_appearances, aes(x = match_date),
             colour = &amp;quot;black&amp;quot;, fill = NA, size = 3,stroke = 2, shape = 22) +
  # colour by football shirt colour
  scale_colour_manual(values = c(&amp;quot;yellow&amp;quot;, &amp;quot;darkblue&amp;quot;, &amp;quot;darkred&amp;quot;, &amp;quot;lightblue&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;darkgreen&amp;quot;, &amp;quot;darkgoldenrod&amp;quot;)) +
  scale_x_date() +
  labs(title = &amp;quot;Lowest National Ranking of Premier League International Caps&amp;quot;,
          subtitle = &amp;quot;premier league player caps in red&amp;quot;,
          y = &amp;quot;national team ELO ranking&amp;quot;) +
  theme_minimal()

plot(p3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-06-20-The_Knowledge_4_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It seems Pakistan have continued to hover around the 200th position pretty consistently over the last 25 years. Indeed, if Zesh Rehman was still playing in the Premier League he would have broken his own record less than 1 month ago, when Pakistan sank to 201st position after &lt;a href=&#34;https://www.national-football-teams.com/matches/report/23278/Pakistan_Cambodia.html&#34;&gt;a defeat to Cambodia&lt;/a&gt;. The Philippines, represented by Neil Etheridge in recent years, have been ranked lower, but not since 2010. Other than that Curacao can be seen to only have come close near to their formation as a FIFA member, and are clearly rapidly improving. Barbados and Grenada might be fruitful for any possible record taker, but are still a good 30 places higher than Pakistan.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lowest-ranked-countries-of-premier-league-winners&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lowest Ranked Countries of Premier League Winners&lt;/h2&gt;
&lt;p&gt;We can answer the first question and find the players with the worst national team rankings by joining in and selecting for teams that won the league. This is done by first initializing a simple df with the league winners per season. The given answers to this question have differed in when they take the country’s rankings, whether at their lowest point within the season, or at the season’s end. I decided to use a different method- to take the average ranking of the nation over the whole season.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data frame of EPL winners over year
epl_winners &amp;lt;- data.frame(
  champion = c(
    &amp;quot;manchester-united&amp;quot;,
    &amp;quot;manchester-united&amp;quot;,
    &amp;quot;blackburn-rovers&amp;quot;,
    &amp;quot;manchester-united&amp;quot;,
    &amp;quot;manchester-united&amp;quot;,
    &amp;quot;arsenal&amp;quot;,
    &amp;quot;manchester-united&amp;quot;,
    &amp;quot;manchester-united&amp;quot;,
    &amp;quot;manchester-united&amp;quot;,
    &amp;quot;arsenal&amp;quot;,
    &amp;quot;manchester-united&amp;quot;,
    &amp;quot;arsenal&amp;quot;,
    &amp;quot;chelsea&amp;quot;,
    &amp;quot;chelsea&amp;quot;,
    &amp;quot;manchester-united&amp;quot;,
    &amp;quot;manchester-united&amp;quot;,
    &amp;quot;manchester-united&amp;quot;,
    &amp;quot;chelsea&amp;quot;,
    &amp;quot;manchester-united&amp;quot;,
    &amp;quot;manchester-city&amp;quot;,
    &amp;quot;manchester-united&amp;quot;,
    &amp;quot;manchester-city&amp;quot;,
    &amp;quot;chelsea&amp;quot;,
    &amp;quot;leicester-city&amp;quot;,
    &amp;quot;chelsea&amp;quot;,
    &amp;quot;manchester-city&amp;quot;,
    &amp;quot;manchester-city&amp;quot;),
  year = 1993:2019
) 

# merge in winning temas
epl_winning_squads &amp;lt;- players_national_elo %&amp;gt;%
  left_join(., epl_winners, by = &amp;quot;year&amp;quot;) %&amp;gt;%
  # filter for players that win the league that season
  filter(team == champion) %&amp;gt;%
  # filter for the year that player wins the league
  filter(date &amp;lt; as.Date(paste0(year, &amp;quot;-06-30&amp;quot;)) &amp;amp;
           date &amp;gt; as.Date(paste0(year-1, &amp;quot;-07-01&amp;quot;))) %&amp;gt;%
  group_by(player, year, team, apps, sub_apps, nation) %&amp;gt;%
  summarise(av_ranking = mean(ranking)) %&amp;gt;%
  arrange(-av_ranking) %&amp;gt;%
  # take the lowest ranking per player/year combination
  select(year, player, team, apps, sub_apps, nation, av_ranking)

# show the 25 players with the worst ranking their nation had during that time
head(epl_winning_squads, n = 25)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 25 x 7
## # Groups:   player, year, team, apps, sub_apps [25]
##     year player       team           apps sub_apps nation        av_ranking
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;
##  1  1998 Christopher~ arsenal           7        9 Liberia            114. 
##  2  2010 Gaël Kakuta  chelsea           0        1 Congo DR           106. 
##  3  2004 Justin Hoyte arsenal           0        1 Trinidad and~      102. 
##  4  2013 Jonny Evans  manchester-u~    21        2 Northern Ire~      101. 
##  5  2002 Igors Stepa~ arsenal           6        0 Latvia              99.1
##  6  2009 Manucho      manchester-u~     0        1 Angola              89.9
##  7  2006 Eidur Gudjo~ chelsea          16       10 Iceland             79.8
##  8  2003 Roy Carroll  manchester-u~     8        2 Northern Ire~       76.5
##  9  2001 David Healy  manchester-u~     0        1 Northern Ire~       76  
## 10  1999 Dwight Yorke manchester-u~    32        0 Trinidad and~       75.8
## # ... with 15 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we have another quite clear winner Christopher Wreh, whose Liberia averaged a ranking of 114 over the 1997/1998 season. Gael Kakuta and Justin Hoyte (and Igors Stepanovs and Manucho) seem like false answers as they only achieved a handful of appearances on the way to winning the league, which only leaves Jonny Evans in 2013 as a real contender for this record. Northern Ireland in this season averaged just under 100th place.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extra-credit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extra Credit&lt;/h2&gt;
&lt;p&gt;We can pretty quickly and easily see how these players compare to what we found above in terms of the lowest ranked nation they’ve turned out for. Again, first i had to manually add links to their pages of &lt;a href=&#34;www.national-football-teams.com&#34;&gt;www.national-football-teams.com&lt;/a&gt; and then scrape each match they’ve appeared for their home country in.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(champions_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   player         url                                                       
##   &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;                                                     
## 1 Christopher W~ https://www.national-football-teams.com/player/13930/Chri~
## 2 Gaël Kakuta    https://www.national-football-teams.com/player/67629/Gael~
## 3 Justin Hoyte   https://www.national-football-teams.com/player/52483/Just~
## 4 Jonny Evans    https://www.national-football-teams.com/player/16586/Jonn~
## 5 Igors Stepano~ https://www.national-football-teams.com/player/3728/Igors~
## 6 Manucho        https://www.national-football-teams.com/player/14353/Manu~&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we do this scrape and analyse we get Jonny Evans as the Premier League winner who appeared for the lowest rated nation during a championship campaign. He’s Northern Ireland team slumped to a 2-0 defeat to Israel in 2013 which left them 106th in the world rankings. 2 months later, he won the Premier League with Manchester United. There’s relatively few challengers with only Igors Stepanovs (Latvia 101st, 6 appearances for Arsenal in 2002) getting close and then Roy Carroll (Northern Ireland), Eidur GOdjohnsen (Iceland), and Rihad Mahrez (Algeria) in the 80s (Manucho excluded due to lack of appearances in Manchester United’s 2009 Premier League campaign).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;low_ranked_champs &amp;lt;- champions_df %&amp;gt;%
  pmap_df(., get_international_matches) %&amp;gt;%
  left_join(., players_national_elo, by = &amp;quot;player&amp;quot;) %&amp;gt;%
  left_join(., epl_winners, by = &amp;quot;year&amp;quot;) %&amp;gt;%
  filter(team == champion) %&amp;gt;%
  filter(match_date == date) 

lowest_ranked_champs &amp;lt;- low_ranked_champs %&amp;gt;%
  arrange(-ranking) %&amp;gt;%
  filter(!duplicated(player)) %&amp;gt;%
  mutate(match = paste(home, &amp;quot;vs&amp;quot;, away)) %&amp;gt;%
  select(date, player, team, nation, ranking, match, result)

head(lowest_ranked_champs, n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          date           player              team           nation ranking
## 1  2013-03-26      Jonny Evans manchester-united Northern Ireland     106
## 2  2001-11-14  Igors Stepanovs           arsenal           Latvia     101
## 3  2009-06-14          Manucho manchester-united           Angola      94
## 4  2003-06-03      Roy Carroll manchester-united Northern Ireland      87
## 5  2018-10-16     Riyad Mahrez   manchester-city          Algeria      85
## 6  2006-02-28 Eidur Gudjohnsen           chelsea          Iceland      82
## 7  2001-06-06      David Healy manchester-united Northern Ireland      82
## 8  2016-06-13       Wes Morgan    leicester-city          Jamaica      72
## 9  2000-09-02       Ryan Giggs manchester-united            Wales      70
## 10 2007-06-02   DONG Fang Zhou manchester-united            China      64
##                                 match result
## 1          Northern Ireland vs Israel    0:2
## 2                    Latvia vs Russia    1:3
## 3                    Angola vs Guinea    0:0
## 4           Italy vs Northern Ireland    2:0
## 5                    Benin vs Algeria    1:0
## 6        Trinidad &amp;amp; Tobago vs Iceland    2:0
## 7  Czech Republic vs Northern Ireland    3:1
## 8                  Uruguay vs Jamaica    3:0
## 9                    Belarus vs Wales    2:1
## 10                       USA vs China    4:1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However! One notable omission is Christopher Wreh, who played in 16 matches for Arsenal in the 1997/1998 season. Wreh only played 26 times for his nation so I assumed he just hadn’t appeared for them that year, but &lt;a href=&#34;https://www.11v11.com/players/christopher-wreh-4/&#34;&gt;11v11.com&lt;/a&gt; lists him playing on July 27th 1997 (1997/1998 season) at which point Liberia were ranked 110th in the world.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to Modelling Soccer Matches in R (part 1)</title>
      <link>/post/dixon_coles_1/</link>
      <pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/dixon_coles_1/</guid>
      <description>


&lt;p&gt;For anyone watching football, being able to predict matches is a key aspect of the hobby. Whether explicitly (e.g. when betting on matches, or deciding on recruitment for an upcoming season), or more implicitly when discussing favourites to win the league in the pub, almost all discussion of the sport on some level require predictions about some set of upcoming games.&lt;/p&gt;
&lt;p&gt;The first step of prediction is some form of quantification of ability. We’d expect a better team to have a better chance of winning than a worse team. For an example of a more sophisticated set of rankings, see &lt;a href=&#34;https://projects.fivethirtyeight.com/soccer-predictions/&#34;&gt;fivethirtyeight’s Soccer Power Index&lt;/a&gt; which is explicitly used to predict the results of various football competitions.&lt;/p&gt;
&lt;p&gt;The accuracy of our predictions therefore relies on the accuracy of our judgement on team’s ability. When discussing football with friends, we might use half-remembered match highlights to form some impression of how strong a team is. When programming however, we have free access to the results of teams thus far in a campaign and should be able to produce a model more grounded in truth.&lt;/p&gt;
&lt;p&gt;Two seminal papers for using recent football results to assess the abilities of football teams (and then use this assessment to predict matches) are &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9574.1982.tb00782.x&#34;&gt;Maher’s 1982 paper&lt;/a&gt; on modelling football scores, which is complimented by &lt;a href=&#34;https://www.jstor.org/stable/pdf/2986290.pdf?casa_token=9deLgF7xOaEAAAAA:fGGfUQKOsezrWBvbmphK56HddtiaohxaUNPdkDBoTApL_beghKXFlru5USztLt7dDVEMSdhAfkg8yzubZsAs7eeyZvp307iAGwqAtVSMMhwk6xhUleM&#34;&gt;Mark Dixon and Stuart Coles’ 1997 paper&lt;/a&gt;. For R various packages to use the methods outlined in these papers exist including &lt;a href=&#34;https://github.com/Torvaney/regista&#34;&gt;Ben Torvaney’s regista&lt;/a&gt;, &lt;a href=&#34;https://github.com/opisthokonta/goalmodel&#34;&gt;opisthokonta’s goalmodel&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, and &lt;a href=&#34;https://cran.r-project.org/web/packages/fbRanks/index.html&#34;&gt;Eli Holmes’ fbRanks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, the overlap between people obsessed enough with football to read mathematical papers on the sport, and those with the formal training in reading math notation to understand these models is fairly low, and I wasn’t able to find&lt;sup&gt;2&lt;/sup&gt; a good intuitive explanation for these models. Hopefully, building up these models from the most basic entry steps to a fully sophisticated model for predicting football matches might help some who want to start modelling football but don’t have the privilege of formal stats/modelling/coding training. As I want to start from pretty much zero, in this first post I make at least one or two claims that are not strictly true (indeed, this post does not actually implement some of the main points of the 1997 Dixon &amp;amp; Coles paper), but will try to point these out as I go, and correct them in later posts.&lt;/p&gt;
&lt;p&gt;First, let’s load libraries and also set a seed for the reproducibility of this document&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# munging
library(tidyverse)

# seed for reproducibility
set.seed(3459)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Set up&lt;/h2&gt;
&lt;p&gt;In reality, we’d probably want to model a whole league or cup. However, these can generally contain 20+ teams, many of which will have similar abilities. For simplicity here, lets instead imagine a summer league between 6 English football clubs where each team plays each other twice (once at home and once away)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;teams &amp;lt;- c(&amp;quot;Arsenal&amp;quot;, # 5th in the 1st tier
           &amp;quot;Blackburn_Rovers&amp;quot;, # 15th in 2nd tier
           &amp;quot;Coventry_City&amp;quot;, # 8th in 3rd tier
           &amp;quot;Dover_Athletic&amp;quot;, # 14th 5th tier 
           &amp;quot;Enfield_Town&amp;quot;, # 10th in 7th tier
           &amp;quot;Frimley_Green&amp;quot;) # 2nd in 9th tier&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ve managed to arrange a league that has a nice stratification between teams, so we’d expect each to be comfortably better than the next best (which will make sanity checking our results easier). Lucky for us, the teams are also in alphabetical order of strength so in case you don’t have any prior on a team, take the first letter of it’s name (A-F).&lt;/p&gt;
&lt;p&gt;Each week each team play one game, so we’ll have a fixture list that looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(fixtures, 8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               home             away gameweek
## 1    Frimley_Green          Arsenal        1
## 2     Enfield_Town Blackburn_Rovers        1
## 3   Dover_Athletic    Coventry_City        1
## 4          Arsenal     Enfield_Town        2
## 5    Frimley_Green   Dover_Athletic        2
## 6 Blackburn_Rovers    Coventry_City        2
## 7   Dover_Athletic          Arsenal        3
## 8    Coventry_City     Enfield_Town        3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously for this we’re going to have to make up our data. For the code used to generate it, see the bottom of the post.&lt;/p&gt;
&lt;p&gt;Let’s say that we’ve had 8 weeks of games played so far, and the results have been as follows&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(results,8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               home             away hgoal agoal gameweek
## 1   Dover_Athletic    Coventry_City     0     3        1
## 2     Enfield_Town Blackburn_Rovers     0     3        1
## 3    Frimley_Green          Arsenal     0     8        1
## 4          Arsenal     Enfield_Town     5     0        2
## 5 Blackburn_Rovers    Coventry_City     1     1        2
## 6    Frimley_Green   Dover_Athletic     1     2        2
## 7 Blackburn_Rovers    Frimley_Green     6     0        3
## 8    Coventry_City     Enfield_Town     2     1        3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A better way to show this is to generate a matrix of home (y axis) vs. away (x axis) and show the goals scored in each match between them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- results %&amp;gt;%
  # remove unplayed games
  filter(!is.na(hgoal)) %&amp;gt;%
  ggplot(., aes(x = away, y = home, fill = hgoal-agoal)) +
  geom_tile() +
  # add the scorelines
  geom_label(aes(label = paste(hgoal, agoal, sep = &amp;quot;-&amp;quot;)), fill = &amp;quot;white&amp;quot;) +
  # colour where green shows home win and red an away win
  scale_fill_gradient2(low = &amp;quot;darkred&amp;quot;, high = &amp;quot;green&amp;quot;, midpoint = 0, guide = FALSE) +
  scale_x_discrete(limits = levels(results$home), position = &amp;quot;top&amp;quot;) +
  scale_y_discrete(limits = rev(levels(results$away))) +
  theme_minimal()

# plot
p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-30-5-dixon-coles-1_files/figure-html/plot_results-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As the colour gradient (from bottom right to top left) shows, the teams we’d expect to do better are. Given the stochastic nature of football though, there are some surprises. E.g. Blackburn only managing to draw at home to Coventry.&lt;/p&gt;
&lt;p&gt;A good sense of teams relative abilities can be seen in the league table of results so far (assuming 3 points for a win, and 1 for a draw):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function to melt results
# returns df with team and goals for and against for each match
melt_results &amp;lt;- function(results_df) {
  results_df %&amp;gt;%
    # select only relevant columns
    select(home, away, hgoal, agoal) %&amp;gt;%
    gather(location, team,  -hgoal, -agoal) %&amp;gt;%
    # calculate goals for/against the team
    mutate(g_for = case_when(
      location == &amp;quot;home&amp;quot; ~ hgoal,
      location == &amp;quot;away&amp;quot; ~ agoal
    )) %&amp;gt;%
    mutate(g_ag = case_when(
      location == &amp;quot;home&amp;quot; ~ agoal,
      location == &amp;quot;away&amp;quot; ~ hgoal
    )) 
}

# function to calculate points won and gd for each team
results_to_table &amp;lt;- function(results_df) {
  results_df %&amp;gt;%
    # use above melting function
    melt_results(.) %&amp;gt;%
    # 3 points for a win, 1 for a draw
    mutate(points = case_when(
      g_for &amp;gt; g_ag ~ 3,
      g_ag &amp;gt; g_for ~ 0,
      g_for == g_ag ~ 1
    )) %&amp;gt;%
    # calculate goal difference for each match
    mutate(gd = g_for - g_ag) %&amp;gt;%
    group_by(team) %&amp;gt;%
    # get the final statistics per team
    summarise(games_played = n(),
              gf = sum(g_for),
              ga = sum(g_ag),
              gd = sum(gd),
              points = sum(points)) %&amp;gt;%
    arrange(-points, -gd, -gf)
}

# calculate league table for our played fixtures
league_table &amp;lt;- results  %&amp;gt;%
  filter(!is.na(hgoal)) %&amp;gt;%
  select(-gameweek) %&amp;gt;%
  results_to_table(.) %&amp;gt;%
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##   team             games_played    gf    ga    gd points
##   &amp;lt;chr&amp;gt;                   &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 Arsenal                     8    39     4    35     24
## 2 Blackburn_Rovers            8    23     6    17     19
## 3 Coventry_City               8    14     8     6     16
## 4 Dover_Athletic              8     8    15    -7      9
## 5 Enfield_Town                8     6    22   -16      3
## 6 Frimley_Green               8     2    37   -35      0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where teams positions are nicely rank ordered (the data for this example is fairly curated so it’s not that surprising).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predictions&lt;/h2&gt;
&lt;p&gt;With two rounds to go, there’s still 6 fixtures we might want to predict (to try and judge which team will end up where, or just to bet on the remaining games).&lt;/p&gt;
&lt;p&gt;This are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get the yet to be played matches
unplayed_games &amp;lt;- fixtures %&amp;gt;%
  filter(gameweek &amp;gt; 8) %&amp;gt;%
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               home             away gameweek
## 1    Coventry_City          Arsenal        9
## 2 Blackburn_Rovers   Dover_Athletic        9
## 3    Frimley_Green     Enfield_Town        9
## 4          Arsenal Blackburn_Rovers       10
## 5    Coventry_City    Frimley_Green       10
## 6   Dover_Athletic     Enfield_Town       10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to predict these results, we need to have data on the strength of the teams above, but also, a good prior on what sort of scores we should expect.&lt;/p&gt;
&lt;p&gt;Using real data from the engsoccerdata package we can get the results of all 48840 English football league games between August 1992 and May 2016. If we melt this to get the goals scored by each team by their location we get a data.frame of 97680 records of a teams performance in a game:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load real data from the english league
real_data &amp;lt;- engsoccerdata::england %&amp;gt;%
  # filter out &amp;#39;premier league era&amp;#39; matches
  filter(Season &amp;gt; 1991) %&amp;gt;%
  # select only relevant columns
  select(home, away = visitor, hgoal, agoal = vgoal) %&amp;gt;%
  # munge
  melt_results() %&amp;gt;%
  select(-hgoal, -agoal) %&amp;gt;%
  mutate(data = &amp;quot;real&amp;quot;)

head(real_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   location    team g_for g_ag data
## 1     home Arsenal     0    1 real
## 2     home Arsenal     0    1 real
## 3     home Arsenal     2    1 real
## 4     home Arsenal     3    0 real
## 5     home Arsenal     3    0 real
## 6     home Arsenal     2    0 real&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here every row shows a team that played a match (as it’s sorted by league then alphabetically, the first 6 records are all for Arsenal). It also shows if the team played home or away. The data also shows the goals scored by (e.g.) Arsenal in g_for, and the goals they conceded in g_ag.&lt;/p&gt;
&lt;p&gt;If we plot the goals scored for each game, we get a nice humped distribution with slightly offset peaks for home and away. That is to say, in most games teams will score 0, 1, or 2 goals, and that scoring more than 6 goals in a match is incredibly rare. The difference between the home and away distributions mean that teams are slightly more likely to score more if playing at home, compared to play away from home.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot goals scored home/away for real english football matches
p2 &amp;lt;- real_data %&amp;gt;%
  ggplot(., aes(x = g_for, fill = location)) +
  # smooth densities
  geom_density(adjust = 8, alpha = 0.5) +
  scale_fill_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;)) +
  scale_x_continuous(breaks = 0:6) +
  labs(title = &amp;quot;Goals scored at home and away in English football&amp;quot;,
       subtitle = &amp;quot;data from 48.8k matches 1992-2016&amp;quot;,
       x = &amp;quot;goals scored&amp;quot;,
       y = &amp;quot;density&amp;quot;) +
  theme_minimal()

# plot
p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-30-5-dixon-coles-1_files/figure-html/plot_real_goal_distributions-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can work out what the average difference between playing at home and away is by taking the means of goals scored at home, and when playing away:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculate mean home and away goals
real_data_means &amp;lt;- real_data %&amp;gt;%
    group_by(location) %&amp;gt;%
    summarise(mean_scored = mean(g_for)) %&amp;gt;%
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   location mean_scored
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 away            1.11
## 2 home            1.48&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Goals in games are both relatively sparse, and relatively stochastic; football is a low scoring game where goals are evenly distributed throughout the game. In theory any attack made by a team i has a probability of being scored dependent upon the strength of team i’s attack (α&lt;sub&gt;i&lt;/sub&gt;) which is independent of all the other attacks that team has made.&lt;/p&gt;
&lt;p&gt;(there is some reason to doubt this may be the case&lt;sup&gt;3&lt;/sup&gt;, but for now this is a fine generalisation)&lt;/p&gt;
&lt;p&gt;By grouping all teams together into “home” and “away” categories (in a league setting each team will play each other home and away so this should average out) and taking the average number of goals scored per match as the Poisson mean (λ) we can see how well our above graph fits a simulated Poisson process.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# generate Poisson distributed vector with mean = real world mean
simulated_poisson &amp;lt;- real_data_means %&amp;gt;%
  split(f = .$location) %&amp;gt;%
  lapply(., function(x) df = data.frame(dist = rpois(100000, x$mean_scored),
                                        location = x$location)) %&amp;gt;%
  # map it all together and label
  map_df(I) %&amp;gt;%
  mutate(data = &amp;quot;simulated&amp;quot;) 

# add these distributions to the plot
p2 + geom_density(data = simulated_poisson, aes(x = dist),
                  fill = NA, adjust = 8, alpha = 0.2) +
  scale_fill_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), guide = FALSE) +
  facet_wrap(~location)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-30-5-dixon-coles-1_files/figure-html/simulated_poisson-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s not perfect, but it’s not a bad fit either. We can quantify how well the Poisson distribution fits the data using a &lt;a href=&#34;https://stats.stackexchange.com/questions/92627/how-to-use-the-chi-squared-test-to-determine-if-data-follow-the-poisson-distribu&#34;&gt;Chi Squared test&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calc chi squared for home and away goals following Poisson distribution
calc_chi_squared &amp;lt;- function(game_location) {
  goals_scored &amp;lt;- filter(real_data, location == game_location)$g_for
  
  observed_goal_counts &amp;lt;- table(goals_scored)

  mean_goals &amp;lt;- mean(goals_scored)
  
  probs = dpois(sort(unique(goals_scored)), lambda = mean_goals) %&amp;gt;%
    append(., 1-sum(.))
  
  # the chi squared test
  test &amp;lt;- chisq.test(x = c(observed_goal_counts,0), p = probs, simulate.p.value = TRUE)
  test$data.name &amp;lt;- game_location
  
  return(test)
}

# run test for both home and away goals
lapply(c(&amp;quot;home&amp;quot;, &amp;quot;away&amp;quot;), calc_chi_squared)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## 
##  Chi-squared test for given probabilities with simulated p-value
##  (based on 2000 replicates)
## 
## data:  home
## X-squared = 53.752, df = NA, p-value = 0.001499
## 
## 
## [[2]]
## 
##  Chi-squared test for given probabilities with simulated p-value
##  (based on 2000 replicates)
## 
## data:  away
## X-squared = 38.599, df = NA, p-value = 0.01599&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s actually perhaps not as significant as might be expected given the sheer amount of observations we have (see above reservations about modelling goals as a Poisson process) but it’s clearly not the worst approximation either. The p-values &amp;lt; 0.05 for both home and away match data show we have a good reason to reject the null hypothesis that the data is not a Poisson distribution.&lt;/p&gt;
&lt;p&gt;If we think that goals scored represents some Poisson process, it can be modeled using the equation which underlies the Poisson distribution. For a given interval (one match), the probability of x events (goals scored) in that interval will be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(x) = \frac{\lambda^{x}e^{-\lambda}}{x!}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The simplest model we can produce is to estimate λ as each team’s attack rating (henceforth α&lt;sub&gt;i&lt;/sub&gt;) which is equal to observed mean rate of goals for that team.&lt;/p&gt;
&lt;p&gt;That is the say the probability of team i scoring x goals against team j is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(x_{i,j} = x) = \frac{\alpha_{i}^{x}e^{-\alpha_{i}}}{x!}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where α&lt;sub&gt;i&lt;/sub&gt; is the sum of all goals scored divided by the total number of matches:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\alpha_{i} = \frac{1}{N}\sum_{n=1}^{N} x\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;grouping by teams makes this easy to calculate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;basic_model &amp;lt;- results %&amp;gt;%
  melt_results() %&amp;gt;%
  group_by(team) %&amp;gt;%
  # we&amp;#39;ll use the goals scored to model the attack
  # and goals conceeded to measure defence rating
  summarise(alpha = mean(g_for),
            beta = mean(g_ag)) %&amp;gt;%
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   team             alpha  beta
##   &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 Arsenal           4.88  0.5 
## 2 Blackburn_Rovers  2.88  0.75
## 3 Coventry_City     1.75  1   
## 4 Dover_Athletic    1     1.88
## 5 Enfield_Town      0.75  2.75
## 6 Frimley_Green     0.25  4.62&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(we’ll come on to the beta parameter in a bit- where alpha is the average scoring rate, beta is the average conceding rate).&lt;/p&gt;
&lt;p&gt;If we take Coventry’s remaining two games as examples we can see that they are yet to play Arsenal and Frimley Green at home&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coventry_games &amp;lt;- unplayed_games %&amp;gt;%
  # filter out Coventry City&amp;#39;s remaining fixtures
  filter(grepl(&amp;quot;Coventry_City&amp;quot;, home)) %&amp;gt;%
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            home          away gameweek
## 1 Coventry_City       Arsenal        9
## 2 Coventry_City Frimley_Green       10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we can take the attack rating (α) of each team and use it to estimate the results&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get the attack ratings of all teams
team_alphas &amp;lt;- basic_model$alpha %&amp;gt;% `names&amp;lt;-`(basic_model$team)

# assume goals scored for each team will be it&amp;#39;s attack rating
e_results &amp;lt;- paste(team_alphas[coventry_games$home],
                   team_alphas[coventry_games$away],
                   sep = &amp;quot;-&amp;quot;) %&amp;gt;%
  # name each match with the teams competing
  `names&amp;lt;-`(c(paste(coventry_games$home, coventry_games$away, sep = &amp;quot;-&amp;quot;))) %&amp;gt;%
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Coventry_City-Arsenal Coventry_City-Frimley_Green 
##                &amp;quot;1.75-4.875&amp;quot;                 &amp;quot;1.75-0.25&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These aren’t ridiculous estimates by any stretch but it’s clear something is up. It’s pretty intuitive that Coventry City would be expected to score more goals at home to Frimley Green than at home to Arsenal.&lt;/p&gt;
&lt;p&gt;We can account for this by introducing an opposing team defence parameter β&lt;sub&gt;j&lt;/sub&gt;. In our very simple model this will be estimating by taking the average rate a team concedes goals. As with the attack rating, this is the calculated as the sum of all goals conceded divided by number of matches. We’ll then multiply α&lt;sub&gt;i&lt;/sub&gt; and β&lt;sub&gt;j&lt;/sub&gt; together to get the score estimate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get and name the defence rating for each team
team_betas &amp;lt;- basic_model$beta %&amp;gt;% `names&amp;lt;-`(basic_model$team)

# assume the goals scored will be the attack rating of the team times 
# the defence rating of it&amp;#39;s opponent
e_results &amp;lt;- paste(round(team_alphas[coventry_games$home]*
                           team_betas[coventry_games$away], 3),
                   round(team_alphas[coventry_games$away]*
                           team_betas[coventry_games$home], 3),
                   sep = &amp;quot;-&amp;quot;) %&amp;gt;%
  `names&amp;lt;-`(c(paste(coventry_games$home, coventry_games$away, sep = &amp;quot;-&amp;quot;))) %&amp;gt;%
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Coventry_City-Arsenal Coventry_City-Frimley_Green 
##               &amp;quot;0.875-4.875&amp;quot;                &amp;quot;8.094-0.25&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The opposition scores remain the same because Coventry have on average conceded 1 goal per game.&lt;/p&gt;
&lt;p&gt;Coventry’s predicted goals though has diverged with them now predicted to score less than a goal against Arsenal and to score 8(!) against Frimley Green, both of which sound reasonable (when you consider that Frimley Green are a team of amateurs).&lt;/p&gt;
&lt;p&gt;However, we’re also missing one final piece of the model we’ll finish with today. Recall modelling the English football data from 1992 onwards, we were left with a difference between the home scoring rate and the away scoring rate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# reprint what we calculated earlier
real_data_means&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   location mean_scored
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 away            1.11
## 2 home            1.48&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s pretty common knowledge that football teams do better at home, so we’ll want to factor that in. A simple estimate is to divide the mean home goals/game by the mean away goals/game.&lt;/p&gt;
&lt;p&gt;We’ll call this parameter γ and can be formalised as the sum of home goals (which we’ll refer to as x from now on) divided by the sum of away goals (y)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\gamma = \frac{\sum{x}}{\sum{y}}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the home advantage is how much easier it is to score at home
home_advantage_gamma &amp;lt;- sum(results$hgoal) / sum(results$agoal)

e_results &amp;lt;- paste(round(team_alphas[coventry_games$home]*
                           team_betas[coventry_games$away] * 
                           # add in home advantage for home team
                           home_advantage_gamma, 3),
                   round(team_alphas[coventry_games$away]*
                           team_betas[coventry_games$home], 3),
                   sep = &amp;quot;-&amp;quot;) %&amp;gt;%
  `names&amp;lt;-`(c(paste(coventry_games$home, coventry_games$away, sep = &amp;quot;-&amp;quot;))) %&amp;gt;%
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Coventry_City-Arsenal Coventry_City-Frimley_Green 
##               &amp;quot;0.955-4.875&amp;quot;                 &amp;quot;8.83-0.25&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which tilts the scales a little towards Coventry’s favour but (as we’d expect- home advantage can only go so far) doesn’t affect the results too much.&lt;/p&gt;
&lt;p&gt;Now we have a method to predict matches, we can use this on the remaining 6 nice and easily:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simplify to just gamma
gamma &amp;lt;- home_advantage_gamma

# wrap the above into a function for home and away teams
predict_results &amp;lt;- function(home, away, parameters) {
  e_goals_home &amp;lt;- parameters$alpha[home]*parameters$beta[away] * gamma
  e_goals_away &amp;lt;- parameters$alpha[away]*parameters$beta[home]
  
  # output a df of expected goals for home and away teams
  df &amp;lt;- data.frame(home = home, away = away,
                   e_hgoal = e_goals_home, e_agoal = e_goals_away)
  return(df)
}

# convert the basic_model df into a list with $attack and $defence parameters
# for each team
basic_parameters &amp;lt;- basic_model %&amp;gt;%
  # rename scored/conceeded to attack/defence
  select(-team) %&amp;gt;%
  # convert to a list and name each element
  as.list() %&amp;gt;%
  lapply(., function(x){names(x) &amp;lt;- teams;return(x)})

# predict results using the function defined above and the list of parameters
# could use e.g. mapply here but I prefer the map2 grammar
# run the predict results function over each game consisting of $home and $away
predicted_fixtures &amp;lt;- map2_df(unplayed_games$home, unplayed_games$away, 
                    predict_results,
                    # parameters forms an extra argument that does not vary
                    basic_parameters) %&amp;gt;%
  # round the outputs
  mutate_if(is.numeric, round, digits = 2) %&amp;gt;%
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               home             away e_hgoal e_agoal
## 1    Coventry_City          Arsenal    0.95    4.88
## 2 Blackburn_Rovers   Dover_Athletic    5.88    0.75
## 3    Frimley_Green     Enfield_Town    0.75    3.47
## 4          Arsenal Blackburn_Rovers    3.99    1.44
## 5    Coventry_City    Frimley_Green    8.83    0.25
## 6   Dover_Athletic     Enfield_Town    3.00    1.41&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of which look reasonable, if maybe a little bullish on the ‘better’ teams prospects.&lt;/p&gt;
&lt;p&gt;However, while this is good for back of the envelope predictions, we know that this is a very basic model. If we want to improve it, first we must quantify how good it is.&lt;/p&gt;
&lt;p&gt;In order to do this we can use the results we have from the first 8 weeks of matches as training data. We know what the ‘correct’ scores are for these matches, so if our model is good, it will predict similar scores to those observed.&lt;/p&gt;
&lt;p&gt;Remember that for the Poisson distribution, the probability of x goals in one match is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(x) = \frac{\lambda^{x}e^{-\lambda}}{x!}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The expected value of the Poisson distribution is equal to λ, so we can plug λ as our predicted goals, and x as the actual goals, and calculate the probability of that results occurring &lt;em&gt;given&lt;/em&gt; the attack/defence/home advantage parameters that we think are correct.&lt;/p&gt;
&lt;p&gt;We then do this for all the matches played and get the likelihood for the home and away teams scores given the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# &amp;#39;predict&amp;#39; the already played matches using our function
predicted_results &amp;lt;- map2_df(results$home, results$away, 
                    predict_results,
                    basic_parameters) %&amp;gt;%
  mutate_if(is.numeric, round, digits = 2) %&amp;gt;%
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                home             away e_hgoal e_agoal
## 1    Dover_Athletic    Coventry_City    1.09    3.28
## 2      Enfield_Town Blackburn_Rovers    0.61    7.91
## 3     Frimley_Green          Arsenal    0.14   22.55
## 4           Arsenal     Enfield_Town   14.62    0.38
## 5  Blackburn_Rovers    Coventry_City    3.14    1.31
## 6     Frimley_Green   Dover_Athletic    0.51    4.62
## 7  Blackburn_Rovers    Frimley_Green   14.51    0.19
## 8     Coventry_City     Enfield_Town    5.25    0.75
## 9    Dover_Athletic          Arsenal    0.55    9.14
## 10          Arsenal    Coventry_City    5.32    0.88
## 11   Dover_Athletic Blackburn_Rovers    0.82    5.39
## 12     Enfield_Town    Frimley_Green    3.78    0.69
## 13 Blackburn_Rovers          Arsenal    1.57    3.66
## 14     Enfield_Town   Dover_Athletic    1.53    2.75
## 15    Frimley_Green    Coventry_City    0.27    8.09
## 16          Arsenal    Frimley_Green   24.60    0.12
## 17 Blackburn_Rovers     Enfield_Town    8.62    0.56
## 18    Coventry_City   Dover_Athletic    3.58    1.00
## 19    Coventry_City Blackburn_Rovers    1.43    2.88
## 20   Dover_Athletic    Frimley_Green    5.05    0.47
## 21     Enfield_Town          Arsenal    0.41   13.41
## 22          Arsenal   Dover_Athletic    9.97    0.50
## 23     Enfield_Town    Coventry_City    0.82    4.81
## 24    Frimley_Green Blackburn_Rovers    0.20   13.30&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculate the likelihood of each home/away team actually scoring that many goals
# given the parameters for attack/defence supplied
likelihoods &amp;lt;- data.frame(lik_hgoal = dpois(results$hgoal,
                                            predicted_results$e_hgoal),
                          lik_agoal = dpois(results$agoal,
                                            predicted_results$e_agoal)) %&amp;gt;%
  # round the probabilities
  mutate_all(round, 4) %&amp;gt;%
  # bind likelihoods to results
  cbind(results, . ) %&amp;gt;%
  # bind in predictions
  left_join(., predicted_results, by = c(&amp;quot;home&amp;quot;, &amp;quot;away&amp;quot;)) %&amp;gt;%
  # select useful parameters
  select(home, away, hgoal, e_hgoal, lik_hgoal, agoal, e_agoal, lik_agoal) %&amp;gt;%
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                home             away hgoal e_hgoal lik_hgoal agoal e_agoal
## 1    Dover_Athletic    Coventry_City     0    1.09    0.3362     3    3.28
## 2      Enfield_Town Blackburn_Rovers     0    0.61    0.5434     3    7.91
## 3     Frimley_Green          Arsenal     0    0.14    0.8694     8   22.55
## 4           Arsenal     Enfield_Town     5   14.62    0.0025     0    0.38
## 5  Blackburn_Rovers    Coventry_City     1    3.14    0.1359     1    1.31
## 6     Frimley_Green   Dover_Athletic     1    0.51    0.3063     2    4.62
## 7  Blackburn_Rovers    Frimley_Green     6   14.51    0.0065     0    0.19
## 8     Coventry_City     Enfield_Town     2    5.25    0.0723     1    0.75
## 9    Dover_Athletic          Arsenal     1    0.55    0.3173     3    9.14
## 10          Arsenal    Coventry_City     3    5.32    0.1228     1    0.88
## 11   Dover_Athletic Blackburn_Rovers     1    0.82    0.3612     2    5.39
## 12     Enfield_Town    Frimley_Green     1    3.78    0.0863     0    0.69
## 13 Blackburn_Rovers          Arsenal     0    1.57    0.2080     2    3.66
## 14     Enfield_Town   Dover_Athletic     1    1.53    0.3313     2    2.75
## 15    Frimley_Green    Coventry_City     0    0.27    0.7634     3    8.09
## 16          Arsenal    Frimley_Green    10   24.60    0.0005     0    0.12
## 17 Blackburn_Rovers     Enfield_Town     4    8.62    0.0415     0    0.56
## 18    Coventry_City   Dover_Athletic     1    3.58    0.0998     0    1.00
## 19    Coventry_City Blackburn_Rovers     1    1.43    0.3422     2    2.88
## 20   Dover_Athletic    Frimley_Green     2    5.05    0.0817     0    0.47
## 21     Enfield_Town          Arsenal     2    0.41    0.0558     4   13.41
## 22          Arsenal   Dover_Athletic     4    9.97    0.0193     0    0.50
## 23     Enfield_Town    Coventry_City     1    0.82    0.3612     2    4.81
## 24    Frimley_Green Blackburn_Rovers     1    0.20    0.1637     5   13.30
##    lik_agoal
## 1     0.2213
## 2     0.0303
## 3     0.0003
## 4     0.6839
## 5     0.3535
## 6     0.1052
## 7     0.8270
## 8     0.3543
## 9     0.0137
## 10    0.3650
## 11    0.0663
## 12    0.5016
## 13    0.1724
## 14    0.2417
## 15    0.0271
## 16    0.8869
## 17    0.5712
## 18    0.3679
## 19    0.2328
## 20    0.6250
## 21    0.0020
## 22    0.6065
## 23    0.0943
## 24    0.0058&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we sum the log of those likelihood values we get a measure of how wrong overall our predictions are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_likehood &amp;lt;- sum(log(likelihoods$lik_hgoal), log(likelihoods$lik_agoal)) * -1

log_likehood&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 105.995&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(n.b. there will be some rounding errors- especially on the pre-log probabilities, but this will suffice for now)&lt;/p&gt;
&lt;p&gt;To get an idea of whether or not this is good, let’s quickly run the model with all the parameters set to zero. Given that we’re pretty sure that at least Arsenal will be a lot better than Frimley Green, this model should do worse than our basic model above.&lt;/p&gt;
&lt;p&gt;If it indeed does fit the results worse we will get a greater error term- the log likelihood sum&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# do the same but set each teams attack and defence to 1
# expect model to be worse as assumes all teams are equal
equal_parameters &amp;lt;- list(
  alpha = rep(1, length(teams)) %&amp;gt;% `names&amp;lt;-`(teams),
  beta = rep(1, length(teams)) %&amp;gt;% `names&amp;lt;-`(teams)
)

# predict results and munge through to find sum of log likelihoods
worse_log_likelihood &amp;lt;- map2_df(results$home, results$away, 
                    predict_results,
                    equal_parameters) %&amp;gt;%
  mutate_if(is.numeric, round, digits = 2) %&amp;gt;%
  # take the log probability straight away this time
  mutate(lik_hgoal = dpois(results$hgoal, e_hgoal, log = TRUE),
         lik_agoal = dpois(results$agoal, e_agoal, log = TRUE)) %&amp;gt;%
  select(lik_hgoal, lik_agoal) %&amp;gt;%
  map_dbl(sum) %&amp;gt;%
  sum(.) * -1 

worse_log_likelihood&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 112.618&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The worse log likelihood (112.6) is worse (only a bit though) than the 106.0 we previously. This suggests that either the teams are actually quite equal, or that our basic model wasn’t all that good.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameter-optimisation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameter Optimisation&lt;/h2&gt;
&lt;p&gt;There will exist some parameters (α and β for each team, and γ for the home field advantage) that will minimise this negative log likelihood. That is to say, they will predict the results of the already played games most accurately.&lt;/p&gt;
&lt;p&gt;If we want to find those we can use the optim() function in the stats package. This will take a vector of parameters and iterate while slightly changing their values until it gets the lowest value it can find as the output for a supplied function. It also takes a data.frame of results between teams. The results of these games are predicted and then checked against this actually observed data.&lt;/p&gt;
&lt;p&gt;At the end, I’ve also set the function to pass some information from each iteration into the global environment, namely, the iteration number (i), the parameter values the optim() function has chosen for this iteration, and the negative log likelihood of those parameters- the likelihood of the observed scores if those parameters are correct.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;optimise_params &amp;lt;- function(parameters, results) {
  # form the parameters back into a list
  # parameters names alpha (attack), beta (defense), and gamma (hfa)
  param_list &amp;lt;- relist_params(parameters)
  
  # predict the expected results for the games that have been played
  e_results &amp;lt;- map2_df(results$home, results$away, 
                      predict_results,
                      param_list)
  
  # calculate the negative log likelihood of those predictions
  # given the parameters how likely are those scores
  neg_log_likelihood &amp;lt;- calculate_log_likelihood(results, e_results)
  
  # capture the parameters and likelihood at each loop
  # only do it if i is initialised
  if(exists(&amp;quot;i&amp;quot;)) {
    i &amp;lt;&amp;lt;- i + 1
    current_parameters[[i]] &amp;lt;&amp;lt;- parameters
    current_nll[[i]] &amp;lt;&amp;lt;- neg_log_likelihood
  }
  
  # return the value to be minimised
  # in this case the negative log likelihood
  return(neg_log_likelihood)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The three separate functions are coded out separately so we can tinker with them shortly:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;to predict our results we have been supplying a list of two elements: alpha and beta, each of which are numeric vectors. optim() can only take one vector to optimise over but we can trick it by supplying unlist(&lt;code&gt;list_of_parameters&lt;/code&gt;). If we do this we then first want to convert this unlisted numeric vector back into our two element list*&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;*it isn’t vital to have the parameters arranged like this, but I think it leads to neater indexing when predicting the results&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;we then need to use these parameters to predict the results of past games. For each home and away team in a data.frame of results we can predict the expected home and expected away goals. These are then bound into a data.frame of home and away teams and these predicted goals for each&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;finally, we need to calculate the negative log likelihood by calculating the log probability of the observed goals given the predicted goals and summing these. We then multiply this by -1 as the sum of the log probabilities will be negative and we want to minimise this number as close to zero as possible. The transformation of prod(neg_log_likelihood, -1) is a quick hack for this&lt;sup&gt;4&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Hopefully this is at least bearable to follow. Formalised, this can be written for teams i and matches k as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathcal L(\alpha_{i},\beta_{i},\gamma;i = 1 ... n) = \prod_{k = 1}^{K}{\frac{\lambda_{k}^{x_{k}}e^{-\lambda_{k}}}{x_{k}!}\frac{\mu_{k}^{y_{k}}e^{-\mu_{k}}}{y_{k}!}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where for match k and teams i and j, home goals, x is defined by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_{k} \sim Poisson(\lambda_{k} = \alpha_{i(k)}\beta_{j(k)}\gamma)\]&lt;/span&gt; and away goals, y&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{k} \sim Poisson(\mu_{k} = \alpha_{j(k)}\beta_{i(k)})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which seems daunting when you write it down, but we’ve already covered everything we need to do solve it. It’s just saying we want to minimise the result of the multiplication (the sum of logs in our case above) of the probability of scoring x and y goals in a game. The probability of goals scored assumed to be Poisson distributed, controlled by parameters α, β, and γ for home and away teams.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optim requires parameters to be supplied as a vector
# we&amp;#39;ll unlist the parameters then relist in the function
relist_params &amp;lt;- function(parameters) {
  parameter_list &amp;lt;- list(
    # alpha = attack rating
    alpha = parameters %&amp;gt;%
      .[grepl(&amp;quot;alpha&amp;quot;, names(.))] %&amp;gt;%
      `names&amp;lt;-`(teams),
    # beta = defence rating
    beta = parameters %&amp;gt;%
      .[grepl(&amp;quot;beta&amp;quot;, names(.))] %&amp;gt;%
      `names&amp;lt;-`(teams),
    # gamma = home field advantage
    gamma = parameters[&amp;quot;gamma&amp;quot;]
  )
  
  return(parameter_list)
}

# use these parameters to predict results for supplied matches
predict_results &amp;lt;- function(home, away, param_list) {
  # expected home goals
  e_goals_home &amp;lt;- param_list$alpha[home] * param_list$beta[away] * param_list$gamma
  # expected away goals
  e_goals_away &amp;lt;- (param_list$alpha[away] * param_list$beta[home])
  
  # bind to df
  df &amp;lt;- data.frame(home = home, away = away,
                   e_hgoal = e_goals_home, e_agoal = e_goals_away)
  
  return(df)
}

# calculate the log likelihood of predict results vs supplied results
calculate_log_likelihood &amp;lt;- function(results, e_results) {
  home_likelihoods = dpois(results$hgoal, lambda = e_results$e_hgoal, log = TRUE)
  away_likelihoods = dpois(results$agoal, lambda = e_results$e_agoal, log = TRUE)
  
  # sum log likelihood and multiply by -1 so we&amp;#39;re minimising neg log likelihood
  likelihood_sum &amp;lt;- sum(home_likelihoods, away_likelihoods)
  neg_log_likelihood &amp;lt;- prod(likelihood_sum, -1)
  
  return(neg_log_likelihood)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll supply parameters that are all equal to 1 to optim to stop it falling into local minima that might affect the ‘optimal’ parameters it finds. The unlisted parameters are then supplied to optim along with the optimise_parameters() function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# start with all parameters equal
equal_parameters &amp;lt;- list(
  alpha = rep(1, length(teams)) %&amp;gt;% `names&amp;lt;-`(teams),
  beta = rep(1, length(teams)) %&amp;gt;% `names&amp;lt;-`(teams),
  gamma = 1
)

# run optim over the functions with these initial parameters
optimised_parameters &amp;lt;- optim(
  # the equal initial parameters
  par = unlist(equal_parameters),
  # run over the function to optimise parameters
  fn = optimise_params,
  # extra arguments to function
  results = results,
  # Nelder-Mead equation with 10k iterations max
  method = &amp;quot;Nelder-Mead&amp;quot;,
  control = list(maxit = 10000)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can take the $par element of the output of this to find the parameters for which the negative log likelihood is minimised&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# display the parameters found to minimise
# the negative log likelihood
optimised_parameters$par&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          alpha.Arsenal alpha.Blackburn_Rovers    alpha.Coventry_City 
##              2.9858302              1.8014838              1.2995271 
##   alpha.Dover_Athletic     alpha.Enfield_Town    alpha.Frimley_Green 
##              0.8192267              0.7762002              0.2748448 
##           beta.Arsenal  beta.Blackburn_Rovers     beta.Coventry_City 
##              0.4738011              0.6346112              0.7503864 
##    beta.Dover_Athletic      beta.Enfield_Town     beta.Frimley_Green 
##              1.2208768              1.5180931              2.5535961 
##                  gamma 
##              1.1663125&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, alpha decreases as teams get worse, and beta increases. The found gamma (1.166) is only marginally higher than the 1.091 for our simple model.&lt;/p&gt;
&lt;p&gt;The $value element gives the negative log likelihood calculated for these parameters&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;optimised_parameters$value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 57.5175&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which is much smaller than the ~100 we got from our very basic model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tinkering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tinkering&lt;/h2&gt;
&lt;p&gt;This is all very well but there’s still some small improvements we can make.&lt;/p&gt;
&lt;p&gt;For starters, I always think it’s simpler to have both scales of α and β to increase as a teams becomes more skillful in attack or defence. In our original equation the expected home and away goals follow the formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_{ij} \sim Poisson(α_{i}β_{j}γ)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[y_{ij} \sim Poisson(α_{j}β_{i})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;if instead of multiplying by β, we divide instead, a stronger defence will reduce the value of x&lt;sub&gt;ij&lt;/sub&gt;/y&lt;sub&gt;ij&lt;/sub&gt; (reducing the number of expected goals for the opposing team).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_{ij} \sim Poisson(\frac{α_{i}γ}{β_{j}})\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[y_{ij} \sim Poisson(\frac{α_{j}}{β_{i}})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To achieve this we just have to flip two lines of the predict_results function. Instead of multiplying α and β, we divide them instead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# change prediction to inverse defence parameters
predict_results &amp;lt;- function(home, away, param_list) {
  e_goals_home &amp;lt;- (param_list$alpha[home] / param_list$beta[away]) * param_list$gamma
  e_goals_away &amp;lt;- (param_list$alpha[away] / param_list$beta[home])
  
  df &amp;lt;- data.frame(home = home, away = away,
                   e_hgoal = e_goals_home, e_agoal = e_goals_away)
  
  return(df)
}

# re run using new subfunction
optimised_parameters2 &amp;lt;- optim(
  par = unlist(equal_parameters),
  fn = optimise_params,
  results = results,
  method = &amp;quot;Nelder-Mead&amp;quot;,
  control = list(maxit = 10000))

# check this does what we want
optimised_parameters2$par&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(n.b. I won’t print out the results of all these steps as this post is long enough, but you can run and see the gradual improvements for yourself)&lt;/p&gt;
&lt;p&gt;Next we want to subtly change how the expected goals are calculated.&lt;/p&gt;
&lt;p&gt;Given that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ A = \frac{B \cdot C}{D}\]&lt;/span&gt; is exactly the same as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ A = e ^{log(B) + log(C) - log(D)}\]&lt;/span&gt; we can convert the parameters we are looking for into log(parameters) and take the exponent of their sum as the predicted goals. This might seem like a minor change, but prevents an important exception. Using home goals as an example, remember that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_{ij} \sim Poisson(\frac{α_{i}γ}{β_{j}})\]&lt;/span&gt; if any of the three parameters become negative then we’re left with a Poisson distribution with a negative mean, which is is absurd: events cannot unhappen. For instance, imagine a football game where one team scores negative goals.&lt;/p&gt;
&lt;p&gt;If we take the log parameters instead we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_{ij} \sim Poisson(e ^ {α_{i} - β_{j} + γ})\]&lt;/span&gt; where no matter what values α, β, or γ take, the exponent of their sum will never be negative. When playing a very strong away teams, the mean goals will tend towards 0 (though will never actually reach it).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# change prediction to use log parameters
# exp(log(x) + log(y)) = x * y
predict_results &amp;lt;- function(home, away, param_list) {
  e_goals_home &amp;lt;- exp(param_list$alpha[home] - param_list$beta[away] + param_list$gamma)
  e_goals_away &amp;lt;- exp(param_list$alpha[away] - param_list$beta[home])
  
  df &amp;lt;- data.frame(home = home, away = away,
                   e_hgoal = e_goals_home, e_agoal = e_goals_away)
  
  return(df)
}

# initialise parameters as all 0
# log(1) = 0
equal_parameters &amp;lt;- list(
  alpha = rep(0, length(teams)) %&amp;gt;% `names&amp;lt;-`(teams),
  beta = rep(0, length(teams)) %&amp;gt;% `names&amp;lt;-`(teams),
  gamma = 0
)

# re run using new subfunction
optimised_parameters3 &amp;lt;- optim(
  par = unlist(equal_parameters), 
  fn = optimise_params,
  results = results,
  # using log will avoid non-finite differences 
  # so can use BFGS model
  method = &amp;quot;BFGS&amp;quot;,
  control = list(maxit = 10000))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ve also switched optimisation algorithm from Nelder-Mead to BFGS. BFGS is &lt;a href=&#34;https://docs.mantidproject.org/v3.7.1/concepts/FittingMinimizersComparisonDetailed.html#minimizers-unweighted-comparison-in-terms-of-accuracy&#34;&gt;quicker&lt;/a&gt; than Nelder-Mead but requires the minimisation function (i.e. the negative log likelihood we calculate) to be finite. Before, we could get infinite negative log likelihoods, as it was possible to calculate a negative mean (expected goals for a team). Running dpois() for a negative lambda will return NaN so it becomes impossible to calculate the final negative log likelihood.&lt;/p&gt;
&lt;p&gt;Finally, we want to constrain the final optimised parameters by fixing the sum of all attack parameters, and the sum of all defence parameters, to equal 0. In practice, this basically means that above average attacking/defending teams will have parameters above 0, and below average teams will have parameters below 0. This is handy, but also the main advantage is this prevents &lt;a href=&#34;https://en.wikipedia.org/wiki/Overfitting&#34;&gt;overfitting&lt;/a&gt; of the parameters by the optimisation algorithm.&lt;/p&gt;
&lt;p&gt;To do this, we can simply drop the first (or last, or any, it doesn’t matter) parameter from attack or defence (the parameters for Arsenal) and then calculate Arsenal’s parameters as the sum of the remaining parameters multiplied by minus 1.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\alpha_{n} = -\sum_{i = 1}^{n-1} \alpha_{i} \]&lt;/span&gt; and also&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\beta_{n} = -\sum_{i = 1}^{n-1} \beta_{i} \]&lt;/span&gt; In terms of code this just requires adding one line to the relist_params() function to append the value back. We also then need to remove this parameter that we will add back in from the initial parameters which is done below.&lt;/p&gt;
&lt;p&gt;Our output will now be missing the parameters for Arsenal (as they will only exist within the function), but we can easily calculate it from the parameters we do get out.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# introduce sum to zero constraint by calculating
# first teams parameters as minus sum of the rest
relist_params &amp;lt;- function(parameters) {
  parameter_list &amp;lt;- list(
    alpha = parameters %&amp;gt;%
      .[grepl(&amp;quot;alpha&amp;quot;, names(.))] %&amp;gt;%
      append(prod(sum(.), -1), .) %&amp;gt;%
      `names&amp;lt;-`(teams),
    beta = parameters %&amp;gt;%
      .[grepl(&amp;quot;beta&amp;quot;, names(.))] %&amp;gt;%
      append(prod(sum(.), -1), .) %&amp;gt;%
      `names&amp;lt;-`(teams),
    gamma = parameters[&amp;quot;gamma&amp;quot;]
  )
  
  return(parameter_list)
}

# remove the first team from the attack and defence ratings
equal_parameters &amp;lt;- list(
  alpha = rep(0, length(teams)-1) %&amp;gt;% `names&amp;lt;-`(teams[2:length(teams)]),
  beta = rep(0, length(teams)-1) %&amp;gt;% `names&amp;lt;-`(teams[2:length(teams)]),
  gamma = 0
)

# initialise i to collect data about the optimisation process at each iteration
i &amp;lt;- 0
# collect current parameter values and neg log likelihood at each iteration
current_parameters &amp;lt;- list()
current_nll &amp;lt;- list()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then final the optim() function one final time to get our final optimised parameters&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# run our final calculation
optimised_parameters4 &amp;lt;- optim(
  par = unlist(equal_parameters), 
  fn = optimise_params,
  results = results,
  method = &amp;quot;BFGS&amp;quot;,
  control = list(maxit = 10000))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot the log likelihood at each iteration. Notice how it starts around &amp;lt;120, which is pretty close what our worse_log_likelihood returned. For these optimisations, the original parameters we are supplying are similar to the zeroed parameters for that example.&lt;/p&gt;
&lt;p&gt;As the optim() function plays with the parameters you can see the log likelihood jumps around quite violently, but over time tend towards zero.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3 &amp;lt;- data.frame(likelihood = unlist(current_nll),
                iteration = seq(length(current_nll))) %&amp;gt;%
  ggplot(aes(x = iteration, y = likelihood)) +
  geom_line(colour = &amp;quot;red&amp;quot;) +
  # cut out some cases where optim() has been a bit ambitious
  coord_cartesian(ylim = c(0, 250)) +
  labs(title = &amp;quot;Negative log likelihood of parameters over iterations&amp;quot;,
       y = &amp;quot;negative log likelihood&amp;quot;,
       x = &amp;quot;iteration&amp;quot;) +
  theme_minimal()

p3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-30-5-dixon-coles-1_files/figure-html/plot_log_liks-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The final parameters can also be extracted from the output from optim() and plotted:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p4 &amp;lt;- optimised_parameters4$par %&amp;gt;%
  # relist to add in first team
  relist_params() %&amp;gt;%
  unlist() %&amp;gt;%
  # select team parameters
  .[grepl(&amp;quot;beta|alpha&amp;quot;, names(.))] %&amp;gt;%
  data.frame(value = .,
             parameter = names(.)) %&amp;gt;%
  separate(parameter, into = c(&amp;quot;parameter&amp;quot;, &amp;quot;team&amp;quot;), &amp;quot;\\.&amp;quot;) %&amp;gt;%
  # spread into wide format
  spread(parameter, value) %&amp;gt;%
  # pipe into a plot
  ggplot(aes(x = alpha, y = beta)) +
  geom_point() +
  ggrepel::geom_text_repel(aes(label = team)) +
  stat_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE) +
  labs(title = &amp;quot;Optimal parameters for teams&amp;quot;,
       subtitle = &amp;quot;given first 8 weeks of results&amp;quot;,
       x = &amp;quot;alpha (more likely to score -&amp;gt;)&amp;quot;,
       y = &amp;quot;beta (less likely to concede -&amp;gt;)&amp;quot;) +
  theme_minimal()

p4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-30-5-dixon-coles-1_files/figure-html/plot_parameters-1.png&#34; width=&#34;672&#34; /&gt; Notice how the teams monotonically increase in both attack and defensive ability. This is by design on how the results were created (see the bottom of this post). With only 8 games per team however, there is quite a lot of noise in the signal. Hitting the crossbar instead of scoring in one game could make a fairly large difference in how the function rates a team.&lt;/p&gt;
&lt;p&gt;Also note how the regression line passes through the origin- this is a result of us constraining the parameters to sum to zero.&lt;/p&gt;
&lt;p&gt;If we want to see how optim() selects these, we can plot how they change over iterations. You can see how it jumps around then settles on incremental improvements to the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p5 &amp;lt;- current_parameters %&amp;gt;%
  # get the parameters for arsenal for each iteration
  lapply(., function(x){ unlist(relist_params(x))}) %&amp;gt;%
  map_df(bind_rows, .id = &amp;quot;iteration&amp;quot;) %&amp;gt;%
  # melt data and split parameters into team and parameter
  gather(&amp;quot;parameter&amp;quot;, &amp;quot;value&amp;quot;, -iteration) %&amp;gt;%
  # get rid of the gamma parameter
  filter(parameter != &amp;quot;gamma.gamma&amp;quot;) %&amp;gt;%
  separate(parameter, into = c(&amp;quot;parameter&amp;quot;, &amp;quot;team&amp;quot;), sep = &amp;quot;\\.&amp;quot;) %&amp;gt;%
  # spread data back by parameter
  spread(parameter, value) %&amp;gt;%
  mutate(iteration = as.numeric(iteration)) %&amp;gt;%
  # plot alpha against beta for each iteration
  ggplot(aes(x = alpha, y = beta)) +
  geom_text(aes(label = team)) +
  labs(title = &amp;#39;Parameters for Iteration {floor(frame_time)}&amp;#39;,
       subtitle = &amp;quot;given first 8 weeks of results&amp;quot;,
       x = &amp;quot;alpha (more likely to score -&amp;gt;)&amp;quot;,
       y = &amp;quot;beta (less likely to concede -&amp;gt;)&amp;quot;) +
  # using gganimate package
  gganimate::transition_time(iteration) +
  gganimate::ease_aes(&amp;#39;linear&amp;#39;) +
  gganimate::view_follow()

# animate the plot
gganimate::animate(p5, nframes = i)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;predict-remaining-matches&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predict Remaining Matches&lt;/h2&gt;
&lt;p&gt;Now we have rated each teams attack/defense, and the advantage to a team to play at home, we can predict the remaining matches between the teams.&lt;/p&gt;
&lt;p&gt;For this, we just have to use the predict_results() function we defined earlier, except this time the output will be the expected goals per team. Earlier we were measuring the deviance from expectation, but not we assume the most likely result is exactly equal to the expected results. If we wanted to we could work out how likely this result is, and what the most likely results are.&lt;/p&gt;
&lt;p&gt;This post is long enough however, so for now, we’ll just detail the most likely results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predicted_results &amp;lt;- predict_results(unplayed_games$home,
                      unplayed_games$away, 
                      relist_params(optimised_parameters4$par)) %&amp;gt;%
  mutate_if(is.numeric, round, 2) %&amp;gt;%
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               home             away e_hgoal e_agoal
## 1    Coventry_City          Arsenal    0.86    2.11
## 2 Blackburn_Rovers   Dover_Athletic    2.62    0.49
## 3    Frimley_Green     Enfield_Town    0.44    1.72
## 4          Arsenal Blackburn_Rovers    2.39    0.99
## 5    Coventry_City    Frimley_Green    4.09    0.17
## 6   Dover_Athletic     Enfield_Town    1.33    0.79&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of these look reasonable, with better teams beating worse ones. The only match that the model thinks might well end in a draw is Dover at home to Enfield, which is not entirely unreasonable.&lt;/p&gt;
&lt;p&gt;We can add these predictions to our earlier matrix of results to get a sense if these fit in with the trend from the observed matches:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p6 &amp;lt;- rbind(
  predicted_results %&amp;gt;%
    rename_if(is.numeric, gsub, pattern = &amp;quot;e_&amp;quot;, replacement = &amp;quot;&amp;quot;) %&amp;gt;%
    mutate(type = &amp;quot;predicted&amp;quot;),
  results %&amp;gt;%
    select(-gameweek) %&amp;gt;%
    mutate(type = &amp;quot;result&amp;quot;)
) %&amp;gt;%
  ggplot(., aes(x = away, y = home, fill = hgoal-agoal)) +
  geom_tile() +
  # add the scorelines
  geom_label(aes(label = paste(hgoal, agoal, sep = &amp;quot;-&amp;quot;), colour = type), fill = &amp;quot;white&amp;quot;) +
  # colour where black for actual results and red for predictions
  scale_colour_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;black&amp;quot;)) +
  # colour where green shows home win and red an away win
  scale_fill_gradient2(low = &amp;quot;darkred&amp;quot;, high = &amp;quot;green&amp;quot;, midpoint = 0, guide = FALSE) +
  scale_x_discrete(limits = levels(results$home), position = &amp;quot;top&amp;quot;) +
  scale_y_discrete(limits = rev(levels(results$away))) +
  theme_minimal()

p6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-30-5-dixon-coles-1_files/figure-html/plot_all_games-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which they do! The predicted results fit in with the gradient of heavier defeats for home teams towards the bottom left, progressing to easy home victories in the top right.&lt;/p&gt;
&lt;p&gt;That’s all for this post. Hopefully using the Poisson distribution to model football matches is a little clearer now. Feel free to email me any questions and check out the packages I stole all the codes/idea from.&lt;/p&gt;
&lt;p&gt;Next time, I’ll go over how to quantify the probability of a range of results for any single match in (hopefully) a shorter post; until then!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; much of the code I use here is stolen/reworked from the code shared on this repo&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; towards the end of writing this post I came across &lt;a href=&#34;https://dashee87.github.io/football/python/predicting-football-results-with-statistical-modelling-dixon-coles-and-time-weighting/&#34;&gt;David Sheehan’s blog&lt;/a&gt; which actually does a pretty good job, but I felt still didn’t quite go through how/why the model uses the maths it does&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;3&lt;/sup&gt; see &lt;a href=&#34;https://arxiv.org/pdf/cond-mat/0110605.pdf&#34; class=&#34;uri&#34;&gt;https://arxiv.org/pdf/cond-mat/0110605.pdf&lt;/a&gt; and also the conclusion of &lt;a href=&#34;https://dashee87.github.io/football/python/predicting-football-results-with-statistical-modelling/&#34;&gt;David Sheehan’s blog on Dixon-Coles processes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;4&lt;/sup&gt; *we could instead &lt;em&gt;maximise&lt;/em&gt; the sum of the log likelihoods and then the error will converge towards 0 from a negative number. Either is fine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results-generation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results Generation&lt;/h2&gt;
&lt;p&gt;First we need to create a data.frame of fixtures for each team&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# https://stackoverflow.com/questions/54099990/is-there-an-efficient-algorithm-to-create-this-type-of-schedule
create_fixtures &amp;lt;- function(teams) {
  # keep team 1 in place
  team1 &amp;lt;- as.character(teams[1])
  #rotate other teams around team 1
  other_teams &amp;lt;- as.character(teams[!teams %in% team1])
  length &amp;lt;- length(other_teams)
  
  # generate fixtures each week
  for(week in seq((length(teams)-1)*2)) {
    
    if(week %% 2 == 0) {
      fixtures &amp;lt;- data.frame(home = c(team1, other_teams[1:2]),
                             away = other_teams[length:3],
                             gameweek = week)
    } else {
      fixtures &amp;lt;- data.frame(home = other_teams[length:3],
                             away = c(team1, other_teams[1:2]),
                             gameweek = week)
      
    }
    
    if(week == 1) {
      fixtures_df &amp;lt;- fixtures 
    } else {
      fixtures_df &amp;lt;- rbind(fixtures_df, fixtures)
    }
    
    # rotate other teams around
    other_teams &amp;lt;- c(other_teams[length], other_teams[1:length-1])
  }
  
  return(fixtures_df)
}

# create the fixtures
fixtures &amp;lt;- create_fixtures(teams) %&amp;gt;%
  mutate_if(is.factor, as.character)

# print the fixture list
fixtures&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                home             away gameweek
## 1     Frimley_Green          Arsenal        1
## 2      Enfield_Town Blackburn_Rovers        1
## 3    Dover_Athletic    Coventry_City        1
## 4           Arsenal     Enfield_Town        2
## 5     Frimley_Green   Dover_Athletic        2
## 6  Blackburn_Rovers    Coventry_City        2
## 7    Dover_Athletic          Arsenal        3
## 8     Coventry_City     Enfield_Town        3
## 9  Blackburn_Rovers    Frimley_Green        3
## 10          Arsenal    Coventry_City        4
## 11   Dover_Athletic Blackburn_Rovers        4
## 12     Enfield_Town    Frimley_Green        4
## 13 Blackburn_Rovers          Arsenal        5
## 14    Frimley_Green    Coventry_City        5
## 15     Enfield_Town   Dover_Athletic        5
## 16          Arsenal    Frimley_Green        6
## 17 Blackburn_Rovers     Enfield_Town        6
## 18    Coventry_City   Dover_Athletic        6
## 19     Enfield_Town          Arsenal        7
## 20   Dover_Athletic    Frimley_Green        7
## 21    Coventry_City Blackburn_Rovers        7
## 22          Arsenal   Dover_Athletic        8
## 23     Enfield_Town    Coventry_City        8
## 24    Frimley_Green Blackburn_Rovers        8
## 25    Coventry_City          Arsenal        9
## 26 Blackburn_Rovers   Dover_Athletic        9
## 27    Frimley_Green     Enfield_Town        9
## 28          Arsenal Blackburn_Rovers       10
## 29    Coventry_City    Frimley_Green       10
## 30   Dover_Athletic     Enfield_Town       10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then create the results&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# using goalmodel package 
# https://github.com/opisthokonta/goalmodel
library(goalmodel)

# have to manually create a list of parameters
model &amp;lt;- list()
# stratify teams abilities in attack and defense
model$parameters &amp;lt;- list(attack = seq(1, -1 + 2/length(teams), by = -2/(length(teams)-1)) %&amp;gt;%
                           append(-sum(.)) %&amp;gt;%
                           `names&amp;lt;-`(teams), 
                         defense = seq(1, -1 + 2/length(teams), by = -2/(length(teams)-1)) %&amp;gt;%
                           append(-sum(.)) %&amp;gt;%
                           `names&amp;lt;-`(teams), 
                         # no base rate of goals
                         intercept = 0, 
                         # roughly accurate hfa for English professional football
                         hfa = 0.3)

# add in teams
model$all_teams &amp;lt;- teams
# use a simple Poisson model with 8 goals max
model$model &amp;lt;- &amp;quot;poisson&amp;quot;
model$maxgoal &amp;lt;- 8

# use the model to predict results using regista package
results &amp;lt;- predict_expg(model, fixtures$home, fixtures$away, return_df = TRUE) %&amp;gt;%
  # add some noise
  mutate(noise1 = rnorm(nrow(.), 0, 0.5),
         noise2 = rnorm(nrow(.), 0, 0.5)) %&amp;gt;%
  mutate(hgoal = round(expg1 + noise1,0 ),
         agoal = round(expg2 + noise2,0),
         home = as.factor(team1),
         away = as.factor(team2)) %&amp;gt;%
  # merge to fixtures
  merge(., fixtures, by = c(&amp;quot;home&amp;quot;, &amp;quot;away&amp;quot;)) %&amp;gt;%
  # cant score less than zero goals
  mutate_at(vars(hgoal:agoal), funs(replace(., .&amp;lt;0, 0))) %&amp;gt;%
  select(home, away, hgoal, agoal, gameweek) %&amp;gt;%
  arrange(gameweek, home) %&amp;gt;%
  # treat only first 8 weeks as played
  filter(gameweek &amp;lt;= 8)

# print results
results&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                home             away hgoal agoal gameweek
## 1    Dover_Athletic    Coventry_City     2     3        1
## 2      Enfield_Town Blackburn_Rovers     0     3        1
## 3     Frimley_Green          Arsenal     0     8        1
## 4           Arsenal     Enfield_Town     7     0        2
## 5  Blackburn_Rovers    Coventry_City     2     1        2
## 6     Frimley_Green   Dover_Athletic     1     3        2
## 7  Blackburn_Rovers    Frimley_Green     6     1        3
## 8     Coventry_City     Enfield_Town     3     0        3
## 9    Dover_Athletic          Arsenal     0     3        3
## 10          Arsenal    Coventry_City     3     0        4
## 11   Dover_Athletic Blackburn_Rovers     0     3        4
## 12     Enfield_Town    Frimley_Green     1     0        4
## 13 Blackburn_Rovers          Arsenal     1     1        5
## 14     Enfield_Town   Dover_Athletic     1     1        5
## 15    Frimley_Green    Coventry_City     1     4        5
## 16          Arsenal    Frimley_Green    10     1        6
## 17 Blackburn_Rovers     Enfield_Town     5     0        6
## 18    Coventry_City   Dover_Athletic     2     0        6
## 19    Coventry_City Blackburn_Rovers     1     2        7
## 20   Dover_Athletic    Frimley_Green     3     1        7
## 21     Enfield_Town          Arsenal     0     5        7
## 22          Arsenal   Dover_Athletic     4     1        8
## 23     Enfield_Town    Coventry_City     1     2        8
## 24    Frimley_Green Blackburn_Rovers     0     4        8&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to Discrete Choice Analysis in R</title>
      <link>/post/dca_1/</link>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/dca_1/</guid>
      <description>


&lt;p&gt;When studying why people make the economic choices they do, we need some way of quantifying the value to the person of the offered choices. For instance, when deciding whether to ride to my office by bike or instead catch the bus, there are myriad factors that my brain feeds into an equation to get two values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the utility of taking the bus&lt;/li&gt;
&lt;li&gt;the utility of riding my bike&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For instance, if it looks like it might rain, I’m more likely to take the bus as getting soaked reduces the utility of cycling to work. Conversely, if I glance at my watch and see that I’ve just missed a bus, the utility of taking the bus decreases as I don’t want to have to wait at the bus stop.&lt;/p&gt;
&lt;p&gt;A frequent criticism of economics is that it assumes some &lt;em&gt;homo economicus&lt;/em&gt; who will always choose that which maximizes this utility. Lets say the utilities of the two commute choices were only governed by p(rain) and e(wait time) respectively, then for a set probability of rain and expected wait time for the bus, I should always choose the same mode of transport. This is clearly not how humans (or any other animal) work and so for the last 50 years &lt;a href=&#34;https://eml.berkeley.edu/~mcfadden/discrete/ch5.pdf&#34;&gt;models of probabilistic choice&lt;/a&gt; have been used instead.&lt;/p&gt;
&lt;p&gt;The advantage to this is that by studying the % of times I decide to ride my bike into work vs catching the bus, for any set of parameters, it’s possible to derive the relative utility of that method of transportation. Then if a novel combination of rain/waiting comes up, it’s possible to predict the chance I will choose to ride my bike and the chance I will take the bus.&lt;/p&gt;
&lt;p&gt;However, many of these models are fairly dense to approach without formal economic training, so I wanted to write a guide to deriving and using them in R. For the first post, I’ll consider a toy problem with a simple binary choice paradigm to get some of the basic ideas of random utility modelling down and hopefully build from that in later posts.&lt;/p&gt;
&lt;div id=&#34;example-problem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example Problem&lt;/h1&gt;
&lt;p&gt;Summer is here and it’s time for the annual Behavioural Economics departmental picnic! Due to the collapsing global climate, this year is the hottest yet and you are eagerly anticipating sitting in &lt;a href=&#34;https://en.wikipedia.org/wiki/Grantchester_Meadows&#34;&gt;Granchester Meadows&lt;/a&gt; with your favourite chilled soft drink and discussing your research.&lt;/p&gt;
&lt;p&gt;Unfortunately, you’ve been stuck in the office for most of the afternoon coding up your latest model and will arrive late. Due to the hot weather, most of the drinks have already been consumed and what’s left needs to be rationed. Luckily, you are all very rational behavioural economists who know that if you can find everyone’s utility for the two remaining drinks flavours, you can apportion them appropriately.&lt;/p&gt;
&lt;p&gt;The two remaining drinks are &lt;a href=&#34;https://media2.giphy.com/media/3oriffxcqE2syOd5Ty/giphy.gif&#34;&gt;buzz cola&lt;/a&gt; which comes in cans of 330ml which is very tasty, and &lt;a href=&#34;https://comb.io/qbzoUv.gif&#34;&gt;slurm&lt;/a&gt; which comes in 2 litre bottles (which can be poured into any amount).&lt;/p&gt;
&lt;p&gt;Someone quickly codes up a binary choice task where PhD students have to choose between 1,2, or 3 330ml cans of the desirable Buzz Cola, or some amount between 0 and 2000ml of the slightly less valued Slurm.&lt;/p&gt;
&lt;p&gt;Having spent 10 minutes and 1800 trials doing the task your data looks like&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#show the first ten trials
head(trial_data, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      buzz_cola slurm    choice
## 1446         3   800 buzz_cola
## 227          1   800     slurm
## 209          1   800 buzz_cola
## 1497         3   800 buzz_cola
## 1542         3  1200 buzz_cola
## 34           1     0 buzz_cola
## 1493         3   800 buzz_cola
## 1028         2  1600     slurm
## 656          2     0 buzz_cola
## 266          1   800     slurm&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we group by each combination of n(buzz cola cans),ml(slurm) we can work out the proportion of buzz cola choices&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;choice_data &amp;lt;- trial_data %&amp;gt;%
  #code buzz cola choice as a binary variable
  mutate(buzz_cola_choice = case_when(
    choice == &amp;quot;buzz_cola&amp;quot; ~ 1,
    choice == &amp;quot;slurm&amp;quot; ~ 0
  )) %&amp;gt;%
  #group by combinations and find the proportion of buzz cola choices
  group_by(buzz_cola, slurm) %&amp;gt;%
  summarise(fraction_choose_cola = mean(buzz_cola_choice))

#show the grouped choice data
choice_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 18 x 3
## # Groups:   buzz_cola [3]
##    buzz_cola slurm fraction_choose_cola
##        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;                &amp;lt;dbl&amp;gt;
##  1         1    0                 0.94 
##  2         1  400                 0.580
##  3         1  800                 0.13 
##  4         1 1200.                0.01 
##  5         1 1600                 0    
##  6         1 2000                 0    
##  7         2    0                 1    
##  8         2  400                 0.96 
##  9         2  800                 0.72 
## 10         2 1200.                0.28 
## 11         2 1600                 0.04 
## 12         2 2000                 0    
## 13         3    0                 1    
## 14         3  400                 1    
## 15         3  800                 1    
## 16         3 1200.                0.88 
## 17         3 1600                 0.34 
## 18         3 2000                 0.02&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we can plot a logistic regression of this data to see how much x cans of buzz cola are worth in y ml of slurm&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#quick binomial smoothing function
#from https://ggplot2.tidyverse.org/reference/geom_smooth.html
binomial_smooth &amp;lt;- function(...) {
  geom_smooth(method = &amp;quot;glm&amp;quot;, method.args = list(family = &amp;quot;binomial&amp;quot;), ...)
}

#plot the logistic regression on the entire choice data
p1 &amp;lt;- choice_data %&amp;gt;%
  ggplot(., aes(x = slurm, y = fraction_choose_cola, colour = factor(buzz_cola))) +
  geom_point() +
  binomial_smooth(se = FALSE) +
  #add in some aesthetics
  scale_colour_discrete(name = &amp;quot;buzz cola cans \n(x * 330ml)&amp;quot;) +
  labs(title = &amp;quot;what drink do you want for the departmental picnic?&amp;quot;,
       subtitle = &amp;quot;simulated data&amp;quot;,
       x = &amp;quot;slurm (/ml)&amp;quot;,
       y = &amp;quot;fraction of buzz cola choices&amp;quot;) +
  theme_minimal()

p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-24-introduction-to-discrete-choice-analysis_files/figure-html/plot_data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We then need to solve your utilities of buzz cola and slurm. To do this we need to maximize the sum of the log likelihood of each choice you make.&lt;/p&gt;
&lt;p&gt;Basically, for each trial when you are presented with x ml of buzz cola (the number of cans multiplied by 330ml per can) or y ml of slurm there are utility parameters (rho) for both of these which mean you have some total utility of offered buzz cola and total utility of offered slurm.&lt;/p&gt;
&lt;p&gt;As a rational econ PhD student, you are pretty much always going to choose whichever of these utilities is greater. E.g:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#make up some utility parameters
buzz_cola_rho &amp;lt;- 2
slurm_rho &amp;lt;- 1

trial_utilities &amp;lt;- trial_data %&amp;gt;%
  #total utility of each offer is the amount * utility_parameter
  mutate(buzz_cola_utility = buzz_cola * 330 * buzz_cola_rho,
         slurm_utility = slurm * slurm_rho) %&amp;gt;%
  #which utility is greater
  mutate(greater_utility = case_when(
    buzz_cola_utility &amp;gt;= slurm_utility ~ &amp;quot;buzz_cola&amp;quot;,
    slurm_utility &amp;gt; buzz_cola_utility ~ &amp;quot;slurm&amp;quot;
  )) %&amp;gt;%
  #organise columns
  select(buzz_cola, buzz_cola_utility, 
         slurm, slurm_utility,
         greater_utility, choice)

#print first 10 trials
trial_utilities[1:10,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    buzz_cola buzz_cola_utility slurm slurm_utility greater_utility
## 1          3              1980   800           800       buzz_cola
## 2          1               660   800           800           slurm
## 3          1               660   800           800           slurm
## 4          3              1980   800           800       buzz_cola
## 5          3              1980  1200          1200       buzz_cola
## 6          1               660     0             0       buzz_cola
## 7          3              1980   800           800       buzz_cola
## 8          2              1320  1600          1600           slurm
## 9          2              1320     0             0       buzz_cola
## 10         1               660   800           800           slurm
##       choice
## 1  buzz_cola
## 2      slurm
## 3  buzz_cola
## 4  buzz_cola
## 5  buzz_cola
## 6  buzz_cola
## 7  buzz_cola
## 8      slurm
## 9  buzz_cola
## 10     slurm&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s clear to see that mostly the choices fall in line with these made up parameters. The two unexpected choices could be because of random participant mistakes, but is more likely due to our parameters not yet being optimized (more on that in a sec), or because even rational actors may sometimes choose something which seems to have less utility (e.g. when sampling, or as utilities change e.g. via satiety).&lt;/p&gt;
&lt;p&gt;In the above example, given the relative utilities of the two offered drinks, it’s possible to work out the probability that the participant will choose either of them using a simple logit model. (for these formulae I’m copying the notation from &lt;a href=&#34;https://imai.fas.harvard.edu/teaching/files/discrete.pdf&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;we have a binary model such that the choice of buzz cola can be represented by Y: &lt;span class=&#34;math display&#34;&gt;\[Y_{i} \in {0,1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where X is the difference in utility between the choices A (buzz cola) and B (slurm) on each trial, i&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_{i} = u(A_{i}) - u(B_{i})\]&lt;/span&gt; using a logit model such that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\phi_{i} = \frac{1}{1 + e^{-\beta X_{i})}}\]&lt;/span&gt; where beta is the temperature of the logit curve (i.e. the steepness).&lt;/p&gt;
&lt;p&gt;The log probability of choosing A (buzz cola) is therefore the log(phi) when A is chosen, and log(1-phi) when B (slurm) is chosen. The Y/1-Y cancel the other term out as Y can either equal 1 (buzz cola choice) or 0 (slurm chosen).&lt;/p&gt;
&lt;p&gt;We want to sum this over every trial and find the parameters for beta and the rho for both goods A (buzz cola) and B (slurm) which maximize this total sum&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathcal l_{n}(\beta|Y,X) = \sum_{i=1}^{n} Y_{i} log(\phi_{i}) + (1-Y_{i})log(1-\phi_{i})\]&lt;/span&gt; In R this can be expressed as&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#function to calulate the log likelihood per trial over data
#parameters is a vector of beta,rho_a,rho_b
#(a = buzz cola, b = slurm)
#data is our trial data
log_likelihood_func &amp;lt;- function(parameters, data) {
  #I want to plot how optim works so will gather the parameters
  #it selects for each iteration
  i &amp;lt;&amp;lt;- i + 1
  vals[[i]] &amp;lt;&amp;lt;- parameters
  
  #pull the individual parameters out of the vector
  beta &amp;lt;- parameters[&amp;quot;beta&amp;quot;]
  buzz_cola_rho &amp;lt;- parameters[&amp;quot;rho_a&amp;quot;]
  slurm_rho &amp;lt;- parameters[&amp;quot;rho_b&amp;quot;]
  
  #find the trial utility of the offered buzz cola and slurm
  trial_bc_utility &amp;lt;- (data$buzz_cola * 330) / 1000 * buzz_cola_rho
  trial_s_utility &amp;lt;- (data$slurm/ 1000) * slurm_rho
  #find the difference in utility between the two offered goods
  delta_utility &amp;lt;- trial_bc_utility - trial_s_utility
  
  #find the phi term for this trial
  #using the logit model
  phi &amp;lt;- 1 / (1 + exp(-beta*delta_utility))
  
  #find the log likelihood for the choice made in each trial
  log_likelihood &amp;lt;- (data$buzz_cola_choice * log(phi)) + ((1-data$buzz_cola_choice) * log(1-phi))
  
  sumloglik[[i]] &amp;lt;&amp;lt;- sum(log_likelihood)
  
  #return the sum over every trial of these log likelihoods
  #we want to vary the parameters to maximise this sum
  return(sum(log_likelihood))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can start out with a parameter assuming the two utilities are equal&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#make up initial parameters
#1, 1, 1 is unlikely but a reasonable starting point
initial_parameters &amp;lt;- c(1, 1, 1) %&amp;gt;%
  `names&amp;lt;-`(c(&amp;quot;beta&amp;quot;, &amp;quot;rho_a&amp;quot;, &amp;quot;rho_b&amp;quot;))

initial_parameters&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  beta rho_a rho_b 
##     1     1     1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then we can use the optim function which will pass the parameter vector into the log likelihood function and iteratively change the values in the parameter vector until the greatest sum is returned. We’re looking for the greatest sum as the log likelihood per trial boils down to log(phi / 1-phi) where phi is between 0 and 1 and is greatest where the utility parameters make the post-hoc choice most clear. E.g. when choosing the buzz cola the log likelihood = log(phi) and we want to maximize phi.&lt;/p&gt;
&lt;p&gt;Taking the log of x between 0 and 1 will give a negative number that approaches 0 as x approaches 1. So the total sum of log likelihood terms will approach 0 as the parameters maximize the phi term.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#add a binary variable for the choice of buzz_cola
trial_data_binary &amp;lt;- trial_data %&amp;gt;%
  mutate(buzz_cola_choice = case_when(
    choice == &amp;quot;buzz_cola&amp;quot; ~ 1,
    choice == &amp;quot;slurm&amp;quot; ~ 0
  ))

#initialise a list to store the parameters over iterations
i &amp;lt;- 0
vals &amp;lt;- list()
sumloglik &amp;lt;- list()

optim_params &amp;lt;- optim(par = initial_parameters,
                      #the functionise to optimise these over
                      fn = log_likelihood_func,
                      #other arguments to the function
                      data = trial_data_binary,
                      #optimisation algorithm to use
                      method = &amp;quot;Nelder-Mead&amp;quot;,
                      #we are looking to maximise the sum
                      #so fnscale set to -1
                      control = list(fnscale = -1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;optim() works by taking a vector of parameters and slightly adjusting them every iteration until the output from fn = … is minimized via an algorithm (in this case &lt;a href=&#34;https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method&#34;&gt;Nelder-Mead&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;By collecting the parameter values it selects and the subsequent log likelihood sum for each iteration we can get a sense of how it works&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load gganimate
library(gganimate)

#rbind the values per iteration
p2 &amp;lt;- vals %&amp;gt;%
  do.call(rbind, .) %&amp;gt;%
  #add a column for iteration number
  as.data.frame() %&amp;gt;%
  mutate(iteration = 1:n()) %&amp;gt;%
  #gather for plotting
  gather(&amp;quot;parameter&amp;quot;, &amp;quot;value&amp;quot;, -iteration) %&amp;gt;%
  #plt a bar chart of parameters over iterations
  ggplot(., aes(x = parameter, y = value)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  #show the iteration in the title
  labs(title = &amp;#39;iteration: {frame_time}&amp;#39;) +
  #aesthetics
  theme_minimal() +
  #gganimate
  transition_time(iteration) +
  ease_aes(&amp;#39;cubic-in-out&amp;#39;)

p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-24-introduction-to-discrete-choice-analysis_files/figure-html/plot_parameter_optimisation-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#unlist the log likelihood sum per iteration
p3 &amp;lt;- unlist(sumloglik) %&amp;gt;%
  #add the iteration number as a variable
  data.frame(sum = .,
             iteration = 1:length(.)) %&amp;gt;%
  #plot it
  ggplot(., aes(x = iteration, y = sum)) +
  geom_line() +
  labs(title = &amp;quot;how optim maximises the sum of the log likelihood&amp;quot;,
       x = &amp;quot;iteration&amp;quot;,
       y = &amp;quot;sum of the log likelihoods per trial&amp;quot;) +
  theme_minimal()

p3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-24-introduction-to-discrete-choice-analysis_files/figure-html/plot_optimisation_errors-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The final maximized value of -400 is still some way off 0 but seems to be the highest it will go. Looking at p1, we can see that the curves are some way off a step function that would mean that no ‘sub-optimal’ choices would be made (there’s some threshold of slurm vs buzz cola that means only one or the other is chosen).&lt;/p&gt;
&lt;p&gt;We can then get the optimized parameters from the object returned from optimization function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#print the optimised parameters
optim_params$par&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     beta    rho_a    rho_b 
## 3.520318 2.563380 1.702415&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and there we have it! you &lt;em&gt;do&lt;/em&gt; prefer buzz cola (rho_a) to slurm (rho_b). For every unit ml of both drinks you are offered, you prefer buzz cola ~1.5x as much.&lt;/p&gt;
&lt;p&gt;We can plot your likelihood to choose the offered ml of buzz cola over the offered ml of slurm for ratios of slurm:buzz cola by changing around a few terms in our logit model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#function to calculate likelihood of choosing good A given ratio B/A
calc_likelihood &amp;lt;- function(beta = optim_params$par[&amp;quot;beta&amp;quot;],
                            rho_a = optim_params$par[&amp;quot;rho_a&amp;quot;],
                            rho_b = optim_params$par[&amp;quot;rho_b&amp;quot;],
                            ratio_ba) {
  #instead of working out the difference in utility by comparing offered amounts
  #use the ratio of good b:good a
  utility_term &amp;lt;- rho_a/rho_b - ratio_ba
  
  #calculate as before
  phi &amp;lt;- 1 / (1 + exp(-beta* utility_term))
}

#plot this data for the rations 1:3 to 3:1
p4 &amp;lt;- seq(1/3, 3, by = 0.1) %&amp;gt;%
  data.frame(ratio_ba = .,
             likelihood_a = calc_likelihood(ratio_ba = .)) %&amp;gt;%
  ggplot(., aes(x = ratio_ba, y = likelihood_a)) +
  geom_point() +
  #show the indifference point ratio of B:A
  geom_segment(aes(x = optim_params$par[&amp;quot;rho_a&amp;quot;]/optim_params$par[&amp;quot;rho_b&amp;quot;],
                   xend = optim_params$par[&amp;quot;rho_a&amp;quot;]/optim_params$par[&amp;quot;rho_b&amp;quot;],
                   y = 0.5,
                   yend = 0)) +
  geom_text(label = &amp;quot;50% likelihood at rho_a/rho_b&amp;quot;, x = 1, y = 0.2) +
  #aesthetics
  labs(title = &amp;quot;Utility curve for Buzz Cola vs Slurm&amp;quot;,
       x = &amp;quot;ratio of ml slurm:buzz cola&amp;quot;,
       y = &amp;quot;likelihood of choosing buzz cola&amp;quot;) +
  theme_minimal()

p4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-24-introduction-to-discrete-choice-analysis_files/figure-html/plot_utility_curve-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;how-parameters-vary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How Parameters Vary&lt;/h2&gt;
&lt;p&gt;Imagine that two of your colleagues, Andreas and Béatrice also arrive at the picnic and take part in the utility measurement.&lt;/p&gt;
&lt;p&gt;Béatrice is entirely indifferent between the two soft drinks, they both taste the same as far as she is concerned so 1ml of slurm == 1ml of buzz cola.&lt;/p&gt;
&lt;p&gt;Andreas, on the other hand, also prefers buzz cola to slurm, but is hyper-rational. Once the utility of one option exceeds the utility of another he will pretty much always choose the former and samples the lower utility option very rarely.&lt;/p&gt;
&lt;p&gt;If we plot their choice proportions we get similar curves as before, but with slightly different shapes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p5 &amp;lt;- colleagues_choice_data %&amp;gt;%
  ggplot(., aes(x = slurm, y = fraction_choose_cola, colour = factor(buzz_cola))) +
  geom_point() +
  binomial_smooth(se = FALSE) +
  #add in some aesthetics
  scale_colour_discrete(name = &amp;quot;buzz cola cans \n(x * 330ml)&amp;quot;) +
  labs(title = &amp;quot;what drink do you want for the departmental picnic?&amp;quot;,
       subtitle = &amp;quot;simulated data&amp;quot;,
       x = &amp;quot;slurm (/ml)&amp;quot;,
       y = &amp;quot;fraction of buzz cola choices&amp;quot;) +
  theme_minimal() +
  facet_wrap(~person)

p5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-24-introduction-to-discrete-choice-analysis_files/figure-html/plot_colleagues_choices-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Andreas shows a step function in choosing between buzz cola and slurm, whereas Béatrice show much more linearity, with lines that pass 0.5 where ml slurm == ml buzz cola.&lt;/p&gt;
&lt;p&gt;I won’t show the code for calculating the optimal parameters for these two as it’s much the same as before, but upon calculation we get&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#the optimal parameters for Andreas and Béatrice
lapply(colleague_parameters, function(x) x$par)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Andreas
##     beta    rho_a    rho_b 
## 6.208545 5.976904 3.501038 
## 
## $Béatrice
##     beta    rho_a    rho_b 
## 2.806346 1.321279 1.330345&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we can see that Andreas has a much higher temperature (steepness) of his utility curve (beta), whereas Béatrice has a lower calculated beta.&lt;/p&gt;
&lt;p&gt;In contrast, while Andreas has a similar preference for buzz cola to you (rho_a / rho_b ~ 1.5, see below to refresh on your calculated optimal parameters from earlier), the relative utilities of the two sodas are equal (~1.3) for Béatrice, who is indifferent between them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#your optimal parameters for refreshing memory
optim_params$par&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     beta    rho_a    rho_b 
## 3.520318 2.563380 1.702415&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting the utility curves for these two also shows this&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot the utility curves per colleague
p6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-24-introduction-to-discrete-choice-analysis_files/figure-html/plot_colleague_utility_curves-1.png&#34; width=&#34;672&#34; /&gt; Where I’ve used geom_line to link between the points. Andreas’ curve is much steeper than yours which we plotted previously, but passes the indifference point (0.5 on the y axis) at roughly the same place.&lt;/p&gt;
&lt;p&gt;Béatrice’s is less steep but her indifference point is where the ml of the two sodas are equal (x == 1).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-creation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Creation&lt;/h2&gt;
&lt;p&gt;All data in this post is generation using the normal distribution and is fake. However, it approximates what you’d expect real data to look like pretty well so is fine for a tutorial and saves the need to have to upload real lab data.&lt;/p&gt;
&lt;p&gt;I haven’t included the data generation for Andreas and Béatrice’s data, but it follows almost identical steps. Set the sd in pnorm = 0 to achieve the step function.&lt;/p&gt;
&lt;p&gt;The code used to generate the data is provided below&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#set up
library(tidyverse)
set.seed(220892)

#generally cumulative normal distribution curves
#stand in for real choice curve data

#seq over the range of 0-1 of slurm
choice_data &amp;lt;- seq(0, 1, 0.2) %&amp;gt;%
  #generate distributions
  data.frame(x = .,
             y1 = 1-pnorm(., 0.23, 0.15),
             y2 = 1-pnorm(., 0.5, 0.175),
             y3 = 1-pnorm(., 0.75, 0.125)) %&amp;gt;%
  #melt data
  gather(&amp;quot;group&amp;quot;, &amp;quot;dist&amp;quot;, - x) %&amp;gt;%
  #rename data with our variables
  #buzz cola and slurm
  mutate(buzz_cola = case_when(
    group == &amp;quot;y1&amp;quot; ~ 1,
    group == &amp;quot;y2&amp;quot; ~ 2,
    group == &amp;quot;y3&amp;quot; ~ 3
  )) %&amp;gt;%
  mutate(slurm = x * 2000) %&amp;gt;%
  #round dist data
  mutate(fraction_choose_cola = round(dist, 2))  %&amp;gt;%
  select(buzz_cola, slurm, fraction_choose_cola)


#create some fake trial data by stretching this choice data
generate_trial_data &amp;lt;- function(combination_row) {
  #for this combination how many times in cola chosen
  cola_choice &amp;lt;- round(combination_row$fraction_choose_cola*100)
  
  #create a df of 100 trials for this combination
  data.frame(buzz_cola = rep(combination_row$buzz_cola, 100),
             slurm = rep(combination_row$slurm, 100),
             choice = c(rep(&amp;quot;buzz_cola&amp;quot;, cola_choice),
                        rep(&amp;quot;slurm&amp;quot;, 100 - cola_choice))
             )
}

#split choice data by combination (row)
trial_data &amp;lt;- choice_data %&amp;gt;%
  split(f = seq(nrow(.))) %&amp;gt;%
  #apply the stretching function
  lapply(., generate_trial_data) %&amp;gt;%
  #map together the data
  map_df(I) %&amp;gt;%
  #randomly shuffle the data
  .[sample(nrow(.)),]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Knowledge 7th February 2019</title>
      <link>/post/the-knowledge-7th-february-2019/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/the-knowledge-7th-february-2019/</guid>
      <description>


&lt;p&gt;In what is becoming a &lt;a href=&#34;http://www.robert-hickman.eu/post/the-knowledge-4th-august-2018/&#34;&gt;repeated&lt;/a&gt; &lt;a href=&#34;http://www.robert-hickman.eu/post/counties_league_points/&#34;&gt;series&lt;/a&gt;, I enjoy answering trivia questions from The Guardian’s &lt;a href=&#34;https://www.theguardian.com/football/series/theknowledge&#34;&gt;The Knowledge&lt;/a&gt; football trivia column.&lt;/p&gt;
&lt;p&gt;There’s a few questions that built up that seemed amenable to coding answers so I’ve taken a stab at them here&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#munging
library(tidyverse)
library(data.table)
library(zoo)
#english football data
library(engsoccerdata)
#web data scraping
library(rvest)
#plotting
library(openair)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;calendar-boys&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Calendar Boys&lt;/h1&gt;
&lt;p&gt;The first question this week concerns players scoring on (or nearest to) every day of the year&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Has any player played/or even scored on every date in a calendar year. What’s the nearest anyone has come?
&lt;/p&gt;
— David Thomson (&lt;span class=&#34;citation&#34;&gt;@thomsonionioni&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/thomsonionioni/status/1090206478479298560?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2019&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;answer-cristiano-ronaldo-using-data-from-around-the-turn-of-the-millenium---244-days&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Answer: Cristiano Ronaldo (using data from around the turn of the millenium) - 244 days&lt;/h3&gt;
&lt;p&gt;Getting the data for this is the main problem. The best (free) source I tend to use is transfermarkt.com, but data there becomes less reliable from before 2000 (and only has a few years of data from more obscure years where I could believe some players are banging in goals for fun). Nonetheless, it should at least gives us some ideas&lt;/p&gt;
&lt;p&gt;For each player sampled we’re going to want the data for each goal scored both for their club and country. Saving the competition data is also useful as it also allows us to sort out friendlies which may or may not count depending on interpretation of the question.&lt;/p&gt;
&lt;p&gt;Two quick functions will do this for any given player id&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#create a data frame of club goals
get_club_goals &amp;lt;- function(club_stats) {
  #read the page
  read &amp;lt;- read_html(club_stats)
  
  #get the players names
  name &amp;lt;- read %&amp;gt;%
    html_nodes(&amp;quot;.dataName b&amp;quot;) %&amp;gt;%
    html_text()
    
  #read the table of goals scored and munge together
  club_df &amp;lt;- read %&amp;gt;%
    html_nodes(xpath = &amp;#39;//*[@id=&amp;quot;main&amp;quot;]/div[10]/div/div/div[4]/table&amp;#39;) %&amp;gt;%
    html_table(fill = TRUE) %&amp;gt;%
    as.data.frame() %&amp;gt;%
    select(Date, Minute, Competition.1) %&amp;gt;%
    mutate(minute = as.numeric(gsub(&amp;quot;&amp;#39;.*&amp;quot;, &amp;quot;&amp;quot;, Minute))) %&amp;gt;%
    filter(!is.na(minute)) %&amp;gt;% 
    #convert date to day of the year
    mutate(date = case_when(
      Date != &amp;quot;&amp;quot; ~ strftime(as.Date(Date, &amp;quot;%m/%d/%y&amp;quot;), &amp;quot;%m/%d&amp;quot;)
    )) %&amp;gt;%
    mutate(competition = ifelse(Competition.1 == &amp;quot;&amp;quot;, NA, Competition.1)) %&amp;gt;%
    select(competition, date, minute) %&amp;gt;%
    #fill down the competition and date if missing
    do(na.locf(.)) %&amp;gt;%
    mutate(scored_for = &amp;quot;club&amp;quot;, name = name)
}

#do the same for national team goals
get_nt_goals &amp;lt;- function(nt_stats) {
  read &amp;lt;- read_html(nt_stats)
  
  name &amp;lt;- read %&amp;gt;%
    html_nodes(&amp;quot;.dataName b&amp;quot;) %&amp;gt;%
    html_text()

  goal_table &amp;lt;- read %&amp;gt;%
    html_nodes(xpath = &amp;#39;//*[@id=&amp;quot;main&amp;quot;]/div[10]/div[1]/div[3]/div[4]/table&amp;#39;)
  
  #some players won&amp;#39;t have any national team goals
  #return NA
  if(!is_empty(goal_table)) {
    nt &amp;lt;- goal_table %&amp;gt;%
      html_table(fill = TRUE) %&amp;gt;%
      as.data.frame() %&amp;gt;%
      select(For, Date, Var.11) %&amp;gt;%
      mutate(goals = as.numeric(Var.11)) %&amp;gt;%
      mutate(date = case_when(
        Date != &amp;quot;&amp;quot; ~ strftime(as.Date(Date, &amp;quot;%m/%d/%y&amp;quot;), &amp;quot;%m/%d&amp;quot;)
      )) %&amp;gt;%
      mutate(competition = ifelse(For == &amp;quot;&amp;quot;, NA, For)) %&amp;gt;%
      mutate(competition = na.locf(competition)) %&amp;gt;%
      filter(!is.na(date)) %&amp;gt;%
      select(competition, date, goals) 
    
    #if more than 1 goal is scored on a game it&amp;#39;s counted as two rows
    #separate these out
    if(any(nt$goals != 1)) {
      nt_df &amp;lt;- do.call(&amp;quot;c&amp;quot;, (mapply(rep, c(nt$competition, nt$date), nt$goals))) %&amp;gt;%
        matrix(., 2, byrow = TRUE) %&amp;gt;%
        t() %&amp;gt;%
        as.data.frame() %&amp;gt;%
        select(competition = V1, date = V2)
    } else {
      nt_df &amp;lt;- nt %&amp;gt;%
        select(-goals)
    }
    
    #finish off munging
    df &amp;lt;- nt_df %&amp;gt;%
      mutate(minute = NA, scored_for = &amp;quot;nation&amp;quot;, name = name)
  } else {
    df &amp;lt;- NA
  }
  return(df)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can get into the scraping. For each player some parts of the URL stay the same, so lets save those as objects so we don’t have to deal with massive long urls.&lt;/p&gt;
&lt;p&gt;I decided to test out the functions using Cristiano Ronaldo as his 675 goals for club and country is (I believe) more than any active player. Pasting the url together and running on the functions does the trick&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#for each player some parts of url stay the same
base_url &amp;lt;- &amp;quot;https://www.transfermarkt.co.uk/&amp;quot;
club_text1 &amp;lt;- &amp;quot;/alletore/spieler/&amp;quot;
club_text2 &amp;lt;- &amp;quot;/saison//verein/0/liga/0/wettbewerb//pos/0/trainer_id/0/minute/0/torart/0/plus/1&amp;quot;
nt_text1 &amp;lt;- &amp;quot;/nationalmannschaft/spieler/&amp;quot;
nt_text2 &amp;lt;- &amp;quot;/verein_id/3300/plus/0?hauptwettbewerb=&amp;amp;wettbewerb_id=&amp;amp;trainer_id=&amp;amp;start=Aug+20%2C+2003&amp;amp;ende=Feb+4%2C+2019&amp;amp;nurEinsatz=2&amp;quot;

#get all the goals scored by Cristiano Ronaldo
ronaldo &amp;lt;- rbind(
  paste0(base_url, &amp;quot;player_name&amp;quot;, club_text1, 8198, club_text2) %&amp;gt;%
    get_club_goals(),
  paste0(base_url, &amp;quot;player_name&amp;quot;, nt_text1, 8198, nt_text2) %&amp;gt;%
    get_nt_goals()
)

#count the number of unique dates scored on
length(unique(ronaldo$date))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 244&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So Ronaldo has scored on 244 of the 366 possible days of the year. It’s not surprising that scoring on &lt;em&gt;every&lt;/em&gt; day would be difficult. The club season only runs August-June and there are unlikely to be many possible games to play in July at all. Plus days such a Christmas are usually taken off from football.&lt;/p&gt;
&lt;p&gt;In terms of goals per day using 2019s calendar this looks like (plot made using the &lt;a href=&#34;https://cran.r-project.org/web/packages/openair/index.html&#34;&gt;openair package&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ronaldo %&amp;gt;%
  #count goals per date
  group_by(date) %&amp;gt;%
  summarise(goals = n()) %&amp;gt;%
  #convert to 2019 dates
  mutate(date = as.Date(paste0(&amp;quot;2019/&amp;quot;, date))) %&amp;gt;%
  #use calendarPlot from the openair package
  calendarPlot(., pollutant = &amp;quot;goals&amp;quot;, year = 2019)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-05-The_Knowledge_3_files/figure-html/ronaldo_calendar-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which shows more deviation in scoring than I thought it would. Nevertheless, from September-May each year is pretty blocked out, though there is a run of Saturdays this December which could be fertile ground for increasing his total.&lt;/p&gt;
&lt;p&gt;Next we need to get a list of likely players who could come close to matching Ronaldo’s record.&lt;/p&gt;
&lt;p&gt;For this I took the first page of transfermarkt’s top scorers of the year across all leagues. It’s possible that a player might (e.g.) be on the second page each year and have scored a ton, but I don’t think it’s super likely.&lt;/p&gt;
&lt;p&gt;I run this through the top scorers page from 1995 (the earliest year available) to 2018 and grab each player id. Afterwards, save the scraped list as an .rds to prevent needing to continually re scrape the page and put extra load onto the server.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#the url for the pages of top scorers
top_scorer_ids &amp;lt;- paste0(base_url, 
                         &amp;quot;spieler-statistik/jahrestorschuetzen/&amp;quot;,
                         &amp;quot;statistik/stat/plus/0/galerie/0?jahr=&amp;quot;,
                         1995:2018,
                         &amp;quot;&amp;amp;wettbewerb=alle&amp;amp;monatVon=01&amp;amp;monatBis=12&amp;amp;altersklasse=&amp;amp;&amp;quot;,
                         &amp;quot;land_id=&amp;amp;ausrichtung=alle&amp;amp;spielerposition_id=alle&amp;amp;art=0&amp;quot;) %&amp;gt;%
  #scrape the ids of players
  lapply(., function(year) {
    read_html(year) %&amp;gt;%
      html_nodes(&amp;quot;#yw1 .spielprofil_tooltip&amp;quot;) %&amp;gt;%
      html_attr(&amp;quot;id&amp;quot;)
  }) %&amp;gt;%
  unlist() %&amp;gt;%
  unique()

#save this to prevent need for re-scraping
saveRDS(top_scorer_ids, &amp;quot;transfermarkt_top_scorers.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then all that’s left is to scrape the goals for each player whose id we’ve scraped. Again, save this once run, especially as it takes a fair while to complete. For this article the data was scraped on the 5th February 2019&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;player_goals &amp;lt;- top_scorer_ids %&amp;gt;%
  #for each player scrape every goal
  lapply(., function(id) {
    goals &amp;lt;- rbind(
      paste0(base_url, &amp;quot;player_name&amp;quot;, club_text1, id, club_text2) %&amp;gt;%
        get_club_goals(),
      paste0(base_url, &amp;quot;player_name&amp;quot;, nt_text1, id, nt_text2) %&amp;gt;%
        get_nt_goals()
    ) %&amp;gt;%
      #remove NAS
      #this is where a player hasn&amp;#39;t scored for their nation
      filter(!is.na(date)) %&amp;gt;%
      mutate(id = id)
  }) %&amp;gt;%
  do.call(rbind, .)

#and save
saveRDS(player_goals, &amp;quot;top_scorer_goals.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a list of every goal scored by prolific strikers, we just have to group by each player and count how many dates they’ve scored on. To get the players with the highest number of unique dates we group by their id and count the length of the unique dates they’ve scored on.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;days_per_player &amp;lt;- player_goals %&amp;gt;%
  #group by player
  group_by(id) %&amp;gt;%
  #count the dates scored on
  summarise(days = length(unique(date))) %&amp;gt;%
  arrange(-days) %&amp;gt;%
  #rejoin the name data back in
  left_join(.,
            player_goals %&amp;gt;%
              select(id, name) %&amp;gt;%
              unique(),
            by = &amp;quot;id&amp;quot;) %&amp;gt;%
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 413 x 3
##    id     days name             
##    &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;            
##  1 8198    244 Cristiano Ronaldo
##  2 28003   210 Messi            
##  3 7349    203 Raúl             
##  4 3455    200 Ibrahimovic      
##  5 3207    189 Henry            
##  6 4257    187 Eto&amp;#39;o            
##  7 3924    173 Drogba           
##  8 48280   173 Cavani           
##  9 44352   172 Suárez           
## 10 7980    171 Villa            
## # ... with 403 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;so perhaps unsurprisingly, Ronaldo comes out on top. As expected given the data source, most of the top players are very recent strikers- all of the top 10 were active well into the 2010s. &lt;a href=&#34;https://en.wikipedia.org/wiki/Ulf_Kirsten&#34;&gt;Ulf Kirsten&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Toni_Polster&#34;&gt;Toni Polster&lt;/a&gt; are the torchbearers for strikers from the 90s.&lt;/p&gt;
&lt;p&gt;As always in these posts, I try to learn some new stuff as I do them. I thought this might be a good time to try some circular plotting. I don’t think the resultant plots actually inform that much but they are cool to look at.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;circular_data &amp;lt;- player_goals %&amp;gt;%
  #filter out top 16 scorers
  filter(id %in% days_per_player$id[1:16]) %&amp;gt;%
  #group by month and player and sum
  mutate(month = gsub(&amp;quot;\\/.*&amp;quot;, &amp;quot;&amp;quot;, date)) %&amp;gt;%
  group_by(id, month, competition) %&amp;gt;%
  summarise(goals = n()) %&amp;gt;%
  left_join(.,
            player_goals %&amp;gt;%
              select(id, name) %&amp;gt;%
              unique(),
            by = &amp;quot;id&amp;quot;)

#too many competititon for legend
#sort out into broad groups
competition_types &amp;lt;- data.frame(competition = circular_data$competition) %&amp;gt;%
  unique() %&amp;gt;%
  mutate(competition = as.character(competition)) %&amp;gt;%
  mutate(competition_type = case_when(
    grepl(&amp;quot;MLS&amp;quot;, competition) ~ &amp;quot;Domestic&amp;quot;,
    grepl(&amp;quot;World Cup qualification| Qualifiers&amp;quot;, competition) ~ &amp;quot;International&amp;quot;,
    grepl(&amp;quot;Friendlies&amp;quot;, competition) ~ &amp;quot;International Friendlies&amp;quot;,
    grepl(&amp;quot;World Cup [0-9]{4}|Confederations|EURO [0-9]{4}&amp;quot;, competition) ~ &amp;quot;International Tournament&amp;quot;,
    grepl(&amp;quot;UEFA|Champions League|UI Cup|Cup Winners|European Cup|Europa&amp;quot;, competition) ~ &amp;quot;European&amp;quot;,
    grepl(&amp;quot;Club World&amp;quot;, competition) ~ &amp;quot;International Club&amp;quot;,
    grepl(&amp;quot;Cup|cup|Pokal|copa|Copa|beker|Coupe|coppa|Kupasi|Trophée|Kupa&amp;quot;, competition) ~ &amp;quot;Domestic Cup&amp;quot;
  )) %&amp;gt;%
  mutate(competition_type = ifelse(is.na(competition_type), &amp;quot;Domestic&amp;quot;, competition_type)) %&amp;gt;%
  #convert to factor for plot fill order
  mutate(competition_type = fct_rev(factor(competition_type)))

#plot as circular radar plots
circular_data %&amp;gt;%
  left_join(., competition_types, by = &amp;quot;competition&amp;quot;) %&amp;gt;%
  ggplot(., aes(x = month, y = goals, fill = competition_type)) +
  #convert to polar coordinates
  coord_polar(theta = &amp;quot;x&amp;quot;, start = -.13) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  scale_fill_discrete(name = &amp;quot;Competition Type&amp;quot;) +
  ggtitle(&amp;quot;Goals Per Month for Top 16 Unique Day Scorers&amp;quot;) +
  facet_wrap(~name) +
  theme_minimal() +
  theme(axis.text = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-05-The_Knowledge_3_files/figure-html/plot_circular-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One nice thing that pops out is how Kirsten rarely scored in December/January- probably due to the Bundesliga mid season break.&lt;/p&gt;
&lt;p&gt;I also found it interesting that &lt;a href=&#34;https://en.wikipedia.org/wiki/Dirk_Kuyt&#34;&gt;Dirk Kuyt&lt;/a&gt; featured in the top 16, despite not being renowned as a great goalscorer.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#count total goals per player
left_join(
  days_per_player,
  player_goals %&amp;gt;%
    group_by(id) %&amp;gt;%
    summarise(goals = n()),
  by = &amp;quot;id&amp;quot;
) %&amp;gt;%
  #work out days/total goals
  mutate(proportion_unique = days / goals) %&amp;gt;%
  arrange(-days) %&amp;gt;%
  filter(days &amp;gt; 150) %&amp;gt;%
  select(name, goals, days, proportion_unique) %&amp;gt;%
  arrange(-proportion_unique)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 34 x 4
##    name      goals  days proportion_unique
##    &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;             &amp;lt;dbl&amp;gt;
##  1 Gilardino   231   154             0.667
##  2 Cissé       269   166             0.617
##  3 Signori     260   160             0.615
##  4 Di Vaio     269   165             0.613
##  5 Toni        273   165             0.604
##  6 Kuyt        281   169             0.601
##  7 Lampard     258   154             0.597
##  8 Frei        257   153             0.595
##  9 Drogba      300   173             0.577
## 10 Trézéguet   265   152             0.574
## # ... with 24 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When sorted by how evenly their goals/date coverage is (i.e. the ideal ratio would be 1 goal on every day), Dirk Kuyt pops up again (and did in fact score many more goals than I had assumed). &lt;a href=&#34;https://en.wikipedia.org/wiki/Alberto_Gilardino&#34;&gt;Alberto Gilardino&lt;/a&gt; really stands out as a player who has maximum date coverage despite (relative to other members of the list!) a low number of total goals scored.&lt;/p&gt;
&lt;p&gt;I’m not sure what, if any, insight that adds but is a cool piece of trivia.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;first-losers&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;First Losers&lt;/h1&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Which club holds the record for most 2nd place finishes in the English top flight?
&lt;/p&gt;
— Tom Goddard (&lt;span class=&#34;citation&#34;&gt;@Tom_Goddard_13&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/Tom_Goddard_13/status/1092785174046228480?ref_src=twsrc%5Etfw&#34;&gt;February 5, 2019&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;answer-manchester-united-in-the-top-flight---14-times&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Answer: Manchester United in the top flight - 14 times&lt;/h3&gt;
&lt;p&gt;The fist question this week where I get to dive back into &lt;a href=&#34;https://github.com/jalapic/engsoccerdata&#34;&gt;James Curley’s engsoccerdata package&lt;/a&gt; asks which teams have finished second in their league the most.&lt;/p&gt;
&lt;p&gt;First lets load up the engsoccerdata for English leagues 1882-2016. I’ve munged it in a pretty verbose way; there’s definitely a faster way to do it but that’s not really necessary. All we need are the indicators used to sort the league (points, goal difference, and goal scored) for every match in a long format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eng_data &amp;lt;- engsoccerdata::england %&amp;gt;%
  #select only pertinent variables
  select(Date, Season, home, visitor, hgoal, vgoal, division) %&amp;gt;%
  rename_all(tolower) %&amp;gt;%
  #melt the data to long format
  reshape2::melt(id.vars = c(&amp;quot;date&amp;quot;, &amp;quot;season&amp;quot;, &amp;quot;hgoal&amp;quot;, &amp;quot;vgoal&amp;quot;, &amp;quot;division&amp;quot;),
                 value.name = &amp;quot;team&amp;quot;, variable.name = &amp;quot;location&amp;quot;) %&amp;gt;%
  #this can be done in one step but for sanity
  mutate(result = case_when(
    hgoal &amp;gt; vgoal &amp;amp; location == &amp;quot;home&amp;quot; ~ &amp;quot;W&amp;quot;,
    vgoal &amp;gt; hgoal &amp;amp; location == &amp;quot;visitor&amp;quot; ~ &amp;quot;W&amp;quot;,
    hgoal &amp;lt; vgoal &amp;amp; location == &amp;quot;home&amp;quot; ~ &amp;quot;L&amp;quot;,
    vgoal &amp;lt; hgoal &amp;amp; location == &amp;quot;visitor&amp;quot; ~ &amp;quot;L&amp;quot;,
    vgoal == hgoal ~ &amp;quot;D&amp;quot;
  )) %&amp;gt;%
  #points for a win changed in 1981
  mutate(points = case_when(
    result == &amp;quot;L&amp;quot; ~ 0,
    result == &amp;quot;D&amp;quot; ~ 1,
    result == &amp;quot;W&amp;quot; &amp;amp; season &amp;lt; 1981 ~ 2,
    result == &amp;quot;W&amp;quot; &amp;amp; season &amp;gt; 1980 ~ 3
  )) %&amp;gt;%
  #and get the goal info too
  mutate(goal_diff = case_when(
    location == &amp;quot;home&amp;quot; ~ hgoal - vgoal,
    location == &amp;quot;visitor&amp;quot; ~ vgoal - hgoal
  )) %&amp;gt;%
  mutate(goals = case_when(
    location == &amp;quot;home&amp;quot; ~ hgoal,
    location == &amp;quot;visitor&amp;quot; ~ vgoal
  )) %&amp;gt;%
  #only save the variables we care about then sort
  select(date, season, division, team, points, goals, goal_diff) %&amp;gt;%
  arrange(date, team)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then find the final positions of each team in each season of English football sorted by points, goal difference and goals for&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_positions &amp;lt;- eng_data %&amp;gt;%
  setDT() %&amp;gt;%
  #find the match number
  .[, match := 1:.N, by = c(&amp;quot;season&amp;quot;, &amp;quot;team&amp;quot;)] %&amp;gt;%
  #find the cumulative points, goal difference and goals for
  .[, season_points := cumsum(points), by = c(&amp;quot;season&amp;quot;, &amp;quot;team&amp;quot;)] %&amp;gt;%
  .[, season_gd := cumsum(goal_diff), by = c(&amp;quot;season&amp;quot;, &amp;quot;team&amp;quot;)] %&amp;gt;%
  .[, season_g := cumsum(goals), by = c(&amp;quot;season&amp;quot;, &amp;quot;team&amp;quot;)] %&amp;gt;%
  #filter out the final matches totals and order
  .[.[, .I[match == max(match)], by= c(&amp;quot;season&amp;quot;, &amp;quot;division&amp;quot;)]$V1] %&amp;gt;%
  .[order(season, division, -season_points)] %&amp;gt;%
  #assign the final positions
  .[, final_position := 1:.N, by = c(&amp;quot;season&amp;quot;, &amp;quot;division&amp;quot;)] %&amp;gt;%
  .[, c(&amp;quot;team&amp;quot;, &amp;quot;division&amp;quot;, &amp;quot;final_position&amp;quot;)] %&amp;gt;%
  #count by final position
  .[, pos_count := .N, by = c(&amp;quot;team&amp;quot;, &amp;quot;division&amp;quot;, &amp;quot;final_position&amp;quot;)] %&amp;gt;%
  unique()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can filter out those who have finished second most&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;second_place &amp;lt;- final_positions %&amp;gt;%
  #filter out second place finishes
  .[final_position == 2] %&amp;gt;%
  .[, c(&amp;quot;team&amp;quot;, &amp;quot;division&amp;quot;, &amp;quot;final_position&amp;quot;, &amp;quot;pos_count&amp;quot;)] %&amp;gt;%
  .[order(-pos_count)]

head(data.frame(second_place), 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 team division final_position pos_count
## 1  Manchester United        1              2        14
## 2            Arsenal        1              2        12
## 3          Liverpool        1              2        11
## 4        Aston Villa        1              2         9
## 5   Sheffield United        2              2         8
## 6    Birmingham City        2              2         8
## 7            Everton        1              2         7
## 8         Sunderland        1              2         7
## 9  Preston North End        1              2         6
## 10   Manchester City        1              2         6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perhaps unsurprisingly, most of the teams to finish second have finished second in the top flight. Manchester United lead the way with Arsenal and Liverpool following up.&lt;/p&gt;
&lt;p&gt;Bristol City, Charlton Athletic, Oldham Athletic, Blackpool, QPR, Watford and Southampton have finished runners up in the top division without winning it, all having achieved this exactly once.&lt;/p&gt;
&lt;p&gt;Plymouth Argyle have perhaps the most heartbreaking run of all though- having finished second in the old 3rd Division South &lt;a href=&#34;https://en.wikipedia.org/wiki/Plymouth_Argyle_F.C.#Honours&#34;&gt;SIX times in a row between 1922-1927&lt;/a&gt; before finally winning it in 1929.&lt;/p&gt;
&lt;p&gt;I thought I might as well also plot every teams league finishes as a proportion of their season in the league. Position here refers to total overall position (so 1st in Division two might be 21st overall). The darker the colour, the more likely the team was the end the season in that position. All teams have been sorted by their mean final league position.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eng_data %&amp;gt;%
  #filter out modern era
  filter(season &amp;gt; 1991) %&amp;gt;%
  setDT() %&amp;gt;%
  #find finish positions and count as above
  .[, match := 1:.N, by = c(&amp;quot;season&amp;quot;, &amp;quot;team&amp;quot;)] %&amp;gt;%
  .[, season_points := cumsum(points), by = c(&amp;quot;season&amp;quot;, &amp;quot;team&amp;quot;)] %&amp;gt;%
  .[, season_gd := cumsum(goal_diff), by = c(&amp;quot;season&amp;quot;, &amp;quot;team&amp;quot;)] %&amp;gt;%
  .[, season_g := cumsum(goals), by = c(&amp;quot;season&amp;quot;, &amp;quot;team&amp;quot;)] %&amp;gt;%
  .[.[, .I[match == max(match)], by= c(&amp;quot;season&amp;quot;, &amp;quot;division&amp;quot;)]$V1] %&amp;gt;%
  .[order(season, division, -season_points)] %&amp;gt;%
  .[, final_position := 1:.N, by = c(&amp;quot;season&amp;quot;, &amp;quot;division&amp;quot;)] %&amp;gt;%
  .[, total_position := 1:.N, by = c(&amp;quot;season&amp;quot;)] %&amp;gt;%
  .[, c(&amp;quot;team&amp;quot;, &amp;quot;division&amp;quot;, &amp;quot;final_position&amp;quot;, &amp;quot;total_position&amp;quot;)] %&amp;gt;%
  .[, pos_count := .N, by = c(&amp;quot;team&amp;quot;, &amp;quot;division&amp;quot;, &amp;quot;final_position&amp;quot;)] %&amp;gt;%
  .[, team_appearances := .N, by = c(&amp;quot;team&amp;quot;)] %&amp;gt;%
  .[, mean_pos := sum(total_position)/team_appearances, by = c(&amp;quot;team&amp;quot;)] %&amp;gt;%
  unique() %&amp;gt;%
  #order by mean position
  .[order(mean_pos)] %&amp;gt;%
  .[, team := fct_rev(fct_relevel(as.factor(team), unique(.$team)))] %&amp;gt;%
  #plot
  ggplot(., aes(x = total_position, y = team)) + 
  geom_tile(aes(alpha = pos_count/team_appearances), fill = &amp;quot;blue&amp;quot;) +
  scale_alpha_continuous(guide = FALSE) +
  ggtitle(&amp;quot;Teams Ordered by Mean Final Position 1992-2016&amp;quot;,
          subtitle = &amp;quot;Weight indicates proportion of finishes in that position&amp;quot;) +
  xlab(&amp;quot;Total League Position&amp;quot;) +
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-05-The_Knowledge_3_files/figure-html/all_final_positions-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There’s probably too much data to graph here, but it’s still a fun way to look at 140 years of English football&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;slip-slidin-away&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Slip Slidin’ Away&lt;/h1&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
As we&#39;re starting to reach that time of season (we&#39;ll, we are at &lt;a href=&#34;https://twitter.com/hashtag/itfc?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#itfc&lt;/a&gt;)… What&#39;s the earliest &lt;em&gt;collectively&lt;/em&gt; a team from each of the top 4 English leagues has been relegated?
&lt;/p&gt;
— Philip Genochio (&lt;span class=&#34;citation&#34;&gt;@philipgenochio&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/philipgenochio/status/1092782376852156416?ref_src=twsrc%5Etfw&#34;&gt;February 5, 2019&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;answer-rochdale-with-8-games-to-go-in-division-3-in-19731974&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Answer: Rochdale with 8 games to go in Division 3 in 1973/1974&lt;/h3&gt;
&lt;p&gt;A similar question involves the earliest teams to get relegated. Obviously for this first we need to know how many teams are relegated from each league per season. Having only really started watching football around the turn of the millenium I was a bit surprised how much this has changed over the years (n.b. I’m only counting automatic relegation- playoffs and re-elections don’t count).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#manually enter the number of relegation spots per league
relegation_spots &amp;lt;- eng_data %&amp;gt;%
  .[, c(&amp;quot;season&amp;quot;, &amp;quot;division&amp;quot;)] %&amp;gt;%
  unique() %&amp;gt;%
  mutate(relegation_spots = case_when(
    season &amp;gt;= 1995 &amp;amp; division == 1 ~ 3,
    season &amp;gt;= 1994 &amp;amp; division == 1 ~ 4,
    season &amp;gt;= 1991 &amp;amp; division == 1 ~ 3,
    season &amp;gt;= 1990 &amp;amp; division == 1 ~ 2,
    season &amp;gt;= 1973 &amp;amp; division == 1 ~ 3,
    season &amp;gt;= 1898 &amp;amp; division == 1 ~ 2,
    season &amp;gt;= 1995 &amp;amp; division == 2 ~ 3,
    season &amp;gt;= 1994 &amp;amp; division == 2 ~ 4,
    season &amp;gt;= 1991 &amp;amp; division == 2 ~ 3,
    season &amp;gt;= 1990 &amp;amp; division == 2 ~ 2,
    season &amp;gt;= 1988 &amp;amp; division == 2 ~ 3,
    season &amp;gt;= 1986 &amp;amp; division == 2 ~ 2,
    season &amp;gt;= 1973 &amp;amp; division == 2 ~ 3,
    season &amp;gt;= 1921 &amp;amp; division == 2 ~ 2,
    season &amp;gt;= 1920 &amp;amp; division == 2 ~ 1,
    season &amp;gt;= 1919 &amp;amp; division == 2 ~ 3,
    season &amp;gt;= 1995 &amp;amp; division == 3 ~ 4,
    season &amp;gt;= 1994 &amp;amp; division == 3 ~ 5,
    season &amp;gt;= 1991 &amp;amp; division == 3 ~ 4,
    season &amp;gt;= 1990 &amp;amp; division == 3 ~ 3,
    season &amp;gt;= 1988 &amp;amp; division == 3 ~ 4,
    season &amp;gt;= 1986 &amp;amp; division == 3 ~ 3,
    season &amp;gt;= 1958 &amp;amp; division == 3 ~ 4,
    season &amp;gt;= 2002 &amp;amp; division == 4 ~ 2,
    season &amp;gt;= 1996 &amp;amp; division == 4 ~ 1,
    season &amp;gt;= 1993 &amp;amp; division == 4 ~ 0,
    season &amp;gt;= 1992 &amp;amp; division == 4 ~ 1,
    season &amp;gt;= 1990 &amp;amp; division == 4 ~ 0,
    season &amp;gt;= 1986 &amp;amp; division == 4 ~ 1
 )) %&amp;gt;%
  mutate(relegation_spots = ifelse(relegation_spots == 0, NA, relegation_spots))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then need to work out how many points each team has, and how many they could possibly achieve, after every match in a season.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;possible_positions &amp;lt;- eng_data %&amp;gt;%
    setDT() %&amp;gt;%
  #get the match number
  .[, match := 1:.N, by = c(&amp;quot;season&amp;quot;, &amp;quot;team&amp;quot;)] %&amp;gt;%
  #get the current points for the team
  .[, season_points := cumsum(points), by = c(&amp;quot;season&amp;quot;, &amp;quot;team&amp;quot;)] %&amp;gt;%
  .[order(division, season, match, -season_points)] %&amp;gt;%
  #get the current position for the team
  .[, position := 1:.N, by = c(&amp;quot;division&amp;quot;, &amp;quot;season&amp;quot;, &amp;quot;match&amp;quot;)] %&amp;gt;%
  #how many teams are in the league
  .[, teams := max(position), by = c(&amp;quot;division&amp;quot;, &amp;quot;season&amp;quot;)] %&amp;gt;%
  #find how many matches each team has left to play
  .[, matches_remaining := max(match) - match, by = c(&amp;quot;division&amp;quot;, &amp;quot;season&amp;quot;)] %&amp;gt;%
  #the max points assumes each team wins all of their remaining matches
  .[season &amp;lt; 1981, possible_points := season_points + (matches_remaining * 2)] %&amp;gt;%
  .[season &amp;gt; 1980, possible_points := season_points + (matches_remaining * 3)] %&amp;gt;%
  #merge in the relegation spots and find what position each team needs to be safe
  merge(., relegation_spots, by = c(&amp;quot;division&amp;quot;, &amp;quot;season&amp;quot;)) %&amp;gt;%
  .[, lowest_safe_position := teams - relegation_spots] %&amp;gt;%
  #the threshold for safety is the number of points the lowest safe team has
  .[position == lowest_safe_position, lowest_safe_points := season_points]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then it’s a simple case of finding teams in the relegation zone and finding the point at which they can no longer catch the lowest safe team&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;relegation_secured &amp;lt;- possible_positions %&amp;gt;%
  #filter out teams in relegation trouble
  .[!is.na(lowest_safe_position)] %&amp;gt;%
  .[position &amp;gt;= lowest_safe_position] %&amp;gt;%
  .[, lowest_safe_points := na.locf(lowest_safe_points)] %&amp;gt;%
  .[possible_points &amp;lt; lowest_safe_points] %&amp;gt;%
    .[, c(&amp;quot;season&amp;quot;, &amp;quot;division&amp;quot;, &amp;quot;team&amp;quot;, &amp;quot;season_points&amp;quot;,
          &amp;quot;matches_remaining&amp;quot;)] %&amp;gt;%
  .[order(-matches_remaining)] %&amp;gt;%
  #remove duplicates
  .[!duplicated(paste0(season, division, team))]

head(data.frame(relegation_secured), 15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    season division                team season_points matches_remaining
## 1    1973        3            Rochdale            16                 8
## 2    1984        1          Stoke City            17                 7
## 3    2001        2    Stockport County            17                 7
## 4    2003        2           Wimbledon            21                 7
## 5    2016        2    Rotherham United            17                 7
## 6    1961        3      Newport County            18                 7
## 7    1984        3    Cambridge United            17                 7
## 8    1993        3              Barnet            23                 7
## 9    2000        3       Oxford United            22                 7
## 10   1930        1   Manchester United            16                 6
## 11   1954        1 Sheffield Wednesday            19                 6
## 12   1975        1    Sheffield United            13                 6
## 13   1994        1        Ipswich Town            23                 6
## 14   2007        1        Derby County            11                 6
## 15   1952        2            Barnsley            17                 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So Rochdale hold the questionable honour of being the team knowing they are doomed with the most matches to go (with 8 in division 3 in 1973). There’s quite a large chasing pack of teams who have known with 7 or 6 matches left too. Ipswich are currently ‘only’ 8 points off of safety with 16 games left to go so seems unlikely to beat 8 but it could be close…&lt;/p&gt;
&lt;p&gt;I also wanted to see what the earliest a team has ever been certain of their final position is.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;certain_final_positions &amp;lt;- possible_positions %&amp;gt;%
  #find the possible points for the teams above and below each team
  .[, poss_points_nextworst := lead(possible_points), by = c(&amp;quot;season&amp;quot;, &amp;quot;division&amp;quot;, &amp;quot;match&amp;quot;)] %&amp;gt;%
  .[, points_nextbest := lag(season_points), by = c(&amp;quot;season&amp;quot;, &amp;quot;division&amp;quot;, &amp;quot;match&amp;quot;)] %&amp;gt;%
  #filter out teams that cannot beat/fall below the next best/worst teams
  .[(is.na(poss_points_nextworst) | season_points &amp;gt; poss_points_nextworst) &amp;amp; 
      (is.na(points_nextbest) | possible_points &amp;lt; points_nextbest) &amp;amp;
      matches_remaining &amp;gt; 0] %&amp;gt;%
  #order and select columns
  .[order(-matches_remaining)] %&amp;gt;%
  .[, c(&amp;quot;division&amp;quot;, &amp;quot;season&amp;quot;, &amp;quot;team&amp;quot;, &amp;quot;position&amp;quot;, &amp;quot;matches_remaining&amp;quot;, &amp;quot;teams&amp;quot;)]

head(data.frame(certain_final_positions), 15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    division season                 team position matches_remaining teams
## 1         4   1968 Bradford Park Avenue       24                 7    24
## 2         1   1984           Stoke City       22                 6    22
## 3         2   1949    Tottenham Hotspur        1                 6    22
## 4         2   1971              Watford       22                 6    22
## 5         2   1973        Middlesbrough        1                 6    22
## 6         2   2001     Stockport County       24                 6    24
## 7         3   1966  Queens Park Rangers        1                 6    24
## 8         3   1984     Cambridge United       24                 6    24
## 9        3b   1952              Walsall       24                 6    24
## 10        4   1968 Bradford Park Avenue       24                 6    24
## 11        4   1977              Watford        1                 6    24
## 12        4   1997         Notts County        1                 6    24
## 13        1   1980       Crystal Palace       22                 5    22
## 14        1   1982            Liverpool        1                 5    22
## 15        1   1984              Everton        1                 5    22&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Spare a thought for fans of Bradford Park Avenue in 1968-1969 who knew their team would finish bottom of the 3rd Division North with 7 matches (of 46) remaining. Luckily they weren’t relegated as they were already in the bottom division of the football league and we re-elected for the next season. They repeated this feat, now in Division 4, 5 years later, finishing bottom with 6 games to go.&lt;/p&gt;
&lt;p&gt;Most of these involve teams either winning or finishing bottom of their league. If we filter these out we’re left with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;certain_final_positions %&amp;gt;%
  #filter off bottom or top teams
  .[position != 1 &amp;amp; position != teams] %&amp;gt;%
  data.frame() %&amp;gt;%
  head(., 15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    division season                team position matches_remaining teams
## 1        3a   1931        New Brighton       20                 4    22
## 2        3a   1931            Rochdale       21                 4    22
## 3         2   2003       Bradford City       23                 3    24
## 4         2   2005    Sheffield United        2                 3    24
## 5        3a   1931        New Brighton       20                 3    22
## 6        3a   1931            Rochdale       21                 3    22
## 7        3b   1929           Brentford        2                 3    22
## 8         4   1975    Northampton Town        2                 3    24
## 9         1   1888         Aston Villa        2                 2    12
## 10        1   1930         Aston Villa        2                 2    22
## 11        1   1934          Sunderland        2                 2    22
## 12        1   1946           Brentford       21                 2    22
## 13        1   1957   Preston North End        2                 2    22
## 14        1   1970             Burnley       21                 2    22
## 15        1   1978 Queens Park Rangers       20                 2    22&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which really emphasizes how only exceptionally good/bad teams are ever really certain of their position before the end of the season.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;draw-me-like-one-of-your-top-flight-teams&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Draw Me Like One of Your Top Flight Teams&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.theguardian.com/football/2019/feb/06/what-are-the-lowest-xg-scoring-football-matches-in-history-expected-goals-the-knowledge-football&#34;&gt;““Tottenham have currently played 29 consecutive Premier League games without drawing one,” notes Wouter van Dael. “What is the longest ever such league run?””&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;answer-aston-villa---50-games-in-18911892-but-otherwise-spurs-in-modern-football&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Answer: Aston Villa - 50 games in 1891/1892, but otherwise Spurs in modern football&lt;/h3&gt;
&lt;p&gt;(N.b. the engsoccerdata package only has data up until the 2016/2017 season, so Tottenham’s run won’t appear in the results below)&lt;/p&gt;
&lt;p&gt;To do this we just need to select every game with a non-zero goal difference (i.e. a draw) and then find consecutive runs for teams.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;draws &amp;lt;- eng_data %&amp;gt;%
  setDT() %&amp;gt;%
  .[order(team, date)] %&amp;gt;%
  #give each match a consecutive &amp;#39;id&amp;#39;
  .[, game_id := 1:.N, by = team] %&amp;gt;%
  #find matches with a non zero goal difference (not a draw)
  .[goal_diff != 0] %&amp;gt;%
  #find consecutive matches with non zero goal difference
  .[, consecutive := lead(game_id) - game_id, by = team] %&amp;gt;%
  .[consecutive != 1, consecutive := NA] %&amp;gt;%
  #count all consecutive runs
  .[, count := .N*!is.na(consecutive), rleid(!is.na(consecutive))] %&amp;gt;%
  #find the start and end of each run
  .[count != lead(count) | count != lag(count)] %&amp;gt;%
  .[order(-count, team, date)] %&amp;gt;%
  #set the start points and end point and spread to separate columns
  .[, run_point := c(&amp;quot;start&amp;quot;, &amp;quot;end&amp;quot;)] %&amp;gt;%
  spread(run_point, date) %&amp;gt;%
  .[, start := na.locf(start)] %&amp;gt;%
  .[!is.na(end) &amp;amp; !is.na(start)] %&amp;gt;%
  #select columns to print
  .[, c(&amp;quot;start&amp;quot;, &amp;quot;end&amp;quot;, &amp;quot;division&amp;quot;, &amp;quot;team&amp;quot;, &amp;quot;count&amp;quot;)] %&amp;gt;%
  .[order(-count)]

head(data.frame(draws), 25)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         start        end division                    team count
## 1  1891-01-01 1892-12-10        1             Aston Villa    50
## 2  1895-03-30 1896-11-09        1              Stoke City    45
## 3  1907-12-26 1909-02-27        1              Sunderland    45
## 4  1913-01-01 1914-02-14        2    Bradford Park Avenue    43
## 5  1909-01-30 1910-03-25        2          Leicester City    43
## 6  1894-01-06 1896-09-05        2                 Walsall    43
## 7  1892-12-10 1894-09-15        1         Birmingham City    42
## 8  1896-03-21 1897-09-25        2                  Darwen    39
## 9  1928-03-17 1929-02-02        1              Portsmouth    37
## 10 1904-10-22 1905-11-11        1        Sheffield United    37
## 11 1947-11-15 1948-09-25       3b          Bristol Rovers    36
## 12 1930-05-03 1931-03-28        2 Wolverhampton Wanderers    36
## 13 1894-03-23 1895-04-20        2            Lincoln City    35
## 14 1946-10-12 1947-08-23       3a        Stockport County    35
## 15 1904-01-30 1904-12-27        2            Bristol City    34
## 16 1915-04-03 1920-02-21        1    West Bromwich Albion    34
## 17 1934-12-01 1935-09-16        2        Doncaster Rovers    33
## 18 1895-11-16 1896-11-28        2        Newcastle United    33
## 19 1925-05-02 1926-02-22        2           Middlesbrough    32
## 20 1935-08-31 1936-03-14       3b                 Reading    32
## 21 1925-12-26 1926-10-02       3a                Rochdale    32
## 22 1896-01-20 1897-02-13        2        Burton Wanderers    31
## 23 1927-12-24 1928-09-15       3a      Accrington Stanley    30
## 24 1905-11-04 1906-09-03        2    Gainsborough Trinity    30
## 25 1946-11-23 1947-08-25        2         Plymouth Argyle    30&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tottenham’s run without a draw doesn’t even make the top 25 such runs! And they still would have to wait until at least next season until they can match Aston Villa’s run from New Years Day 1891 until Christmas Eve in 1892, a run of 50 matches without a draw- a run that included 23 losses, and 27 wins. It is the longest for quite sometime though- there’s few similar runs in the post-war years.&lt;/p&gt;
&lt;p&gt;We can restrict this easily to just runs in the top division my modifying one line&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;draws &amp;lt;- eng_data %&amp;gt;%
  setDT() %&amp;gt;%
  .[order(team, date)] %&amp;gt;%
  .[, game_id := 1:.N, by = team] %&amp;gt;%
  #filter out only the top division matches
  .[division == 1] %&amp;gt;%
  .[goal_diff != 0] %&amp;gt;%
  .[, consecutive := lead(game_id) - game_id, by = team] %&amp;gt;%
  .[consecutive != 1, consecutive := NA] %&amp;gt;%
  .[, count := .N*!is.na(consecutive), rleid(!is.na(consecutive))] %&amp;gt;%
  .[count != lead(count) | count != lag(count)] %&amp;gt;%
  .[order(-count, team, date)] %&amp;gt;%
  .[, run_point := c(&amp;quot;start&amp;quot;, &amp;quot;end&amp;quot;)] %&amp;gt;%
  spread(run_point, date) %&amp;gt;%
  .[, start := na.locf(start)] %&amp;gt;%
  .[!is.na(end) &amp;amp; !is.na(start)] %&amp;gt;%
  .[, c(&amp;quot;start&amp;quot;, &amp;quot;end&amp;quot;, &amp;quot;division&amp;quot;, &amp;quot;team&amp;quot;, &amp;quot;count&amp;quot;)] %&amp;gt;%
  .[order(-count)]

head(data.frame(draws), 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         start        end division                    team count
## 1  1891-01-01 1892-12-10        1             Aston Villa    50
## 2  1895-03-30 1896-11-09        1              Stoke City    45
## 3  1907-12-26 1909-02-27        1              Sunderland    45
## 4  1928-03-17 1929-02-02        1              Portsmouth    37
## 5  1904-10-22 1905-11-11        1        Sheffield United    37
## 6  1915-04-03 1920-02-21        1    West Bromwich Albion    34
## 7  1895-09-28 1896-09-12        1 Wolverhampton Wanderers    29
## 8  1964-09-12 1965-03-31        1             Aston Villa    28
## 9  1953-04-25 1954-01-02        1                 Burnley    28
## 10 1891-03-14 1892-04-30        1              Sunderland    28&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where we can see that Spurs’ run is at least the longest modern top flight drawless run.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Riddler 1st February 2019</title>
      <link>/post/riddler-1st-feb-2019/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/riddler-1st-feb-2019/</guid>
      <description>


&lt;div id=&#34;riddler-classic&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Riddler Classic&lt;/h1&gt;
&lt;p&gt;In my spare time &lt;a href=&#34;http://www.robert-hickman.eu/post/riddler-27th-april-2018/&#34;&gt;I enjoy solving&lt;/a&gt; &lt;a href=&#34;https://fivethirtyeight.com/tag/the-riddler/&#34;&gt;538’s The Riddler column&lt;/a&gt;. This week I had a spare few hours waiting for the Superbowl to start and decided to code up a solution to the latest problem to keep me busy.&lt;/p&gt;
&lt;p&gt;The question revolves around a card game in which whatever choice a player makes, they are likely to lose to a con artist. Formally this is phrased as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You meet someone on a street corner who’s standing at a table on which there are three decks of playing cards. He tells you his name is “Three Deck Monte.” Knowing this will surely end well, you inspect the decks. Each deck contains 12 cards …&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Red Deck: four aces, four 9s, four 7s Blue Deck: four kings, four jacks, four 6s Black Deck: four queens, four 10s, four 8s The man offers you a bet: You pick one of the decks, he then picks a different one. You both shuffle your decks, and you compete in a short game similar to War. You each turn over cards one at a time, the one with a higher card wins that turn (aces are high), and the first to win five turns wins the bet. (There can’t be ties, as no deck contains any of the same cards as any other deck.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Should you take the bet? After all, you can pick any of the decks, which seems like it should give you an advantage against the dealer. If you take the bet, and the dealer picks the best possible counter deck each time, how often will you win?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Obviously if you’ve ever seen a trick like this you’ll know you shouldn’t. But what is the probability you lose?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(Rcpp)

#set up the parameters
deck_names &amp;lt;- c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;black&amp;quot;)
decks &amp;lt;- list(
  c(rep(14, 4), rep(9, 4), rep(7, 4)),
  c(rep(13, 4), rep(11, 4), rep(6, 4)),
  c(rep(12, 4), rep(10, 4), rep(8, 4))
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in tidy R we can easily simulate a game using a quickly written function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;play_game &amp;lt;- function(deck_player, deck_grifter) {
  #shuffle the decks
  deck_player &amp;lt;- sample(decks[[grep(deck_player, deck_names)]])
  deck_grifter &amp;lt;- sample(decks[[grep(deck_grifter, deck_names)]])
  
  #set the point to zero
  points_player &amp;lt;- 0
  points_grifter &amp;lt;- 0
  
  #set the turn to 0
  n &amp;lt;- 1
  
  #keep drawing cards until one player wins 5 times
  while(points_grifter &amp;lt; 5 &amp;amp; points_player &amp;lt; 5) {
    if(deck_player[n] &amp;gt; deck_grifter[n]) {
      points_player &amp;lt;- points_player + 1
    } else {
      points_grifter &amp;lt;- points_grifter + 1
    }
    
    #and update the turn 
    n &amp;lt;- n + 1
  }
  
  if(points_player &amp;gt; points_grifter) {
    return(1)
  } else {
    return(0)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, there’s not much learnt from just answering these question as easily/quickly as possible, so I frequently try and write out my solutions using the &lt;a href=&#34;https://cran.r-project.org/web/packages/Rcpp/index.html&#34;&gt;Rcpp package&lt;/a&gt; from Dirk Edelbuettel which allows for C++ integration into R.&lt;/p&gt;
&lt;p&gt;For problems like this it isn’t reaaalllly necessary, but it’s good practice nonetheless.&lt;/p&gt;
&lt;p&gt;The equivalent Rcpp function looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#first declare the type_of_output function_name(type_of_input argument) up top
cppFunction(&amp;#39;int play_gameC(NumericVector player_deck, NumericVector grifter_deck) {
  //shuffle the decks
  std::random_shuffle(player_deck.begin(), player_deck.end());
  std::random_shuffle(grifter_deck.begin(), grifter_deck.end());
  
  //initialise
  int turn = 0;
  int points_player = 0;
  int points_grifter = 0;

  //play each round
  while(points_player &amp;lt; 5 &amp;amp;&amp;amp; points_grifter &amp;lt; 5) {
    int player_card = player_deck(turn);
    int grifter_card = grifter_deck(turn);

    if(player_card &amp;gt; grifter_card)
    {
      points_player = points_player + 1;
    } 
    else 
    {
      points_grifter = points_grifter + 1;
    }

    turn = turn + 1;
  }
  
  //say default result =0 and update when player wins
  int result = 0;

  if(points_player &amp;gt; points_grifter) {
    result = result + 1;
  }

//return the game result
  return result;
}&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;all that’s left is to rerun this a load of times. Fortunately purrr makes this super easy&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#create a df of all deck combinations
combinations &amp;lt;- data.frame(player_choice = deck_names,
                           grifter_choice = deck_names) %&amp;gt;%
    expand(player_choice, grifter_choice) %&amp;gt;%
    filter(player_choice != grifter_choice)

#choose how many games to play
number_of_games &amp;lt;- 10000

#find how often the player wins for each deck choice
results &amp;lt;- rerun(number_of_games,
                 map2(combinations$player_choice,
                      combinations$grifter_choice,
                      play_game)
                 ) %&amp;gt;%
  unlist(.) %&amp;gt;%
  matrix(ncol = number_of_games) %&amp;gt;%
  #as a proportion of games
  rowSums() / number_of_games 

#mutate this back onto the combinations
combinations &amp;lt;- combinations %&amp;gt;%
  mutate(win_chance = results) %&amp;gt;%
  print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   player_choice grifter_choice win_chance
##   &amp;lt;fct&amp;gt;         &amp;lt;fct&amp;gt;               &amp;lt;dbl&amp;gt;
## 1 black         blue                0.299
## 2 black         red                 0.701
## 3 blue          black               0.704
## 4 blue          red                 0.306
## 5 red           black               0.295
## 6 red           blue                0.709&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So whatever deck you pick you have a 70% chance of losing providing the grifter has memorized the winning counter-deck. What an unfortunate state of affairs.&lt;/p&gt;
&lt;p&gt;Giving we’ve written the C++ code (which I didn’t use in the end to run my model), it’s worth seeing what the speed advantage would have been. If you’re eagle-eyed you might notice that play_game and play_gameC have a slightly different way to defining the decks (the R function selects based on name in the first line) so I also wrote a play_gameR function that functions the same as the C++ one for a fairer comparison.&lt;/p&gt;
&lt;p&gt;We can benchmark these using the microbenchmark&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;microbenchmark&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#we&amp;#39;ll just use deck1 vs deck2 for the example
deck1 &amp;lt;- decks[[1]]
deck2 &amp;lt;- decks[[2]]

microbenchmark(
  play_gameR(deck1, deck2),
  play_gameC(deck1, deck2),
  times = 10000
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: microseconds
##                      expr   min    lq      mean median     uq      max
##  play_gameR(deck1, deck2) 8.388 9.847 14.202272 10.576 13.858 7745.265
##  play_gameC(deck1, deck2) 1.094 1.459  2.333585  1.824  2.553  815.771
##  neval
##  10000
##  10000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we can see that despite being very similar, the C++ code is much faster. For problems like this it makes no difference (a mean of 13 vs. 2ms isn’t going to be noticeable to a human except on very large numbers of reruns), but it’s fun to know how to get some free speed out of code in any case.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8nTFjVm9sTQ&#34;&gt;Radiohead - House of Cards&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;riddler-express---can-you-escape-a-maze-without-walls&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Riddler Express - Can You Escape a Maze Without Walls&lt;/h1&gt;
&lt;p&gt;I also completed the riddler express which involves a maze. Obviously look away if you don’t want it spoiled, but the key is to work backwards and see there’s 1 clear fastest path. You can complete the maze in 42 moves. I’ve tried to show my messy working&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/riddler_maze.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Which English County Has Won the Most Points</title>
      <link>/post/counties_league_points/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/counties_league_points/</guid>
      <description>


&lt;p&gt;Every so often a question on The Guardian’s &lt;a href=&#34;https://www.theguardian.com/football/series/theknowledge&#34;&gt;The Knowledge&lt;/a&gt; football trivia section piques my interest and is amenable to analysis using R. Previously, I looked at &lt;a href=&#34;http://www.robert-hickman.eu/post/the-knowledge-4th-august-2018/&#34;&gt;club name suffixes and young World Cup winners&lt;/a&gt; last August. This week (give or take), a question posed on twitter caught my attention:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
&lt;a href=&#34;https://twitter.com/TheKnowledge_GU?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@TheKnowledge_GU&lt;/span&gt;&lt;/a&gt; was just chatting to some colleagues in the kitchen at work about why Essex doesn&#39;t have many big football clubs and it got me thinking. If you combined all the points from every league team in the ceremonial counties in England, which county would be on top?
&lt;/p&gt;
— BoxBoron (&lt;span class=&#34;citation&#34;&gt;@Rutland_Walker&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/Rutland_Walker/status/1082641231853899781?ref_src=twsrc%5Etfw&#34;&gt;January 8, 2019&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;To start with as always load the libraries needed to analyse this&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get data
library(engsoccerdata)
library(rvest)
#munging
library(tidyverse)
library(magrittr)
#spatial analysis
library(sf)
library(rgdal)
#for plotting maps
library(ggthemes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The easiest way to get a total of points is using the engsoccerdata:: packages database of every English football match from the top four divisions (this does not include data from the 2017-2018, or 2018-2019 seasons). We can work out the points easily from the goals scored for each team&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load the data
match_data &amp;lt;- engsoccerdata::england %&amp;gt;%
  #select only the necessary columns and melt
  select(season = Season, home, visitor, hgoal, vgoal, tier) %&amp;gt;%
  reshape2::melt(id.vars = c(&amp;quot;season&amp;quot;, &amp;quot;hgoal&amp;quot;, &amp;quot;vgoal&amp;quot;, &amp;quot;tier&amp;quot;),
                 variable.name = &amp;quot;location&amp;quot;,
                 value.name = &amp;quot;team&amp;quot;) %&amp;gt;%
  #will need to match this to location data so some club names need cleaning
  mutate(team_subbed = case_when(
    team == &amp;quot;Yeovil&amp;quot; ~ &amp;quot;Yeovil Town&amp;quot;,
    team == &amp;quot;AFC Bournemouth&amp;quot; ~ &amp;quot;A.F.C. Bournemouth&amp;quot;,
    team == &amp;quot;Halifax Town&amp;quot; ~ &amp;quot;F.C. Halifax Town&amp;quot;,
    team == &amp;quot;Aldershot&amp;quot; ~ &amp;quot;Aldershot Town F.C&amp;quot;,
    team == &amp;quot;Wimbledon&amp;quot; ~ &amp;quot;A.F.C. Wimbledon&amp;quot;,
    team == &amp;quot;AFC Wimbledon&amp;quot; ~ &amp;quot;A.F.C. Wimbledon&amp;quot;,
    team == &amp;quot;Macclesfield&amp;quot; ~ &amp;quot;Macclesfield Town&amp;quot;,
    team == &amp;quot;Rushden &amp;amp; Diamonds&amp;quot; ~ &amp;quot;A.F.C. Rushden &amp;amp; Diamonds&amp;quot;,
    team == &amp;quot;Milton Keynes Dons&amp;quot; ~ &amp;quot;Milton Keynes&amp;quot;,
    team == &amp;quot;Dagenham and Redbridge&amp;quot; ~ &amp;quot;Dagenham &amp;amp; Redbridge&amp;quot;,
    team == &amp;quot;Stevenage Borough&amp;quot; ~ &amp;quot;Stevenage&amp;quot;
  )) %&amp;gt;%
  #if cleaning isnt required, take original
  mutate(team_subbed = ifelse(is.na(team_subbed), team, team_subbed))

#peek at the data
head(match_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   season hgoal vgoal tier location            team     team_subbed
## 1   1888     1     1    1     home Accrington F.C. Accrington F.C.
## 2   1888     0     2    1     home Accrington F.C. Accrington F.C.
## 3   1888     2     3    1     home Accrington F.C. Accrington F.C.
## 4   1888     5     1    1     home Accrington F.C. Accrington F.C.
## 5   1888     6     2    1     home Accrington F.C. Accrington F.C.
## 6   1888     3     1    1     home Accrington F.C. Accrington F.C.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 388k (194k matches) data.frame seems daunting, but actually only results in many fewer unique teams that have played at least one match in the top 4 divisions in England&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(unique(match_data$team_subbed))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 141&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The location of each club can then be found using the wikipedia pages for them/their stadia. This matches 121 of the 141 clubs pretty nicely which is a fairly good percentage all things considered&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the links to each clubs wikipedia page
wiki &amp;lt;- read_html(&amp;quot;https://en.wikipedia.org/wiki/List_of_football_clubs_in_England&amp;quot;) %&amp;gt;%
  html_nodes(&amp;quot;td:nth-child(1)&amp;quot;) %&amp;gt;%
  .[which(grepl(&amp;quot;href&amp;quot;, .))]

#get the names for each club
wiki_clubs &amp;lt;- wiki %&amp;gt;% html_text() %&amp;gt;% gsub(&amp;quot; \\(.*\\)$&amp;quot;, &amp;quot;&amp;quot;, .)

#can match 121/141 right off the bat
(unique(match_data$team_subbed) %in% wiki_clubs) %&amp;gt;%
  which() %&amp;gt;%
  length()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 121&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can find the location of these matching clubs by finding the page for their stadia and then finding the coordinates. It’s a bit of a messy function because I was just jamming stuff together to get data out as best as possible. This takes ~1 minute to run through all 121 teams (for the blog post I actually saved an RDS of the output from this and load it just to save time/server calls)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matching_club_locations &amp;lt;- wiki %&amp;gt;% 
  #take only the matching clubs
  .[which(wiki_clubs %in% unique(match_data$team_subbed))] %&amp;gt;%
  html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;%
  #get the wiki page link
  html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
  paste0(&amp;quot;https://en.wikipedia.org&amp;quot;, .) %&amp;gt;%
  #for each club page find the stadium and its coordinates
  lapply(., function(team) {
    link &amp;lt;- read_html(team) %&amp;gt;%
      html_nodes(&amp;quot;.label a&amp;quot;) %&amp;gt;%
      .[1] %&amp;gt;%
      html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% 
      paste0(&amp;quot;https://en.wikipedia.org&amp;quot;,. )
    coords &amp;lt;- link %&amp;gt;%
      read_html() %&amp;gt;% 
      html_nodes(&amp;quot;#coordinates a&amp;quot;) %&amp;gt;%
      html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
      .[2]
    #if coords not found use NA
    if(is.na(coords)) {
      coord_df &amp;lt;- data.frame(lat = NA,
                             lon = NA)
    } else {
      coords &amp;lt;- coords %&amp;gt;%
        paste0(&amp;quot;https:&amp;quot;, .) %&amp;gt;%
        read_html() %&amp;gt;%
        html_nodes(&amp;quot;.geo&amp;quot;) %&amp;gt;%
        html_text() %&amp;gt;%
        strsplit(., split = &amp;quot;, &amp;quot;)
      coord_df &amp;lt;- data.frame(lat = as.numeric(coords[[1]][1]),
                             lon = as.numeric(coords[[1]][2]))
    }
    return(coord_df)
  })  %&amp;gt;%
  #bind everything together
  do.call(rbind, .) %&amp;gt;%
  #add the club name as a new column
  mutate(team = wiki_clubs[
    which(wiki_clubs %in% unique(match_data$team_subbed))
    ]) %&amp;gt;%
  #filter out missing data
  filter(!is.na(lat) &amp;amp; !is.na(lon))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which gives us the location of 114 of our 141 clubs. Most of the remaining ones are now-defunct clubs (e.g. Middlesbrough Ironopolis, Leeds City etc.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;missing_teams &amp;lt;- unique(match_data$team_subbed)[which(!unique(match_data$team_subbed) %in% matching_club_locations$team)]
missing_teams&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Accrington F.C.&amp;quot;           &amp;quot;Darwen&amp;quot;                   
##  [3] &amp;quot;Burton Swifts&amp;quot;             &amp;quot;Port Vale&amp;quot;                
##  [5] &amp;quot;Middlesbrough Ironopolis&amp;quot;  &amp;quot;Rotherham Town&amp;quot;           
##  [7] &amp;quot;Burton Wanderers&amp;quot;          &amp;quot;Loughborough&amp;quot;             
##  [9] &amp;quot;Blackpool&amp;quot;                 &amp;quot;New Brighton Tower&amp;quot;       
## [11] &amp;quot;Burton United&amp;quot;             &amp;quot;Leeds City&amp;quot;               
## [13] &amp;quot;Rotherham County&amp;quot;          &amp;quot;Bristol Rovers&amp;quot;           
## [15] &amp;quot;Darlington&amp;quot;                &amp;quot;Wigan Borough&amp;quot;            
## [17] &amp;quot;Aberdare Athletic&amp;quot;         &amp;quot;New Brighton&amp;quot;             
## [19] &amp;quot;Thames&amp;quot;                    &amp;quot;Aldershot Town F.C&amp;quot;       
## [21] &amp;quot;Hereford United&amp;quot;           &amp;quot;Scarborough&amp;quot;              
## [23] &amp;quot;Cheltenham&amp;quot;                &amp;quot;A.F.C. Rushden &amp;amp; Diamonds&amp;quot;
## [25] &amp;quot;Accrington&amp;quot;                &amp;quot;Crawley Town&amp;quot;             
## [27] &amp;quot;Fleetwood Town&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given it was a Saturday morning where I had nothing better to do, I simply located these clubs home grounds manually and created a data.frame for their locations. It’s not really great practice but whatever.&lt;/p&gt;
&lt;p&gt;These are then all bound together and converted to an sf spatial object with the correct projection&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#add in the missing locations
missing_locations &amp;lt;- data.frame(
  lat = c(53.7646, 53.711772, 52.799, 53.049722, 54.5641, 53.42644, 52.8146,
          52.7743, 53.804722, 53.4359, 52.799, 53.7778, 53.428367, 51.48622,
          54.508425, 53.554914, 51.7127, 53.4292, 51.514431, 51.248386,
          52.060719, 54.265478, 51.906158, 52.328033, 53.7646, 51.405083, 53.9165),
  lon = c(-2.358, -2.477292, -1.6354, -2.1925, -1.2456, -1.34377, -1.6335, -1.1992,
          -3.048056, -3.0377, -1.6354, -1.5722, -1.370231, -2.583134, -1.534394,
          -2.650661, -3.4374, -3.0407, 0.034739, -0.754789, -2.717711, -0.418247,
          -2.060211, -0.5999, -2.358, -0.281944, -3.0247),
    team = as.character(missing_teams)
)

#bind together and convert to sf
all_locations &amp;lt;- rbind(matching_club_locations,
                       missing_locations) %&amp;gt;%
  st_as_sf(coords = c(&amp;quot;lon&amp;quot;, &amp;quot;lat&amp;quot;), crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;)) %T&amp;gt;%
  #make a quick plot of locations for sanity check
  plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-21-The_Knowledge_2_files/figure-html/bind_missing_locations-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that we have all the teams, we need the English historical county boundaries to group them by. I’d actually already used these for football analysis, looknig at &lt;a href=&#34;https://www.citymetric.com/horizons/football-could-independent-yorkshire-win-world-cup-3961&#34;&gt;if an independent Yorkshire could win the World Cup&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Load the data (the boundary file can be download from the &lt;a href=&#34;https://www.ordnancesurvey.co.uk/business-and-government/products/boundaryline.html&#34;&gt;Ordnance Survey&lt;/a&gt;) and make a quick plot of the boundaries and teams&lt;/p&gt;
&lt;p&gt;(I also created an sf object engwal which is just the counties from England and Wales selected out for background plotting)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile 
## Source: &amp;quot;C:\Users\Alaa\Desktop\geo_data\boundary\Data\Supplementary_Ceremonial&amp;quot;, layer: &amp;quot;Boundary-line-ceremonial-counties_region&amp;quot;
## with 91 features
## It has 2 fields&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load the boundary file
counties &amp;lt;- readOGR(dsn = &amp;quot;path/to/file&amp;quot;,
                    layer = &amp;quot;county_boundaries&amp;quot;) %&amp;gt;%
  #convert to sf and project as northing/easting
  st_as_sf(., crs = st_crs(&amp;quot;+init=epsg:27700&amp;quot;)) %&amp;gt;%
  #only interested in the county name
  select(county = NAME) %&amp;gt;%
  #transform the projection to match that of the club locations
  st_transform(., crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;))

engwal &amp;lt;- counties %&amp;gt;%
  .[c(1:54, 88, 90),]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#make a quick plot of counties and teams
ggplot() +
  geom_sf(data = counties, fill = NA) +
  geom_sf(data = all_locations) +
  ggtitle(&amp;quot;Location of Teams to have Played Top\n 4 English Football Divisions&amp;quot;) +
  theme_minimal() +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-21-The_Knowledge_2_files/figure-html/plot_team_locations-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(by the way the artifacts around Bristol and the Wirral are from the OS dataset- it’s very annoying)&lt;/p&gt;
&lt;p&gt;Then we need to determine which teams are within which counties. The easiest way to do this is to use a spatial join of the team names in all_locations by which county they fall into (using st_contains from the sf package)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#bind the team names to each county
counties %&amp;lt;&amp;gt;%
  st_join(., all_locations, join = st_contains) %&amp;gt;%
  #remove counties that contain zero teams
  filter(!is.na(team)) %&amp;gt;%
  mutate(county = as.character(county))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## although coordinates are longitude/latitude, st_contains assumes that they are planar&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#quick plot of number of teams per county (missing = 0)
counties %&amp;gt;%
  group_by(county) %&amp;gt;%
  summarise(n_clubs = n()) %&amp;gt;%
  ggplot(data = .) +
  geom_sf(data = engwal) +
  geom_sf(aes(fill = n_clubs), colour = &amp;quot;black&amp;quot;) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;, name = &amp;quot;# clubs&amp;quot;) +
  ggtitle(&amp;quot;Number of Top 4 Division Playing\n Teams in each Ceremonial County&amp;quot;) +
  theme_minimal() +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-21-The_Knowledge_2_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which shows that most English historic counties (and a few Welsh ones due to teams like Cardiff City/ Swansea City etc.) have at least 1 team that has competed in the top 4 flights of English football at some point (those that do not are: Isle of Wight, Rutland, Surrey, Warwickshire, West Sussex and Cornwall).&lt;/p&gt;
&lt;p&gt;To finally get the total number of points won by these teams, the county data needs to be joined back onto the match data from the top. First I clean it up a bit then make the left_join by team name. Finally the number of points per match is calculated using case_when and points are grouped by county and summed&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county_match_data &amp;lt;- match_data %&amp;gt;% 
  mutate(team = team_subbed) %&amp;gt;%
  select(-team_subbed) %&amp;gt;%
  left_join(., counties, by = &amp;quot;team&amp;quot;) %&amp;gt;%
  mutate(points = case_when(
    location == &amp;quot;home&amp;quot; &amp;amp; hgoal &amp;gt; vgoal ~ 3,
    location == &amp;quot;visitor&amp;quot; &amp;amp; vgoal &amp;gt; hgoal ~ 3,
    location == &amp;quot;home&amp;quot; &amp;amp; hgoal &amp;lt; vgoal ~ 0,
    location == &amp;quot;visitor&amp;quot; &amp;amp; vgoal &amp;lt; hgoal ~ 0,
    hgoal == vgoal ~ 1
  ))

county_points &amp;lt;- county_match_data %&amp;gt;%
  group_by(county) %&amp;gt;%
  summarise(total_points = sum(points))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perhaps unsurprisingly, the county with the most points is Greater London, with Greater Manchester following and other footballing hotspots/ large counties in the West Midlands, Lancashire and around Yorkshire in the trailing group&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(arrange(county_points, -total_points))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   county             total_points
##   &amp;lt;chr&amp;gt;                     &amp;lt;dbl&amp;gt;
## 1 Greater London            67189
## 2 Greater Manchester        47203
## 3 West Midlands             37413
## 4 Lancashire                30808
## 5 South Yorkshire           30061
## 6 West Yorkshire            24947&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By contrast, Worcestshire and Northumberland barely have any points, with a few Welsh counties also struggling&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(arrange(county_points, total_points))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   county          total_points
##   &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;
## 1 Worcestershire           275
## 2 Northumberland           398
## 3 Mid Glamorgan            744
## 4 Somerset                 813
## 5 Gloucestershire          994
## 6 Herefordshire           1739&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we group by tier as well as county, it’s possible to see how well each county has done at specific tiers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county_match_data %&amp;gt;%
  group_by(county, tier) %&amp;gt;%
  summarise(total_points = sum(points)) %&amp;gt;%
  left_join(.,
            select(counties, county),
            by = &amp;quot;county&amp;quot;) %&amp;gt;%
  ggplot(data = .) +
  geom_sf(data = engwal) +
  geom_sf(aes(fill = total_points), colour = &amp;quot;black&amp;quot;) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;, name = &amp;quot;total points&amp;quot;) +
  ggtitle(&amp;quot;Number of Points Won by each County\n per Tier of English Football&amp;quot;) +
  facet_wrap(~tier) +
  theme_minimal() +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-21-The_Knowledge_2_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And for the Premier League era this clears up to&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county_match_data %&amp;gt;%
  filter(season &amp;gt; 1991) %&amp;gt;%
  group_by(county, tier) %&amp;gt;%
  summarise(total_points = sum(points)) %&amp;gt;%
  left_join(.,
            select(counties, county),
            by = &amp;quot;county&amp;quot;) %&amp;gt;%
  ggplot(data = .) +
  geom_sf(data = engwal) +
  geom_sf(aes(fill = total_points), colour = &amp;quot;black&amp;quot;, name = &amp;quot;total points&amp;quot;) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;) +
  ggtitle(&amp;quot;Number of Points Won by each County\n per Tier of English Football&amp;quot;,
          subtitle = &amp;quot;From Begining of 1992/1993 Season&amp;quot;) +
  facet_wrap(~tier) +
  theme_minimal() +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Ignoring unknown parameters: name&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-21-The_Knowledge_2_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which shows just how dominant London has been in the top division of English football (especially as it is only competitive at lower levels).&lt;/p&gt;
&lt;p&gt;I had wanted to weight points by the average ELO of that league and see which county has the most weight-adjusted points but got bored for this small blog post.&lt;/p&gt;
&lt;p&gt;Best,&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting the 2018-19 Women&#39;s Super League Using xG and Dixon-Coles</title>
      <link>/post/wsl-prediction-1/</link>
      <pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/wsl-prediction-1/</guid>
      <description>


&lt;p&gt;Over the last few years since I started coding I’d always been interested in how data science could help predict football results/ identify footballing talents, and just generally ‘solve’ football.&lt;/p&gt;
&lt;p&gt;One of the major problems with analysing football had been the availability of data. Though there’s a lot of great published stuff freely available to read, a lot of the cutting edge work revolves around &lt;a href=&#34;https://www.optasports.com/services/analytics/advanced-metrics/&#34;&gt;advanced metrics&lt;/a&gt;, such as expected goals, which it’s hard to get the data for.&lt;/p&gt;
&lt;p&gt;Over the summer StatsBomb committed to sharing &lt;a href=&#34;https://statsbomb.com/resource-centre/&#34;&gt;free data&lt;/a&gt; on (amongst others) the &lt;a href=&#34;http://www.fawsl.com/index.html&#34;&gt;Women’s Super League&lt;/a&gt; (the top women’s competition in England), and I’d been interested in looking into this since then.&lt;/p&gt;
&lt;p&gt;This post is basically just a reproduction of two blogs by the excellent &lt;a href=&#34;https://twitter.com/torvaney?lang=en&#34;&gt;Ben Torvaney&lt;/a&gt;, using the &lt;a href=&#34;https://dashee87.github.io/football/python/predicting-football-results-with-statistical-modelling-dixon-coles-and-time-weighting/&#34;&gt;Dixon-Coles method&lt;/a&gt; to predict the the final positions of teams at the end of a football season.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;http://www.statsandsnakeoil.com/2018/06/22/dixon-coles-and-xg-together-at-last/&#34;&gt;first of these&lt;/a&gt; published over the summer combines this method with xG data and the &lt;a href=&#34;http://www.statsandsnakeoil.com/2019/01/01/predicting-the-premier-league-with-dixon-coles/&#34;&gt;second&lt;/a&gt; from this week is a simple and fully reproducible tutorial on implementing Dixon-Coles.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(as both of these posts are pretty comprehensive I’m going to be sparse with commenting/explaining for this post- any questions will probably be answered by the above two articles)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;First, loading the libraries needed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

#Ben Torvaney&amp;#39;s soccer analysis packages
#devtools::install_github(&amp;quot;torvaney/footballdatr&amp;quot;)
library(footballdatr)
#devtools::install_github(&amp;quot;torvaney/regista&amp;quot;)
library(regista)

#women&amp;#39;s football data
#devtools::install_github(statsbomb/StatsBombR)
library(StatsBombR)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can use the StatsBombR package to download the data we need. First we grab a tibble of every match so far in the WSL season, and then use this to get another tibble of every shot from every game which we bind to the original as a new column.&lt;/p&gt;
&lt;p&gt;At the end, we save this, so we don’t have to bombard the API every time we want to rerun the script&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#only want to run this once to avoid overloading the API
#save data after pulling and set chunk eval = FALSE

#get free match info from the StatsBombR package
wsl_matches &amp;lt;- StatsBombR::FreeCompetitions() %&amp;gt;%
  #only interested in WSL
  filter(competition_name == &amp;quot;FA Women&amp;#39;s Super League&amp;quot;) %&amp;gt;%
  #find free matches from WSL
  #(all matches played so far)
  select(competition_id) %&amp;gt;%
  StatsBombR::FreeMatches(.) %&amp;gt;%
  #only want info that helps us predict scores
  select(match_id,
         competition.competition_id,
         season.season_id,
         home = home_team.home_team_name,
         away = away_team.away_team_name,
         hgoals = home_score,
         agoals = away_score)

#get the shot information from each match and bind a tibble as a column
wsl_matches$shots &amp;lt;- wsl_matches %&amp;gt;%
  #split match info into separate rows
  split(f = 1:nrow(.)) %&amp;gt;%
  #get the shots per game
  lapply(., function(x){
    StatsBombR::get.matchFree(x) %&amp;gt;%
      select(team = possession_team.name,
             xG = shot.statsbomb_xg) %&amp;gt;%
      filter(!is.na(xG)) %&amp;gt;%
      #join home/away information
      left_join(x %&amp;gt;% 
                  select(home, away) %&amp;gt;%
                  gather(location, team),
                ., by = &amp;quot;team&amp;quot;) %&amp;gt;%
      select(-team)
  })

#save data
#ONLY WANT TO RUN THIS CHUNK ONCE
saveRDS(wsl_matches, &amp;quot;saved_wsl_matches.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For every rerun we want to load this data again. I also got rid of the LFC/WFC suffixes from each team as it’s a bit redundant and changed the team name columns to factors, which will be required when modelling later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load saved data
wsl_matches &amp;lt;- readRDS(&amp;quot;./saved_wsl_matches.rds&amp;quot;) %&amp;gt;%
  mutate(home = gsub(&amp;quot; WFC| LFC&amp;quot;, &amp;quot;&amp;quot;, home),
         away = gsub(&amp;quot; WFC| LFC&amp;quot;, &amp;quot;&amp;quot;, away)) %&amp;gt;%
  mutate(home = factor(home), away = factor(away))

#peek at what data we have
head(wsl_matches)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
##   match_id competition.com~ season.season_id home  away  hgoals agoals
##      &amp;lt;int&amp;gt;            &amp;lt;int&amp;gt;            &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;  &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;
## 1    19751               37                4 West~ Chel~      0      2
## 2    19727               37                4 Read~ Birm~      0      1
## 3    19719               37                4 West~ Read~      0      0
## 4    19731               37                4 West~ Yeov~      2      1
## 5    19730               37                4 Chel~ Brig~      2      0
## 6    19733               37                4 Birm~ Manc~      2      3
## # ... with 1 more variable: shots &amp;lt;list&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of these function comes pretty much verbatim from the &lt;a href=&#34;http://www.statsandsnakeoil.com/2019/01/01/predicting-the-premier-league-with-dixon-coles/&#34;&gt;first blog post by Ben Torvaney&lt;/a&gt;. We run through the shot data (and the expected goals for every shot over every match), and find the probability of every plausible score from these matches happening.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#http://www.statsandsnakeoil.com/2019/01/01/predicting-the-premier-league-with-dixon-coles/
add_if_missing &amp;lt;- function(data, col, fill = 0.0) {
  # Add column if not found in a dataframe
  # We need this in cases where a team has 0 shots (!)
  if (!(col %in% colnames(data))) {
    data[, col] &amp;lt;- fill
  }
  data
}

team_goal_probs &amp;lt;- function(xgs, side) {
  # Find P(Goals=G) from a set of xGs by the
  # poisson-binomial distribution
  # Use tidyeval to prefix column names with
  # the team&amp;#39;s side (&amp;quot;h&amp;quot;ome or &amp;quot;a&amp;quot;way)
  tibble(!!str_c(side, &amp;quot;goals&amp;quot;) := 0:length(xgs),
         !!str_c(side, &amp;quot;prob&amp;quot;)  := poisbinom::dpoisbinom(0:length(xgs), xgs))
}

simulate_game &amp;lt;- function(shot_xgs) {
  shot_xgs %&amp;gt;%
    split(.$location) %&amp;gt;%
    imap(~ team_goal_probs(.x$xG, .y)) %&amp;gt;%
    reduce(crossing) %&amp;gt;%
    # If there are no shots, give that team a 1.0 chance of scoring 0 goals
    add_if_missing(&amp;quot;homegoals&amp;quot;, 0) %&amp;gt;%
    add_if_missing(&amp;quot;homeprob&amp;quot;, 1) %&amp;gt;%
    add_if_missing(&amp;quot;awaygoals&amp;quot;, 0) %&amp;gt;%
    add_if_missing(&amp;quot;awayprob&amp;quot;, 1) %&amp;gt;%
    mutate(prob = homeprob * awayprob) %&amp;gt;%
    select(homegoals, awaygoals, prob)
}

simulated_games &amp;lt;- wsl_matches %&amp;gt;%
  mutate(simulated_probabilities = map(shots, simulate_game)) %&amp;gt;%
  select(match_id, home, away, simulated_probabilities) %&amp;gt;%
  unnest() %&amp;gt;%
  filter(prob &amp;gt; 0.001) %&amp;gt;%  # Keep the number of rows vaguely reasonable
  rename(hgoals = homegoals, agoals = awaygoals)

simulated_games&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,291 x 6
##    match_id home            away    hgoals agoals    prob
##       &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;           &amp;lt;fct&amp;gt;    &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
##  1    19751 West Ham United Chelsea      0      0 0.0869 
##  2    19751 West Ham United Chelsea      1      0 0.0420 
##  3    19751 West Ham United Chelsea      2      0 0.00755
##  4    19751 West Ham United Chelsea      0      1 0.212  
##  5    19751 West Ham United Chelsea      1      1 0.103  
##  6    19751 West Ham United Chelsea      2      1 0.0184 
##  7    19751 West Ham United Chelsea      3      1 0.00161
##  8    19751 West Ham United Chelsea      0      2 0.201  
##  9    19751 West Ham United Chelsea      1      2 0.0969 
## 10    19751 West Ham United Chelsea      2      2 0.0174 
## # ... with 1,281 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use this data to get estimates of each team’s offensive and defensive strengths using the Dixon-Coles method. From here on out I’m going to refer to “actual goals” as “accomplished” (the number of goals that in reality occurred either for or against a team so far this season) and use “expected goals” for xG.&lt;/p&gt;
&lt;p&gt;In the final tibble, “off” refers to the offensive strength (higher = good) and “def” the “defensive weakness” (i.e. higher is worse).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#ag_model
ag_model &amp;lt;- dixoncoles(
  hgoal = hgoals,
  agoal = agoals,
  hteam = home,
  ateam = away,
  data  = factor_teams(wsl_matches, c(&amp;quot;home&amp;quot;, &amp;quot;away&amp;quot;))
)

#xg_model
xg_model &amp;lt;- dixoncoles(
  hgoal   = hgoals,
  agoal   = agoals,
  hteam   = home,
  ateam   = away,
  weights = prob,
  data    = factor_teams(simulated_games, c(&amp;quot;home&amp;quot;, &amp;quot;away&amp;quot;))
)

#join these together
estimates &amp;lt;-
  inner_join(
    broom::tidy(ag_model),
    broom::tidy(xg_model),
    by = c(&amp;quot;parameter&amp;quot;, &amp;quot;team&amp;quot;),
    suffix = c(&amp;quot;_accomplished&amp;quot;, &amp;quot;_xg&amp;quot;)
  ) %&amp;gt;%
  mutate(value_accomplished = exp(value_accomplished),
         value_xg      = exp(value_xg))

# Preview results, ordered by the biggest difference
estimates %&amp;gt;%
  arrange(desc(abs(value_xg - value_accomplished))) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   parameter team                   value_accomplished value_xg
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;                               &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 def       Brighton &amp;amp; Hove Albion              2.65     1.57 
## 2 off       Arsenal                             2.75     2.03 
## 3 def       Yeovil Town                         3.10     2.41 
## 4 off       Chelsea                             0.788    1.42 
## 5 def       Liverpool                           1.73     1.23 
## 6 rho       &amp;lt;NA&amp;gt;                                1.48     0.996&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s quickly plot the expected and accomplished goals based strength for each team and facet by offense and defence:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot the Dixon-Coles strengths
estimates %&amp;gt;%
  arrange(desc(abs(value_xg - value_accomplished))) %&amp;gt;%
  filter(parameter %in% c(&amp;quot;def&amp;quot;, &amp;quot;off&amp;quot;)) %&amp;gt;%
  ggplot(aes(x = value_xg, y = value_accomplished)) +
  geom_abline(slope = 1, intercept = 0, linetype = &amp;quot;dotted&amp;quot;) +
  geom_text(aes(label = team), alpha = 0.7) +
  labs(title = &amp;quot;Dixon-Coles parameter estimates for WSL teams&amp;quot;,
       subtitle = &amp;quot;Based on ...&amp;quot;,
       x = &amp;quot;... expected goals&amp;quot;,
       y = &amp;quot;... accomplished goals&amp;quot;) +
  coord_equal(ratio=1) +
  theme_minimal() +
  facet_wrap(~parameter)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-04-wsl1_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt; So Arsenal who have been &lt;a href=&#34;https://www.arsenal.com/results?field_arsenal_team_target_id=5&#34;&gt;outrageously good&lt;/a&gt; so far this year have been overperforming in offense (accomplished goals-based Dixon-Coles assume they are better than they probably are), whereas relegation-candidate Yeovil have been hugely underperforming in defense.&lt;/p&gt;
&lt;p&gt;The next step is then to get a list of all unplayed fixtures and predict the probabilities of each score in these using the same models as above (re-stated in the chunk).&lt;/p&gt;
&lt;p&gt;From here we’re using code from the &lt;a href=&#34;http://www.statsandsnakeoil.com/2019/01/01/predicting-the-premier-league-with-dixon-coles/&#34;&gt;second blog post&lt;/a&gt; by Ben Torvaney.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get list of unplayed fixtures
unplayed_games &amp;lt;-
  crossing(home = wsl_matches$home,
           away = wsl_matches$home) %&amp;gt;%
  filter(home != away) %&amp;gt;%
  anti_join(wsl_matches, by = c(&amp;quot;home&amp;quot;, &amp;quot;away&amp;quot;))

#don&amp;#39;t need to reinitialise the models really 
#maybe makes it a bit easier to follow
xg_model &amp;lt;- dixoncoles(hgoals, agoals, home, away, data = simulated_games)
ag_model &amp;lt;- dixoncoles(hgoals, agoals, home, away, data = wsl_matches)

#predict future scorelines using these models
unplayed_xg_scorelines &amp;lt;-
  broom::augment(xg_model, unplayed_games, type.predict = &amp;quot;scorelines&amp;quot;) %&amp;gt;%
  unnest() %&amp;gt;%
  mutate(model = &amp;quot;expected goals&amp;quot;)

unplayed_ag_scorelines &amp;lt;-
  broom::augment(ag_model, unplayed_games, type.predict = &amp;quot;scorelines&amp;quot;) %&amp;gt;%
  unnest() %&amp;gt;%
  mutate(model = &amp;quot;accomplished goals&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bind the data from these predictions to the data from the played games in one big tibble&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;played_scorelines &amp;lt;-
  wsl_matches %&amp;gt;%
  select(home, away, hgoal = hgoals, agoal = agoals) %&amp;gt;%
  mutate(prob = 1.0)

scorelines &amp;lt;- bind_rows(
  mutate(played_scorelines, model = &amp;quot;expected goals&amp;quot;),
  mutate(played_scorelines, model = &amp;quot;accomplished goals&amp;quot;),
  unplayed_xg_scorelines,
  unplayed_ag_scorelines
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These functions again are pretty much verbatim copied from Ben’s work. They simulate the season using Monte-Carlo sampling and build a final predicted WSL table&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simulate_season &amp;lt;- function(scoreline_probabilities) {
  scoreline_probabilities %&amp;gt;%
    nest(hgoal, agoal, prob, .key = &amp;quot;scorelines&amp;quot;) %&amp;gt;%
    mutate(sampled = map(scorelines, ~ sample_n(., 1, weight = prob))) %&amp;gt;%
    select(-scorelines) %&amp;gt;%
    unnest()
}

calculate_table &amp;lt;- function(games) {
  games_augmented &amp;lt;-
    games %&amp;gt;%
    mutate(
      hpoints = case_when(
        hgoal &amp;gt; agoal  ~ 3,
        hgoal == agoal ~ 1,
        agoal &amp;gt; hgoal  ~ 0
      ),
      apoints = case_when(
        hgoal &amp;gt; agoal  ~ 0,
        hgoal == agoal ~ 1,
        agoal &amp;gt; hgoal  ~ 3
      )
    )

  games_home &amp;lt;-
    games_augmented %&amp;gt;%
    select(
      team   = home,
      gf     = hgoal,
      ga     = agoal,
      points = hpoints
    )

  games_away &amp;lt;-
    games_augmented %&amp;gt;%
    select(
      team   = away,
      gf     = agoal,
      ga     = hgoal,
      points = apoints
    )

  bind_rows(games_home, games_away) %&amp;gt;%
    group_by(team) %&amp;gt;%
    summarise(w  = sum(gf &amp;gt; ga),
              d  = sum(gf == ga),
              l  = sum(gf &amp;lt; ga),
              gf = sum(gf),
              ga = sum(ga),
              gd = gf - ga,
              points = sum(points)) %&amp;gt;%
    arrange(desc(points), desc(gd), desc(gf)) %&amp;gt;%
    mutate(position = row_number())
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then it’s simply a case of running these function n times. 1000 for each model is a nice balance of predictive power and runtime.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_simulations &amp;lt;- 1000

simulated_tables &amp;lt;- scorelines %&amp;gt;%
  split(.$model) %&amp;gt;%
  lapply(., function(sim_data) {
    rerun(n_simulations, simulate_season(sim_data)) %&amp;gt;%
      map(calculate_table) %&amp;gt;%
      bind_rows(.id = &amp;quot;simulation_id&amp;quot;) %&amp;gt;%
      #add which model we&amp;#39;re using to each sim
      mutate(model = unique(sim_data$model))
  }) %&amp;gt;%
  do.call(rbind, .)

simulated_tables&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 22,000 x 11
##    simulation_id team      w     d     l    gf    ga    gd points position
##  * &amp;lt;chr&amp;gt;         &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;
##  1 1             Arse~    17     0     3    81    14    67     51        1
##  2 1             Manc~    15     4     1    67     9    58     49        2
##  3 1             Chel~    12     7     1    33     5    28     43        3
##  4 1             Birm~    14     1     5    35    15    20     43        4
##  5 1             Read~    10     4     6    30    24     6     34        5
##  6 1             West~     7     2    11    28    42   -14     23        6
##  7 1             Bris~     7     2    11    17    36   -19     23        7
##  8 1             Brig~     5     1    14    15    46   -31     16        8
##  9 1             Ever~     4     3    13    15    43   -28     15        9
## 10 1             Live~     4     2    14    13    36   -23     14       10
## # ... with 21,990 more rows, and 1 more variable: model &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally we can see how many points each team is predicted to achieve at the end of the season for each model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_colour &amp;lt;- &amp;quot;#1a4990&amp;quot;
n_teams &amp;lt;- length(unique(wsl_matches$home))

simulated_tables %&amp;gt;%
  count(team, points, model) %&amp;gt;%
  ggplot(aes(x = points, y = reorder(team, points))) +
  geom_tile(aes(alpha = n / n_simulations),
            fill = plot_colour) +
  scale_alpha_continuous(range = c(0, 1), name = &amp;quot;probability&amp;quot;) +
  labs(title = &amp;quot;Points total probabilities for the Women&amp;#39;s Super League&amp;quot;,
       subtitle = paste(&amp;quot;n =&amp;quot;, n_simulations, &amp;quot;simulations each&amp;quot;),
       x = &amp;quot;final season points&amp;quot;,
       y = NULL) +
  theme_minimal() +
  facet_wrap(~model)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-04-wsl1_files/figure-html/plot_season-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both models show similar results (most likely Arsenal winning and Yeovil relegated). However, at roughly the season halfway point there are some interesting discrepancies. For instance the xG model is more bullish on Chelsea than the accomplished goals one, and more bearish on Bristol.&lt;/p&gt;
&lt;p&gt;There’s a lot of analysis that can be done here (and if I have time will get round to looking at) but for now I’m pretty satisfied with this as just a very slight synthesis of xG Dixon-Coles and Dixon-Coles tutorial.&lt;/p&gt;
&lt;p&gt;As one final addendum we can look at the probabilities each model assumes for the team to be relegated this year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simulated_tables %&amp;gt;%
  group_by(team, model) %&amp;gt;%
  summarise(p_rel = mean(position == 11)) %&amp;gt;%
  filter(p_rel &amp;gt; 0.01) %&amp;gt;%
  arrange(desc(p_rel))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
## # Groups:   team [3]
##   team                   model              p_rel
##   &amp;lt;fct&amp;gt;                  &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;
## 1 Yeovil Town            expected goals     0.898
## 2 Yeovil Town            accomplished goals 0.818
## 3 Brighton &amp;amp; Hove Albion accomplished goals 0.17 
## 4 Brighton &amp;amp; Hove Albion expected goals     0.064
## 5 Everton                expected goals     0.034
## 6 Everton                accomplished goals 0.012&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which shows broad agreement between the two models. Yeovil are in real trouble. The expected goals model might give some relief to Brighton fans though.&lt;/p&gt;
&lt;p&gt;Doing the same for the eventual champion:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simulated_tables %&amp;gt;%
  group_by(team, model) %&amp;gt;%
  summarise(p_rel = mean(position == 1)) %&amp;gt;%
  filter(p_rel &amp;gt; 0.01) %&amp;gt;%
  arrange(desc(p_rel))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 3
## # Groups:   team [2]
##   team            model              p_rel
##   &amp;lt;fct&amp;gt;           &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;
## 1 Arsenal         expected goals     0.891
## 2 Arsenal         accomplished goals 0.801
## 3 Manchester City accomplished goals 0.199
## 4 Manchester City expected goals     0.099&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Chelsea only sneak into the mix for the expected goals model but it doesn’t look likely they’ll win the league. In the accomplished goals model there may be a slight title race (3/4 that Arsenal win, 1/4 that Man City overtake them), but using expected goals, Arsenal should be pretty confident of winning the league this season.&lt;/p&gt;
&lt;p&gt;That’s all for now. Will hopefully do a lot more football analysis in the coming year(s) first to expand on this post then look at other stuff.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TidyTuesday Week One</title>
      <link>/post/tidytuesday-2019-1/</link>
      <pubDate>Wed, 02 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday-2019-1/</guid>
      <description>


&lt;p&gt;Given it’s the new year, I decided to try and get back onto more regular posting on this blog (mostly just to build up a portfolio of work).&lt;/p&gt;
&lt;p&gt;A quick way to get something to work with that can be published unpolished is &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#TidyTuesday&lt;/a&gt; on twitter which (as far as I know/can tell) is organised by &lt;a href=&#34;https://twitter.com/thomas_mock&#34;&gt;Thomas Mock&lt;/a&gt; from RStudio.&lt;/p&gt;
&lt;p&gt;This week, the data comes in the form of a massive corpus of every tweet using the #rstats hashtag, curated by rtweet package creator Mike Kearney.&lt;/p&gt;
&lt;p&gt;I’m only going to leave sparse notes as this is just a post from some lunchtime work cleaned up and published after. I probably won’t fully spellcheck it either.&lt;/p&gt;
&lt;p&gt;First, libraries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#for data #tidytuesday data manipulation
library(tidyverse)
#used for clustering later
library(lsa)
library(e1071)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When loading the data, the first thing I decided to look at was the evolution of the hashtags use over time. As far as I can tell, first used in spring 2009 by &lt;a href=&#34;https://twitter.com/gappy3000&#34;&gt;Giuseppe Paleologo&lt;/a&gt;. Since then, it’s grown pretty exponentially.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#data at https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-01
#rstats_data &amp;lt;- readRDS(&amp;quot;../../Downloads/rstats_tweets.rds&amp;quot;)

#quickly plot tweets over time
p &amp;lt;- rstats_data %&amp;gt;%
  select(created_at) %&amp;gt;%
  arrange(created_at) %&amp;gt;%
  mutate(total_tweets = row_number()) %&amp;gt;%
  ggplot(., aes(x = created_at, y = total_tweets)) +
  geom_line() +
  xlab(&amp;quot;Date&amp;quot;) +
  ylab(&amp;quot;Total #rstats Tweets&amp;quot;) +
  ggtitle(&amp;quot;#rstats Tweets Over Time&amp;quot;) +
  theme_minimal()

p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-02-tidytuesday1_files/figure-html/peek_data-1.png&#34; width=&#34;672&#34; /&gt; I decided only to work with the most prolific #rstats tweeters, mostly to save space in plots as the corpus contains over 26k unique persons and 430k tweets&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#filter out people who tweet about rstats &amp;gt;=500 times
rstats_data %&amp;lt;&amp;gt;% 
  group_by(user_id) %&amp;gt;%
  mutate(tweet_count = n()) %&amp;gt;%
  filter(tweet_count &amp;gt; 499) %&amp;gt;%
  ungroup() %&amp;gt;%
  arrange(-tweet_count) %&amp;gt;%
  #also filter out feeds
  filter(!screen_name %in% c(&amp;quot;CRANberriesFeed&amp;quot;, &amp;quot;Rbloggers&amp;quot;, &amp;quot;rweekly_live&amp;quot;, &amp;quot;tidyversetweets&amp;quot;))

#plot the number of tweets per person
p2 &amp;lt;- rstats_data %&amp;gt;%
  ggplot(., aes(x = reorder(user_id, tweet_count))) +
  geom_bar(stat = &amp;quot;count&amp;quot;) +
  ggtitle(&amp;quot;Rstats Tweets By Person&amp;quot;) +
  xlab(&amp;quot;User&amp;quot;) +
  ylab(&amp;quot;Tweets&amp;quot;) +
  theme_minimal() +
  theme(axis.text.x = element_blank())

p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-02-tidytuesday1_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt; Lets see the most prolific tweeters&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#show the most prolific retweeters
rstats_users &amp;lt;- rstats_data %&amp;gt;%
  select(screen_name, tweet_count) %&amp;gt;%
  unique()

head(rstats_users)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   screen_name tweet_count
##   &amp;lt;chr&amp;gt;             &amp;lt;int&amp;gt;
## 1 AndySugs           8216
## 2 dataandme          4113
## 3 gp_pulipaka        3237
## 4 DerFredo           3091
## 5 revodavid          2640
## 6 MangoTheCat        2523&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I had been interested in recreating some analyses from &lt;a href=&#34;https://www.jtimm.net/2018/11/03/twitter-political-ideology-and-the-115-us-senate/&#34; class=&#34;uri&#34;&gt;https://www.jtimm.net/2018/11/03/twitter-political-ideology-and-the-115-us-senate/&lt;/a&gt; recently, and thought this gave a good oppurtunity.&lt;/p&gt;
&lt;p&gt;First I needed the top levels domains of links in #rstats tweets&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#try to find only top level domains for grouping
domain_patterns &amp;lt;- &amp;quot;\\.com.*|\\.org.*|\\.me.*|\\.gl.*|\\.li.*|\\..appspot|\\.blogspot|\\.io.*&amp;quot;
links &amp;lt;- data.frame(url = unlist(rstats_data$urls_url)) %&amp;gt;%
  mutate(domain = gsub(domain_patterns, &amp;quot;&amp;quot;, url)) %&amp;gt;%
  filter(!is.na(domain)) %&amp;gt;%
  group_by(domain) %&amp;gt;%
  mutate(share_count = n()) %&amp;gt;%
  ungroup()

#which are the most tweeted links by the top tweeters
head(links %&amp;gt;% select(-url) %&amp;gt;% unique() %&amp;gt;% arrange(-share_count))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   domain         share_count
##   &amp;lt;chr&amp;gt;                &amp;lt;int&amp;gt;
## 1 goo                   4724
## 2 wp                    4110
## 3 github                3430
## 4 twitter               3201
## 5 cran.r-project        2878
## 6 r-bloggers            2708&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;some of these (e.g. the google/wp/fb/bit.ly) ones seem a bit more to be quick links to pictures and so were removed. I also cut out links to amazon, google, facebook, and youtube, which I was less certain about doing and would probably analyse in a deeper cut.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#remove non-data sciencey links
links %&amp;gt;%
  filter(!grepl(&amp;quot;goo|wp|tweetedtimes|fb|htl|facebook|youtube|amazon|google&amp;quot;, domain)) %&amp;gt;%
  filter(!grepl(&amp;quot;activevoice.us|ift.tt|rviv.ly|bit.ly&amp;quot;, domain)) %&amp;gt;%
  select(-url) %&amp;gt;%
  unique() %&amp;gt;%
  arrange(-share_count) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   domain                   share_count
##   &amp;lt;chr&amp;gt;                          &amp;lt;int&amp;gt;
## 1 github                          3430
## 2 twitter                         3201
## 3 cran.r-project                  2878
## 4 r-bloggers                      2708
## 5 link.rweekly                    2415
## 6 blog.revolutionanalytics        1225&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we need to create a matrix of each domain vs. each user with a value of how many tweets from that user link to that domain.&lt;/p&gt;
&lt;p&gt;I selected 3 users to illustrate the finished matrix (from here on out I’m freely stealing code from the blogpost &lt;a href=&#34;https://www.jtimm.net/2018/11/03/twitter-political-ideology-and-the-115-us-senate/&#34;&gt;linked above&lt;/a&gt;)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find which domains each tweeted link belong to
rstats_domains_shared &amp;lt;- rstats_data %&amp;gt;%
  select(user_id, screen_name, url = urls_url, date = created_at) %&amp;gt;%
  #remove tweets without links
  filter(!is.na(url)) %&amp;gt;%
  #unlist the links
  #can be multiple per tweet
  splitstackshape::listCol_l(., listcol = &amp;quot;url&amp;quot;) %&amp;gt;%
  #merge with domain information
  merge(., unique(select(links, domain, url_ul = url, domain_shares = share_count)), by = &amp;quot;url_ul&amp;quot;) %&amp;gt;%
  #select only domains shared 100 or more times
  filter(domain_shares &amp;gt; 99) %&amp;gt;%
  #remove uninteresting domains
  filter(!grepl(&amp;quot;goo|wp|tweetedtimes|fb|htl|facebook|youtube|amazon|google&amp;quot;, domain)) %&amp;gt;%
  filter(!grepl(&amp;quot;activevoice.us|ift.tt|rviv.ly|bit.ly&amp;quot;, domain)) %&amp;gt;%
  #limit to only frequent tweeters
  filter(screen_name %in% rstats_users$screen_name)

#get a matrix of domains shared vs. users
rstats_shares_by_user &amp;lt;- rstats_domains_shared %&amp;gt;%
  #find the number of times each user tweets a link to a domain
  group_by(screen_name, domain) %&amp;gt;%
  summarize(share_count = n()) %&amp;gt;%
  #filter out those that are untweets
  filter(share_count &amp;gt; 0) %&amp;gt;%
  spread(screen_name, share_count) %&amp;gt;%
  replace(is.na(.), 0)  %&amp;gt;%
  ungroup()

#quickly glance at this
#has many columns so selecting only a few users
users &amp;lt;- c(&amp;quot;hadleywickham&amp;quot;, &amp;quot;drob&amp;quot;, &amp;quot;JennyBryan&amp;quot;)
rstats_shares_by_user %&amp;gt;%
  .[c(1, which(names(rstats_shares_by_user) %in% users))] %&amp;gt;%
  .[1:10,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 4
##    domain                    drob hadleywickham JennyBryan
##    &amp;lt;chr&amp;gt;                    &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
##  1 analyticsvidhya              0             0          0
##  2 andrewgelman                 0             0          0
##  3 arilamstein                  0             0          0
##  4 asbcllc                      0             0          0
##  5 bl.ocks                      0             0          0
##  6 blog.revolutionanalytics     0             5          0
##  7 blog.rstudio                 1           115          6
##  8 cran.r-project               6            12         21
##  9 cran.rstudio                 0             1          1
## 10 datasciencecentral           0             0          0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we use cosine from the lsa package to get a matrix of user-user similarity. This is then crushed down to two dimensions X1 and X2&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the cosine similarity between all users
cosine_rstats &amp;lt;- rstats_shares_by_user %&amp;gt;%
  select(2:ncol(.)) %&amp;gt;%
  data.matrix() %&amp;gt;%
  lsa::cosine(.)

#sort this into two dimensions
#X1 and X2
rstats_clustering &amp;lt;- cmdscale(1-cosine_rstats, eig = TRUE, k = 2)$points %&amp;gt;% 
  data.frame() %&amp;gt;%
  mutate(screen_name = rownames(cosine_rstats)) %&amp;gt;%
  merge(rstats_users, by = &amp;quot;screen_name&amp;quot;)

head(rstats_clustering)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       screen_name         X1         X2 tweet_count
## 1       _ColinFay -0.1192867 -0.2821199         989
## 2        abresler -0.1712703 -0.3224543        1443
## 3     AnalyticsFr -0.3210288  0.4201589        1386
## 4 AnalyticsFrance -0.3210288  0.4201589        1989
## 5 AnalyticsVidhya -0.2969805  0.4152374        1814
## 6        AndySugs  0.1371950  0.2465780        8216&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we plot this we get a nice graph of the top #rstats users qhich fall neatly into two dimensions. The first X1 seems to be ‘social’ vs. ‘professional’. People further to the left are users I recognise off the top of my head for sharing amateur data analyses/package building (e.g. JennyBryan) whereas those on the right seem to be more industrial users (e.g. MangoTheCat).&lt;/p&gt;
&lt;p&gt;The second dimension is a bit harder to gauge but strikes me as sort of software vs. data science divide with more package creators/rstudio employees towards the bottom and people doing analysis of data towards the top (but this is only a gut feeling).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot the users by their cosine similarity and number of tweets
rstats_clustering %&amp;gt;%
  ggplot(aes(X1,X2)) +
  geom_text(aes(label= screen_name, size = tweet_count), alpha = 0.3) +
  scale_size_continuous(range = c(2,5), guide = FALSE) +
  xlab(&amp;quot;Dimension X1&amp;quot;) +
  ylab(&amp;quot;Dimension X2&amp;quot;) +
  ggtitle(&amp;quot;#rstats Tweeters Arranged by Links Shared&amp;quot;,
          subtitle = &amp;quot;position based on cosine similarity between users&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-02-tidytuesday1_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To investigate a bit further I decided to see what each person was sharing. First I used c-means clustering as it’s something else I was working on in a separate project recently to cluster each use based on their cosine similarity (mostly just to have something to order the final plot by).&lt;/p&gt;
&lt;p&gt;I then used geom_tile to show how often each user was sending links from which domains. Roughly, I would say that the ‘industrial’ (green) cluster makes shows a concentration of links to sites such as &lt;a href=&#34;https://www.r-bloggers.com/&#34;&gt;r-bloggers&lt;/a&gt; and &lt;a href=&#34;https://blog.revolutionanalytics.com/&#34;&gt;revolutionanalytics’ blog&lt;/a&gt;, whereas the ‘social data science’ cluster (blue) links much more to twitter itself, github, and CRAN. The red (‘software’) cluster links to these too, but especially much more to the r-project blog in particular.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(22081992)
#use fuzzy c means to find clusters based on cosine similarity
#chose 3 as seems to be 3 clear nodes
c_grouping &amp;lt;- cmeans(select(rstats_clustering, X1, X2) %&amp;gt;% as.matrix(), 3, iter.max = 1000)

#merge this data in
rstats_clustering %&amp;lt;&amp;gt;%
  mutate(cluster = c_grouping$cluster) %&amp;gt;%
  cbind(as.data.frame(c_grouping$membership)) %&amp;gt;%
  mutate(cluster_membership = apply(.[, (ncol(.)-(max(.$cluster)-1)):ncol(.)], 1, max))

#plot a heatmap of links shared vs. cluster grouping
#remember cluster grouping is related to cosine similarity
rstats_shares_by_user %&amp;gt;%
  reshape2::melt(id.vars = &amp;quot;domain&amp;quot;, variable.name = &amp;quot;screen_name&amp;quot;, value.name = &amp;quot;shares&amp;quot;) %&amp;gt;%
  merge(rstats_clustering, by = &amp;quot;screen_name&amp;quot;) %&amp;gt;%
  filter(shares &amp;gt; 0) %&amp;gt;%
  ggplot(., aes(x = domain, y = reorder(screen_name, cluster + cluster_membership))) +
  geom_tile(aes(fill = log(shares), colour = factor(cluster)), size = 0.5) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;, guide = FALSE) +
  scale_colour_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;purple&amp;quot;), guide = FALSE) +
  xlab(&amp;quot;Domain Shared&amp;quot;) +
  ylab(&amp;quot;Screen Name&amp;quot;) +
  ggtitle(&amp;quot;Domains Shared by #rstats Tweeters Coloured by User Cluster&amp;quot;) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-02-tidytuesday1_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, I wanted to recreate the previous cosine similarity graph but with the clusters highlighted just because I think it makes a pretty graph.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#replot our initial plot of cosine similarity with the cluster information
#alpha of screen_name indicates group membership strength
rstats_clustering %&amp;gt;%
  ggplot(aes(X1, X2)) +
  geom_label(aes(label= screen_name, fill = factor(cluster), colour = cluster_membership, size = tweet_count), alpha = 0.3) +
  scale_colour_gradient(high = &amp;quot;black&amp;quot;, low = &amp;quot;white&amp;quot;, guide = FALSE) +
  scale_fill_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;purple&amp;quot;), guide = FALSE) +
  scale_size_continuous(range = c(2,5), guide = FALSE) +
  xlab(&amp;quot;Dimension X1&amp;quot;) +
  ylab(&amp;quot;Dimension X2&amp;quot;) +
  ggtitle(&amp;quot;#rstats Tweeters Grouped by Links Shared&amp;quot;,
          subtitle = &amp;quot;grouping based on cosine similarity between users&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-02-tidytuesday1_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s all for this post. I think I’ll keep on throwing up quick #TidyTuesday posts throughout the year which will be as sparse as this, but hopefully be interesting to one or two people.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Planning a Pub Crawl Using R</title>
      <link>/post/cambridge_pub_crawl/</link>
      <pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/cambridge_pub_crawl/</guid>
      <description>


&lt;p&gt;A few weeks ago I went on the first pub crawl I’d been on in years around my city of Cambridge. Around the same time I had also been visiting &lt;a href=&#34;https://www.google.co.uk/maps/@52.2046202,0.1289874,18z&#34;&gt;4 very good pubs within ~200m of each other&lt;/a&gt; tucked away in a quiet neighbourhood of the town. Together, I wondered if it was possible with freely avaiable data to plan an optimal pub crawl around any town/area of the UK, and also, if it would be feasbile to visit every pub within the city in a single day if travelling optimally.&lt;/p&gt;
&lt;p&gt;Once again, I found that the simple features library (sf) for R basically can do all of this pretty simply, with igraph picking up most of the networking slack. In fact, overall it was much much simpler than I thought it would be. In total, I only needed 9 libraries (though granted tidyverse is one).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#data munging
library(tidyverse)
library(magrittr)

#scrape pub data
library(rvest)
library(googleway)

#spatial manipulation
#almost all done in sf
library(sf)
library(rgdal)

#networking the pubs
library(igraph)
library(TSP)
library(geosphere)

rm(list=ls())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the data, I was able to get by using &lt;a href=&#34;https://www.ordnancesurvey.co.uk/opendatadownload/products.html#BDLINE&#34;&gt;Ordnance Survey data&lt;/a&gt; data for the spatial work. I used Boundary Line for the city boundaries (taking the Cambridge Boro Westminster constituency limits as the city boundaries), and OS OpenRoads for all the road work. These are freely avaiable via email using the link above.&lt;/p&gt;
&lt;p&gt;For reproducibility, these are both presented as if saved in path/to/os/data with folders called ./roads and ./boundary containing the extracted files from both of these.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;file_path &amp;lt;- &amp;quot;path/to/os/data&amp;quot;

#load the westminster constituency boundaries
cambridge &amp;lt;- readOGR(dsn = file.path(file_path, &amp;quot;boundary/Data/GB&amp;quot;), 
                     layer = &amp;quot;westminster_const_region&amp;quot;) %&amp;gt;%
  #convert to sf
  st_as_sf() %&amp;gt;%
  #select only the cambridge constituency
  filter(NAME == &amp;quot;Cambridge Boro Const&amp;quot;) %&amp;gt;%
  #get rid of associated data for cleanness
  select()

#load the road link and node data
#uses the uk national grid to partion road files
#https://en.wikivoyage.org/wiki/National_Grid_(Britain)
#cambridge is in grid TL
roads &amp;lt;- readOGR(dsn = file.path(file_path, &amp;quot;roads/data&amp;quot;), 
                     layer = &amp;quot;TL_RoadLink&amp;quot;) %&amp;gt;%
  st_as_sf() %&amp;gt;%
  #transform to the crs of the city boundary
  st_transform(st_crs(cambridge)) %&amp;gt;%
  #take only the roads which cross into the city
  .[unlist(st_intersects(cambridge, .)),]

nodes &amp;lt;- readOGR(dsn = file.path(file_path, &amp;quot;roads/data&amp;quot;), 
                     layer = &amp;quot;TL_RoadNode&amp;quot;)
#converting straight to sf gives an error so munge data manually
nodes &amp;lt;- cbind(nodes@data, nodes@coords) %&amp;gt;%
  st_as_sf(coords = c(&amp;quot;coords.x1&amp;quot;, &amp;quot;coords.x2&amp;quot;),
           crs = st_crs(&amp;quot;+init=epsg:27700&amp;quot;)) %&amp;gt;%
  st_transform(st_crs(cambridge)) %&amp;gt;%
  #take only nodes which are related to the roads we previously selected
  .[which(.$identifier %in% c(as.character(roads$startNode), 
                              as.character(roads$endNode))),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have these we can make a quick plot of the layout of the city&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#quickly plot the roads and nodes data
(p1 &amp;lt;- ggplot(cambridge) +
   geom_sf() +
   geom_sf(data = roads, colour = &amp;quot;black&amp;quot;) +
   geom_sf(data = nodes, colour = &amp;quot;red&amp;quot;, alpha = 0.5, size = 0.5) +
   theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/roads_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next we need point locations for all the pubs in Cambridge. Fortunately &lt;a href=&#34;www.pubsgalore.co.uk&#34;&gt;pubsgalore.co.uk&lt;/a&gt; has us covered with a pretty extensive list. It doesn’t contain the college bars of the University which is a bit of a shame, but is still a pretty good sample of 199 pubs in/around Cambridge.&lt;/p&gt;
&lt;p&gt;We want the name and address of every open pub which this will scrape.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#page with every pub in cambridge
pub_page &amp;lt;- &amp;quot;https://www.pubsgalore.co.uk/towns/cambridge/cambridgeshire/&amp;quot; %&amp;gt;%
  read_html()


open_pubs &amp;lt;- pub_page %&amp;gt;%
  html_nodes(&amp;quot;.pubicons .pubclosed&amp;quot;) %&amp;gt;%
  html_attr(&amp;quot;src&amp;quot;) %&amp;gt;%
  grep(&amp;quot;grey&amp;quot;, .)

pub_info &amp;lt;- pub_page %&amp;gt;%
  html_nodes(&amp;quot;#pagelist a&amp;quot;) %&amp;gt;%
  html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
  .[open_pubs] %&amp;gt;%
  paste0(&amp;quot;https://www.pubsgalore.co.uk&amp;quot;, .) %&amp;gt;%
  lapply(., function(single_pub) {
    pub_page_read &amp;lt;- single_pub %&amp;gt;%
      read_html()
    
    #get the name of the pub
    pub_name &amp;lt;- pub_page_read %&amp;gt;%
      html_nodes(&amp;quot;.pubname&amp;quot;) %&amp;gt;%
      html_text()
    
    #get the address of the pub 
    line1 &amp;lt;- pub_page_read %&amp;gt;% html_nodes(&amp;quot;.address&amp;quot;) %&amp;gt;% html_text()
    line2 &amp;lt;- pub_page_read %&amp;gt;% html_nodes(&amp;quot;.town&amp;quot;) %&amp;gt;% html_text()
    line3 &amp;lt;- pub_page_read %&amp;gt;% html_nodes(&amp;quot;.postcode&amp;quot;) %&amp;gt;% html_text()
    
    pub_address &amp;lt;- paste0(line1, &amp;quot;, &amp;quot;, line2, &amp;quot;, &amp;quot;, line3)
    
    #put together into data.frame
    pub_data &amp;lt;- data.frame(name = pub_name, address = pub_address)
    return(pub_data)
  }) %&amp;gt;%
  #rbind the lapply results
  do.call(rbind, .) %&amp;gt;%
  #remove duplicated pub addresses
  filter(!duplicated(address))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then need to geocode these adresses into coordinates. Because ggmap has been playing up for me, I tend to use the googleway package with a Google API key which you can get for free &lt;a href=&#34;https://developers.google.com/maps/documentation/javascript/get-api-key&#34;&gt;here&lt;/a&gt;. My key isn’t in the code published here for obvious reasons.&lt;/p&gt;
&lt;p&gt;These coordinates are then bound back onto the pub df and we filter out only the pubs which are located within the city limits (103).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pubs &amp;lt;- pub_info$address %&amp;gt;%
  #convert to character
  as.character() %&amp;gt;%
  #find the coords of every pub address using googleway
  lapply(., function(address) {
    #get the coords
    coords &amp;lt;- google_geocode(address, key = key) %&amp;gt;%
      .$results %&amp;gt;%
      .$geometry %&amp;gt;%
      .$location %&amp;gt;%
      #covert to df and add the address back
      data.frame() %&amp;gt;%
      mutate(address = address)
  }) %&amp;gt;%
  #rbind the results
  do.call(rbind, .) %&amp;gt;%
  #merge back in the pub names
  merge(pub_info, by = &amp;quot;address&amp;quot;) %&amp;gt;%
  #convert to sf
  st_as_sf(coords = c(&amp;quot;lng&amp;quot;, &amp;quot;lat&amp;quot;),
           crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;),
           remove = FALSE) %&amp;gt;%
  #convert to the same crs as the city shapefile
  st_transform(crs = st_crs(cambridge)) %&amp;gt;%
  #only take those which fall within the city shapefile
  #the postal district is a little large and extends into Cambridgeshire
  .[unlist(st_contains(cambridge, .)),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we plot these we can see that most are in the very centre of the city, with some sparsely distributed out in Trumpington (south), Cherry Hinton (east), and Arbury (north).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#quickly plot the roads and nodes data
(p2 &amp;lt;- ggplot(cambridge) +
   geom_sf() +
   geom_sf(data = roads, colour = &amp;quot;black&amp;quot;) +
   #pubs in blue
   geom_sf(data = pubs, colour = &amp;quot;blue&amp;quot;, size = 1.5) +
   theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/pubs_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We could just take the nearest roads to each pub and then easily create a node graph using the lookup between the roads and nodes in the OS data. However, I wanted to play around with manipulating the spatial data (this is a learning exercise after all) and so decided to see how ‘accurate’ I could get the distance on the optimal pub crawl path. In reality, the point locations given by google are probably slightly off anyway, but I’m going to ignore that.&lt;/p&gt;
&lt;p&gt;In order to include ‘half-roads’ (i.e. when the pub is halfway down a road you don’t want to walk the full length of the road), I need to first find the nearest point on &lt;em&gt;any&lt;/em&gt; road to each pub. dist2Line from the geosphere package does this nicely, though it does require turning our sf objects back into SpatialDataFrames.&lt;/p&gt;
&lt;p&gt;(this is by far the longest step in the script btw)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#convert to spatial for dist2Line
pubs_spatial &amp;lt;- st_transform(pubs, crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;)) %&amp;gt;%
  as_Spatial()
roads_spatial &amp;lt;- st_transform(roads, crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;)) %&amp;gt;%
  as_Spatial()

#finds the distance to each nearest line and that point
road_distances &amp;lt;- suppressWarnings(dist2Line(pubs_spatial, roads_spatial)) %&amp;gt;%
  as.data.frame() %&amp;gt;%
  #convert to sf
  st_as_sf(coords = c(&amp;quot;lon&amp;quot;, &amp;quot;lat&amp;quot;),
           crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;),
           remove = FALSE) %&amp;gt;%
  st_transform(crs = st_crs(cambridge))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Taking a peek at these reveals the distance of each pub the nearest road, and the ID of that road and the point on the road nearest the pub.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#display the first few of these
head(road_distances)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Simple feature collection with 6 features and 4 fields
## geometry type:  POINT
## dimension:      XY
## bbox:           xmin: 544907.5 ymin: 256223.8 xmax: 548593.7 ymax: 258690.9
## epsg (SRID):    NA
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.999601272 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.1502,0.247,0.8421,-20.4894 +units=m +no_defs
##     distance       lon      lat   ID                  geometry
## 1  5.8371470 0.1422209 52.20374 2696   POINT (546488.8 258331)
## 2  0.1801197 0.1391954 52.19889 2989   POINT (546298 257784.9)
## 3 29.6403166 0.1720762 52.18425 4059 POINT (548593.7 256223.8)
## 4 10.4782939 0.1223197 52.20734 1765 POINT (545117.3 258690.9)
## 5 32.6370002 0.1203945 52.20662 1757 POINT (544988.1 258606.7)
## 6 10.9321895 0.1191027 52.20427 1751 POINT (544907.5 258343.3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then want to break up the roads that these point lie on. To illustrate this, I’ll use the 3rd pub in the dataset, which is The Robin Hood in Cherry Hinton (as the plots look better and make more sense than the first two in my opinion).&lt;/p&gt;
&lt;p&gt;First we take the pub, and all roads and their nodes within 100m of the pub:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#take the first pub
x_pub &amp;lt;- pubs[3,]
#get all roads within 100m and their nodes
x_roads &amp;lt;- roads %&amp;gt;%
  .[unlist(st_intersects(st_buffer(x_pub, 100), .)),]
x_nodes &amp;lt;- nodes %&amp;gt;%
  .[which(.$nodeid %in% c(as.character(x_roads$start), 
                          as.character(x_roads$end))),]

#plot the roads local to this pub
(p3 &amp;lt;- ggplot() +
  geom_sf(data = x_roads) +
  geom_sf(data = x_nodes, colour = &amp;quot;red&amp;quot;, alpha = 0.5) +
  geom_sf(data = x_pub, colour = &amp;quot;blue&amp;quot;) +
  theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/splitting_roads1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We then use the point on any road nearest to this pub (green) as the ‘entrance’ of the pub (this may not strictly be the case and it might be possible to instead match road names to the address, but whatever).&lt;/p&gt;
&lt;p&gt;Using this point, we split up the road it lies on into two new separate roads (in orange and purple). To get to this pub you would have to travel down one of these to the green point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the nearest point that lies on a road
x_nearest_road_point &amp;lt;- road_distances[3,]

#split that road into two over that point
x_split_roads &amp;lt;- roads %&amp;gt;%
  .[which(.$id == x_nearest_road_point$ID),] %&amp;gt;%
  st_difference(., st_buffer(x_nearest_road_point, 0.2)) %&amp;gt;%
    st_cast(&amp;quot;LINESTRING&amp;quot;) 

#add to the plot
(p3 &amp;lt;- p3 + 
    geom_sf(data = x_split_roads[1,], colour = &amp;quot;purple&amp;quot;, size = 1.5, alpha = 0.5) +
    geom_sf(data = x_split_roads[2,], colour = &amp;quot;goldenrod&amp;quot;, size = 1.5, alpha = 0.5) +
    geom_sf(data = x_nearest_road_point, colour = &amp;quot;darkgreen&amp;quot;, size = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/splitting_roads2-1.png&#34; width=&#34;672&#34; /&gt; I could have used the green ‘entrance’ nodes for the travelling salesman part of the problem, but decided also to create roads from this ‘entrance’ to the geocoded location of the pub (blue). This is probably the equivalent of travelling from the pavement to the bar of each pub and worthy of consideration*.&lt;/p&gt;
&lt;p&gt;These roads are created by binding the green and blue points together from each pub, grouping them, and then casting a line between them.&lt;/p&gt;
&lt;p&gt;*another reason to take this into account is that some pubs may appear far away from roads. One example that I visit fairly often is &lt;a href=&#34;https://www.google.co.uk/maps/place/Fort+St+George/@52.2123057,0.1272911,18.75z/data=!4m5!3m4!1s0x47d870ecb4e8556d:0x3bfb0ff82c243075!8m2!3d52.2124367!4d0.1278073&#34;&gt;Fort St George&lt;/a&gt; which is on a river footpath and does not have direct road access.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#combine the green point on the roads with the point for the pub
x_pub_entrance &amp;lt;- select(x_nearest_road_point) %&amp;gt;%
  rbind(., select(x_pub)) %&amp;gt;%
  group_by(&amp;quot;pub road&amp;quot;) %&amp;gt;%
  summarise() %&amp;gt;%
  #cast to a line (for a new road)
  st_cast(&amp;#39;LINESTRING&amp;#39;)

#plot the pub entrance
(p3 &amp;lt;- p3 +
    geom_sf(data = x_pub_entrance, colour = &amp;quot;lightblue&amp;quot;, size = 1.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/splitting_roads3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#remove all the extra objects we created in the example
rm(list=ls()[grep(&amp;quot;x_&amp;quot;, ls())])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First we create all the new nodes (the green pub entrances, and the blue pub locations)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#add information to the pubs df
pubs &amp;lt;- pubs %&amp;gt;%
  mutate(pub = 1:nrow(.), 
         id = (max(nodes$id)+1):(max(nodes$id) + nrow(.)),
         nodeid = NA,
         class = &amp;quot;pubnode&amp;quot;)

#bind the pubs to the nodes data frame
nodes &amp;lt;- rbind(nodes, select(pubs, pub, nodeid, id, class))

#add the nodes found as the nearest road point to each pub to the nodes df
new_nodes &amp;lt;- road_distances %&amp;gt;%
  select() %&amp;gt;%
  mutate(pub = pubs$pub, 
         id = (max(nodes$id)+1):(max(nodes$id)+nrow(.)),
         nodeid = NA,
         class = &amp;quot;entrancenode&amp;quot;)
nodes &amp;lt;- rbind(nodes, new_nodes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we create all of the split roads in a for loop and all of the roads from the green to the blue points. These are bound back into the original roads data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#split up the roads that have a new node for the netrance of a pub
roads_2_split &amp;lt;- roads %&amp;gt;%
  slice(unique(road_distances$ID))
#leave the rest alone
roads &amp;lt;- roads %&amp;gt;%
  slice(-unique(road_distances$ID))

#for each new node split up the road that it bisects it 
#as we did in the example
for(node in seq(nrow(new_nodes))) {
  #find the road that the pub is nearest to
  split_road &amp;lt;- st_intersects(st_buffer(new_nodes[node,], .2),
                              roads_2_split) %&amp;gt;%
    unlist() %&amp;gt;%
    roads_2_split[.,]
  
  #split this road up
  split_roads &amp;lt;- st_difference(split_road, st_buffer(new_nodes[node,], .2)) %&amp;gt;%
    st_cast(&amp;quot;LINESTRING&amp;quot;) %&amp;gt;%
    select(start_id, end_id, id)

  #keep hold of the old id
  old_id &amp;lt;- unique(split_roads$id)
  
  #get rid of this road from the df
  roads_2_split &amp;lt;- roads_2_split %&amp;gt;%
    slice(-which(roads_2_split$id == old_id))

  #get the nodes for the old road
  old_nodes &amp;lt;- filter(nodes, id %in% c(unique(split_roads$start_id),
                                       unique(split_roads$end_id)))
  
  #add the correct nodes to the newly split road
  split_roads$start_id &amp;lt;- old_nodes$id[
      unlist(st_contains(st_buffer(split_roads, .2), old_nodes))
    ]
  split_roads$end_id &amp;lt;- new_nodes$id[node]
  
  #add in new information for the new road
  split_roads %&amp;lt;&amp;gt;%
    mutate(id = max(roads_2_split$id) + seq(nrow(split_roads)),
           class = &amp;quot;split road&amp;quot;,
           start = NA, 
           end = NA)
  #bind back to the original df
  roads_2_split &amp;lt;- rbind(roads_2_split, split_roads)
}

#bind the split roads to the original roads df
roads &amp;lt;- rbind(roads, roads_2_split)

#generate paths from the nearest point on a road to the pub gecoded location
#i.e. walking from the pavement to the bar itself
pub_roads &amp;lt;- select(road_distances) %&amp;gt;%
  #add in information and bind to equivalent pub points
  mutate(name = pubs$name, start_id = pubs$id, end_id = new_nodes$id) %&amp;gt;%
  rbind(., mutate(select(pubs, name, start_id = id), end_id = new_nodes$id)) %&amp;gt;%
  #group each pub together
  group_by(name, start_id, end_id) %&amp;gt;%
  summarise() %&amp;gt;%
  #cast to a line
  st_cast(&amp;#39;LINESTRING&amp;#39;) %&amp;gt;%
  ungroup() %&amp;gt;%
  #munge required information
  mutate(id = max(roads$id)+1 + seq(nrow(.)),
         start = NA,
         end = NA,
         class = &amp;quot;pub road&amp;quot;) %&amp;gt;%
  select(class, id, start_id, end_id, start, end)

#bind these into the original road df
roads &amp;lt;- rbind(roads, pub_roads)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that I have all the pubs and roads I want to traverse, I can move onto the &lt;a href=&#34;https://en.wikipedia.org/wiki/Travelling_salesman_problem&#34;&gt;travelling salesman&lt;/a&gt; portion of the problem- what is the shortest journey between all of them.&lt;/p&gt;
&lt;p&gt;For this, I need to use the igraph package and convert my df of roads (which contains the node at each end of every road) into a weighted node graph. Once I have this, I iterate through every combination of pubs and find the shortest path between the two and the vertices that comprise it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#add in the length between each pair of nodes using st_length
nodes_df &amp;lt;- roads %&amp;gt;%
  mutate(length = as.numeric(st_length(.))) %&amp;gt;%
  #select only the node ids and length between them
  select(start_id, end_id, length)
#get rid of the geometry
st_geometry(nodes_df) = NULL

#create a node graph from this df
#uses graph.data.frame from the igraph package
node_graph &amp;lt;- graph.data.frame(nodes_df, directed=FALSE)

#to get the shortest distance between every pair of pubs
#need to create each combination of pub id number
combinations &amp;lt;- combn(1:nrow(pubs), 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The shortest path between any two pubs is found using igraph::get.shortest.paths() and then extracting the path of nodes. Each vertex of the path is then found by using pairwise combinations of the nodes, and the travelled vertices for each pub-&amp;gt;pub journey are saved into a (large) df.&lt;/p&gt;
&lt;p&gt;The whole thing is pretty quick but obviously the number of combinations grows quickly. For 103 pubs in Cambridge, it takes ~20mins on my machine.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#go through each of these pairs
journeys &amp;lt;- lapply(seq(length(combinations)/2), function(combination) {
  #the two pubs we&amp;#39;ll test
  pub1 &amp;lt;- combinations[1,][combination]
  pub2 &amp;lt;- combinations[2,][combination]
  
  #get the shortest path (node-node) between these two pubs
  travel_nodes &amp;lt;- get.shortest.paths(node_graph,
                                     from = as.character(pubs$id[pub1]),
                                     to = as.character(pubs$id[pub2]),
                                     weights = E(node_graph)$length) %&amp;gt;%
    .$vpath %&amp;gt;%
    unlist() %&amp;gt;%
    names() %&amp;gt;%
    as.numeric()
  
  #find the vertices that connect between these nodes
  connecting_vertices &amp;lt;- lapply(seq(length(travel_nodes)-1), function(node_pair) {
    between_nodes &amp;lt;- travel_nodes[c(node_pair:(node_pair+1))]
    connecting_vertex &amp;lt;- which(roads$start_id %in% between_nodes &amp;amp;
                                 roads$end_id %in% between_nodes)
    #if more than one potential vertex, take the shorter one
    if(length(connecting_vertex) &amp;gt; 1) {
      connecting_vertex &amp;lt;- connecting_vertex[
        which.min(st_length(roads[connecting_vertex,]))
      ]
    }
    
  return(connecting_vertex)
  }) %&amp;gt;%
    unlist() %&amp;gt;%
    roads[.,] %&amp;gt;%
    #id this journey between pubs
    mutate(journey = paste0(pub1, &amp;quot;-&amp;quot;, pub2))
  
  return(connecting_vertices)
}) %&amp;gt;%
  #rbind it all together
  do.call(rbind, .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s possible to check a journey between two pubs easily using this df, and show that it does seem like igraph is finding the shortest route between the two&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get only the journey between two random pubs
set.seed(3459)
x_random_numbers &amp;lt;- sample(nrow(pubs), 2) %&amp;gt;%
  sort()
x_journey &amp;lt;- journeys %&amp;gt;%
  filter(journey == paste0(x_random_numbers[1], &amp;quot;-&amp;quot;, x_random_numbers[2]))

#get the nodes of this journey
x_journey_nodes &amp;lt;- nodes %&amp;gt;%
  filter(id %in% c(as.character(x_journey$start_id),
                   as.character(x_journey$end_id)))

#find the pubs at the start and end of this journey
#random pubs as defined earlier
x_journey_pubs &amp;lt;- pubs[x_random_numbers,]

#get all roads within a kilometer of each pub
x_local_roads &amp;lt;- roads %&amp;gt;%
  .[unlist(st_intersects(
    #select all roads that at least will be between the two pubs
    st_buffer(x_journey_pubs, max(st_distance(x_journey_pubs)/1.5)
  ), .)),]

#plot the shortest path between these two pubs
(p4 &amp;lt;- ggplot() +
    geom_sf(data = x_local_roads, colour = &amp;quot;grey&amp;quot;) +
    geom_sf(data = x_journey_nodes, colour = &amp;quot;black&amp;quot;) +
    geom_sf(data = x_journey, colour = &amp;quot;black&amp;quot;) +
    geom_sf(data = x_journey_pubs, colour = &amp;quot;blue&amp;quot;, alpha = 0.5, size = 3) +
    theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/check_shortest_paths-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#remove all the extra objects we created in the example
rm(list=ls()[grep(&amp;quot;x_&amp;quot;, ls())])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I have to find the shortest combination of these journeys which visits every pub at least once. For this I find the shortest distances between each pub and every other pub and bind it into a matrix.&lt;/p&gt;
&lt;p&gt;If you just wanted to find the distance of a pub crawl (and not the path) you could just skip to here and save a lot of time.&lt;/p&gt;
&lt;p&gt;This matrix of 103x103 is then converted into TSP object using TSP::TSP() and converted to a Hamtiltonian path problem by inserting a dummy variable. The TSP is then solved given the order of nodes (in this case, just pubs) to visit.&lt;/p&gt;
&lt;p&gt;A lot of help in doing this came from the StackOverflow answer &lt;a href=&#34;https://stackoverflow.com/questions/27363653/find-shortest-path-from-x-y-coordinates-with-start-%E2%89%A0-end&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get the distances 
distances &amp;lt;- lapply(seq(nrow(pubs)), function(node) {
  distance &amp;lt;- shortest.paths(node_graph, 
                             v = as.character(pubs$id[node]),
                             to = as.character(pubs$id[seq(nrow(pubs))]), 
                             weights = E(node_graph)$length)
}) %&amp;gt;%
  do.call(rbind, .) %&amp;gt;%
  as.dist() %&amp;gt;%
  TSP()

#insert a dummy city to turn this into a Hamiltonian path question
#i.e. we do not need to return to the start at the end
tsp &amp;lt;- insert_dummy(distances, label = &amp;quot;cut&amp;quot;)

#solve the shortest Hamiltonian tour of each pub in Cambridge
#get the total distance ()
tour &amp;lt;- solve_TSP(tsp, method=&amp;quot;2-opt&amp;quot;, control=list(rep=10))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: executing %dopar% sequentially: no parallel backend registered&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tour&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## object of class &amp;#39;TOUR&amp;#39; 
## result of method &amp;#39;2-opt_rep_10&amp;#39; for 104 cities
## tour length: 41289.44&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the order of pubs to visit in an optimal Hamtilonian path
tour &amp;lt;-  unname(cut_tour(tour, &amp;quot;cut&amp;quot;))
tour&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1]  38  76  70  14   8  49  86  32  75  63  10   9   2  41   1  27  40
##  [18]  54  20  82  90  80  72   4  61  98  83  77   5  89  47   6  60  88
##  [35]  59  46  57  11 101  78  84   7  36  65  44  71  21  39 103  18 102
##  [52]  93  19  85  62  23  16  22  29 100  42  51  50  48  64  56  79  92
##  [69]  30  28  87  96  55  35  67  69  68  91  13  17  52  94  12  97  58
##  [86]  26  73  66  25  24  31  81  37  33  53  15  74  45  43  99  34   3
## [103]  95&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So to visit every pub, you’d expect to walk just under 41km, which fits with eye-testing the size of Cambridge (approx 10km diameter).&lt;/p&gt;
&lt;p&gt;In order to plot this, the pub order is converted into strings in the format we’ve used for journeys between pubs (e.g. “1-2”) and each journey is then extracted from the df of shortest journeys between all pubs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#it might be possible that doing some journeys in &amp;#39;reverse&amp;#39; is faster
#when considering the tour of all pubs
#e.g. from pub2 -&amp;gt; pub1 rather than the other way round
rev_journeys &amp;lt;- journeys
rev_journeys$journey &amp;lt;- strsplit(journeys$journey, &amp;quot;-&amp;quot;) %&amp;gt;%
  #reverse the journey tag
  lapply(., rev) %&amp;gt;%
  lapply(., paste, collapse = &amp;quot;-&amp;quot;)

#bind these together in a df of all journeys
journeys &amp;lt;- rbind(journeys, rev_journeys) %&amp;gt;%
  mutate(journey = as.character(journey))

#take the nodes from the shortest tour and arrange them as in
#the journeys tag for each path between two pubs
tour_strings &amp;lt;- paste0(tour, &amp;quot;-&amp;quot;, lead(tour)) %&amp;gt;%
  .[-grep(&amp;quot;NA&amp;quot;, .)] %&amp;gt;%
  data.frame(journey = .,
             journey_order = 1:length(.))

#use this to find each vertex that needs traversing in order to complete
#the shortest tour of all pubs
#subset this from the df of all shortest journeys between any two pubs
shortest_tour &amp;lt;- journeys[which(journeys$journey %in% tour_strings$journey),] %&amp;gt;%
  merge(., tour_strings,  by = &amp;quot;journey&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All that’s left to do is plot this shortest pub crawl&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(p5 &amp;lt;- ggplot(data = shortest_tour) +
  geom_sf(data = cambridge) +
  geom_sf(data = roads, size = 0.5) + 
  geom_sf(aes(colour = journey_order), size = 1.5, alpha = 0.7) +
  scale_colour_gradient(high = &amp;quot;darkred&amp;quot;, low = &amp;quot;orange&amp;quot;, name = &amp;quot;journey order&amp;quot;) +
  geom_sf(data = pubs, colour = &amp;quot;blue&amp;quot;, size = 1.5) +
  theme_minimal() +
  ggtitle(&amp;quot;Hamiltonian Path of Every Pub in the City of Cambridge&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/plot_shortest_tour-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As it turns out, you’d want to start in Trumpington at The Lord Byron Inn and then get into central Cambridge where you dart around a lot before heading out to the north and finally the east at The Red Lion in Cherry Hinton.&lt;/p&gt;
&lt;p&gt;A bigger image of the above can be found at imgure &lt;a href=&#34;https://i.imgur.com/r18SG2B.png&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All together this script calculates a distances of ~41km (give or take probably quite a bit) to visit every pub, which is actually kind of doable in a single day (if you forgo [at least some of] the drinking).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How I Learned To Stop Worrying and Love Heatmaps</title>
      <link>/post/getis-ord-heatmaps-tutorial/</link>
      <pubDate>Thu, 04 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/getis-ord-heatmaps-tutorial/</guid>
      <description>


&lt;p&gt;Whilst &lt;del&gt;getting some work done&lt;/del&gt; browsing twitter at work today, I came across &lt;a href=&#34;https://twitter.com/jburnmurdoch/status/1047470445459644416&#34;&gt;this tweet&lt;/a&gt; from the always excellent John Burn-Murdoch on the scourge of heatmaps. What’s most frustrating about these maps is that ggplot2 (which is underrated as mapping software, especially when combined with packages like sf in R) makes it super easy to create this bland, uninformative maps.&lt;/p&gt;
&lt;p&gt;For instance, lets load some mapping libraries&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(sf)
library(rgdal)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this blog I’m going to use data of bus stops in London, because there’s an absolute ton of them and because I love the London Datastore and it was the first public, heavy, point data file I came across.&lt;/p&gt;
&lt;p&gt;Let’s grab some data to use as exemplars&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get the shapefile data of london from GADM
#downloads into a temp file
gadm_url &amp;lt;- &amp;quot;https://biogeo.ucdavis.edu/data/gadm3.6/Rsf/gadm36_GBR_2_sf.rds&amp;quot;
temp_dir &amp;lt;- tempdir()
download.file(gadm_url, destfile = file.path(temp_dir, &amp;quot;london_shapefile.rds&amp;quot;), 
              mode = &amp;quot;wb&amp;quot;, quiet = TRUE)
london &amp;lt;- sf::st_as_sf(readRDS(file.path(temp_dir, &amp;quot;london_shapefile.rds&amp;quot;))) %&amp;gt;%
  filter(grepl(&amp;quot;London&amp;quot;, NAME_2))

#get the bus stop data
#https://data.london.gov.uk/dataset/tfl-bus-stop-locations-and-routes
bus &amp;lt;- read.csv(&amp;quot;https://files.datapress.com/london/dataset/tfl-bus-stop-locations-and-routes/bus-stops-10-06-15.csv&amp;quot;,
                stringsAsFactors = FALSE) %&amp;gt;%
  filter(!is.na(Location_Easting)) %&amp;gt;%
  #convert to simple features
  st_as_sf(coords = c(&amp;quot;Location_Easting&amp;quot;, &amp;quot;Location_Northing&amp;quot;), crs = st_crs(&amp;quot;+init=epsg:27700&amp;quot;)) %&amp;gt;%
  #transform projection to match the boundary data
  st_transform(crs = st_crs(london)) %&amp;gt;%
  #remove bus stops outside of the limits of the london shapefile
  .[unlist(st_intersects(london, .)),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the data, let’s have a first pass at plotting the points on a map of London using ggplot2&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot the bus stop locations
p1 &amp;lt;- ggplot(bus) +
  geom_sf(data = london, fill = &amp;quot;grey&amp;quot;) +
  geom_sf(colour = &amp;quot;blue&amp;quot;, alpha = 0.2) +
  ggtitle(&amp;quot;Bus Stops in London&amp;quot;) +
  theme_void()

plot(p1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-04-getis-ord-tutorial_files/figure-html/plot_bus_stops-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The points here do kiiiind of work alone- it’s possible to see the route along which buses travel. However, it’s still pretty heavy and takes some cognitive effort. It’s also worth remembering this is just the first example of data I came across- if working with stuff like common incident data, even a heavy alpha on the points is going to cover the map and leave it unreadable, as in the tweet up top.&lt;/p&gt;
&lt;p&gt;But let’s say we plot it as a heatmap to try and sumamrise where the points are. In my opinion, this is even worse. For one- it stretches utside the boundaries of the shapes, even though we have filtered data to include no bus stops outside London. In this example that’s not a huge deal- but if London was (e.g.) an Island, the graph is now suggesting commutes into the sea. It’s also incredibly taxing to keep track of the various peaks and troughs- even in a simple model where bus stops generally increase as you go near to the centre&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#bind the coordinates as numeric
bus &amp;lt;- bus %&amp;gt;%
  cbind(., st_coordinates(bus))

#plot the bus stops as a density map
p2 &amp;lt;- ggplot() +
  geom_sf(data = london) +
  stat_density_2d(data = bus, aes(X, Y)) +
  ggtitle(&amp;quot;Bus Stops in London&amp;quot;) +
  theme_void()

plot(p2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-04-getis-ord-tutorial_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Instead, we can bin the point data into equi-sized containers. I’m extremely partial to this, even though it’s not super popular. To do this with hexagonal bins (the closest to circular that still tessellates perfectly), we just have to create a grid of points and connect them up&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#merge the london wards into one boundary file
london_union &amp;lt;- london %&amp;gt;%
  group_by(&amp;quot;group&amp;quot;) %&amp;gt;%
  summarise()

#generate a grid of points separated hexagonally
#no way to do this purely in sf yet afaik
hex_points &amp;lt;- spsample(as_Spatial(london_union), type = &amp;quot;hexagonal&amp;quot;, cellsize = 0.01)

#generate hexgaon polygons from these points
hex_polygons &amp;lt;- HexPoints2SpatialPolygons(hex_points) %&amp;gt;%
  st_as_sf(crs = st_crs(london_union)) %&amp;gt;%
  #clip to the london shapefile
  st_intersection(., london_union)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then find out which bin every bus stop is located in using st_intersects&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find how many bus stops are within each hexagon
hex_polygons$planning_no &amp;lt;- lengths(st_intersects(hex_polygons, bus))

#plot the number of bus stops per bin
p2 &amp;lt;- ggplot(hex_polygons) +
  geom_sf(aes(fill = planning_no)) +
  scale_fill_viridis_c(option = &amp;quot;magma&amp;quot;, &amp;quot;# Bus Stops&amp;quot;) +
  theme_void() +
  ggtitle(&amp;quot;Binned London Bus Stops&amp;quot;)

plot(p2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-04-getis-ord-tutorial_files/figure-html/bin_stops-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, this is still pretty confusing. It seems like bus stops are fairly randomly distributed as by chance one hexagon may contain multiple stops at its edges, whereas a neighbour may be juuuust missing out on these.&lt;/p&gt;
&lt;p&gt;To mitigate this effect, we can study the spatial autocorrelation of each hexagon to it’s neighbours. There are multiple ways to do this, but the one I was first introduced to and have used most is the Getis-Ord local statistic. In this example I will include.self() which means we are using the Gi* variant of the statistic.&lt;/p&gt;
&lt;p&gt;Basically- we tell R to find all the nearest neighbours of any bin (hexagon- though not necessarily so, we could e.g. use wards, but I think it looks messier). It then calculates the ratio of values within the bin and it’s neighbours, to the total number of points (bus stops). The reported Gi* value is a z statistic- it can be positive (more clustering) or negative (less) and used to find significant clusters. I’m not going to do any of that here- just accept for now that a high Getis_Ord Gi* value means a greater cluster of bus stops in that region.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(spdep)

#find the centroid of each hexagon and convert to a matrix of points
hex_points &amp;lt;- do.call(rbind, st_geometry(st_centroid(hex_polygons))) %&amp;gt;%
  unlist() %&amp;gt;%
  as.matrix.data.frame()

#use a k-nearest-neighbour algorithm to find which shape neighbour which
#super easy for the hexagons analytically obvs but important for e.g. using the ward boundaries instead
neighbouring_hexes &amp;lt;- knn2nb(knearneigh(hex_points, k = 6), 
                             row.names = rownames(hex_points)) %&amp;gt;%
  include.self()

#calculate the local G for a given variable (#bus stops) using the neihbours found previously
localGvalues &amp;lt;- localG(x = as.numeric(hex_polygons$planning_no),
                       listw = nb2listw(neighbouring_hexes, style = &amp;quot;B&amp;quot;),
                       zero.policy = TRUE)

#bind this back to the sf as a numeric variable column
hex_polygons$smooth_planning_no &amp;lt;- as.numeric(localGvalues)

#plot the statistic
#+ve indicates more than would be expected
p3 &amp;lt;- ggplot(hex_polygons) +
  geom_sf(aes(fill = smooth_planning_no)) +
  scale_fill_viridis_c(option = &amp;quot;magma&amp;quot;, name = &amp;quot;Gi* Statistic&amp;quot;) +
  theme_void() +
  ggtitle(&amp;quot;Getis-Ord Binned London Bus Stops Statistic&amp;quot;)

plot(p3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-04-getis-ord-tutorial_files/figure-html/getis_ord-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;which generates a nice plot showing smoothed autocorrelation of dense public-transportation access. I like how you can clearly see the darker regions for the Lea Valley and Richmond Park, and in contrast the hubs of Kingston and Croydon, but in a way to is much more manageable than the contour map, or the point data itself.&lt;/p&gt;
&lt;p&gt;It’s also worth bearing in mind, that this data is fairly organised (bus routes are to some extent, logically planned). When I first looked into spatial autocorrelation I was dealing with a huge number of dense points randomly dispersed over a significant proportion of England. At that level, techniques such as the finding the Getis-Ord statistic allow you to make sense of the data, AS WELL AS statistically test it. Though I haven’t ever worked with epidemiology data, apparently it’s a powerful technique to find clusters of disease outbreaks, and indeed, the data packaged with spdep is for SIDS data in North Carolina.&lt;/p&gt;
&lt;p&gt;For more on this, I first learned about Getis-Ord from an excellent &lt;a href=&#34;https://pudding.cool/process/regional_smoothing/&#34;&gt;The Pudding&lt;/a&gt; post (from which some of my code is stolen), &lt;a href=&#34;http://pro.arcgis.com/en/pro-app/tool-reference/spatial-statistics/h-how-hot-spot-analysis-getis-ord-gi-spatial-stati.htm&#34;&gt;ARCGIS has&lt;/a&gt; a pretty good write up, and of course there is the CRAN page for the &lt;a href=&#34;https://cran.r-project.org/web/packages/spdep/index.html&#34;&gt;spdep library&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Best,&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>sf.chlorodot mini-package</title>
      <link>/post/sf.schlorodot/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/sf.schlorodot/</guid>
      <description>


&lt;p&gt;Recently, I’d seen two tweets with stunning examples of maps by Paul Campbell &lt;a href=&#34;https://twitter.com/PaulCampbell91/status/992043182996193280&#34;&gt;here&lt;/a&gt; and (taken inspiration from the first) by Imer Muhović &lt;a href=&#34;https://twitter.com/ImerM1/status/1037358973807210498&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The basic idea of the dot chloropleths is to visualise not only the location clustering of each variable but the number of observations (something traditional ‘filled’ chloropleths don’t do). More importantly than this, the maps also just look really really cool.&lt;/p&gt;
&lt;p&gt;I had a spare few minutes during work on Friday which I tidied up into a package to calculate the random position of dots for such maps which can be found &lt;a href=&#34;https://github.com/RobWHickman/sf.chlorodot&#34;&gt;on my github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Below, I’ll outline the code for the South African example used in the package README. Data comes from Adrian Frith’s &lt;a href=&#34;https://census2011.adrianfrith.com/&#34;&gt;very good 2011 census site&lt;/a&gt; and &lt;a href=&#34;https://gadm.org/download_country_v3.html&#34;&gt;gadm&lt;/a&gt; for the shapefiles.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sf)
library(ggplot2)
library(tidyverse)
library(data.table)
library(rvest)

devtools::install_github(&amp;#39;RobWHickman/sf.chlorodot&amp;#39;)
library(sf.chlordot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, download and scrape the data for the map&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#download the South African shapefile fom gadm
admin_url &amp;lt;- &amp;quot;https://biogeo.ucdavis.edu/data/gadm3.6/Rsf/gadm36_ZAF_3_sf.rds&amp;quot;
download.file(admin_url, destfile = &amp;quot;shapefiles.rds&amp;quot;, mode = &amp;quot;wb&amp;quot;)
south_africa &amp;lt;- readRDS(&amp;quot;shapefiles.rds&amp;quot;) %&amp;gt;%
  #convert to sf
  st_as_sf() %&amp;gt;%
  select(region = NAME_3) %&amp;gt;%
  #merge geometries that have two rows
  group_by(region) %&amp;gt;%
  summarise()

#get the links to the data from Adrian Frith&amp;#39;s site
sa_data_url &amp;lt;- &amp;quot;https://census2011.adrianfrith.com&amp;quot;
south_africa_data &amp;lt;- sa_data_url %&amp;gt;%
  read_html() %&amp;gt;% html_nodes(&amp;quot;.namecell a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% paste0(sa_data_url, .) %&amp;gt;%
  lapply(., function(x) read_html(x) %&amp;gt;% html_nodes(&amp;quot;.namecell a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% paste0(sa_data_url, .)) %&amp;gt;% unlist() %&amp;gt;%
   lapply(., function(x) read_html(x) %&amp;gt;% html_nodes(&amp;quot;.namecell a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% paste0(sa_data_url, .)) %&amp;gt;% unlist()

#scrape the data on primary language from the 2011 South African census
language_data &amp;lt;- rbindlist(lapply(south_africa_data, function(x) {
  read &amp;lt;- read_html(x)
  language_nos &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;.datacell&amp;quot;) %&amp;gt;% html_text()
  start &amp;lt;- grep(&amp;quot;Percentage&amp;quot;, language_nos)[3] + 1
  stop &amp;lt;- grep(&amp;quot;Population&amp;quot;, language_nos) - 1
  #some areas have no data
  if(!is.na(start) &amp;amp; !is.na(stop)) {
    language_nos &amp;lt;- language_nos[start:stop]
    language_nos &amp;lt;- language_nos[seq(1, length(language_nos), 2)]
  } else {
    language_nos &amp;lt;- NA
  }
  
  languages &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;tr &amp;gt; :nth-child(1)&amp;quot;) %&amp;gt;% html_text()
  start &amp;lt;- grep(&amp;quot;First language&amp;quot;, languages) + 1
  stop &amp;lt;- grep(&amp;quot;Name&amp;quot;, languages) - 1
  if(length(start) &amp;gt; 0 &amp;amp; !is.na(stop)) {
    languages &amp;lt;- languages[start:stop]
  } else {
    languages &amp;lt;- NA
  }
  
  region_names &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;.topname&amp;quot;) %&amp;gt;% html_text()
  
  #combine into a df
  df &amp;lt;- data.frame(language = languages, primary_speakers = language_nos, region = region_names)
  return(df)
}))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the lanaguage data then needs to be transformed before the dot position is calculated. It must be in ‘short’ format with variables as column names. At the same time we can do some cleaning in order to match the shape areas with the region names from the census and remove data we don’t want to plot&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;language_data %&amp;lt;&amp;gt;%
  #convert number of speakers to numeric
  mutate(primary_speakers = as.numeric(as.character(primary_speakers))) %&amp;gt;%
  #matching of area names with South African shapefile
  mutate(region = gsub(&amp;quot; NU&amp;quot;, &amp;quot;&amp;quot;, region)) %&amp;gt;%
  mutate(region = gsub(&amp;quot;Tshwane&amp;quot;, &amp;quot;City of Tshwane&amp;quot;, region)) %&amp;gt;%
  #filter only the data we want to merge
  filter(region %in% south_africa$region) %&amp;gt;%
  filter(!is.na(language)) %&amp;gt;%
  filter(language != &amp;quot;Not applicable&amp;quot;) %&amp;gt;%
  #spread the data
  dcast(., region ~ language, value.var = &amp;quot;primary_speakers&amp;quot;, fun.aggregate = sum) %&amp;gt;%
  #join in the spatial geometry
  left_join(., south_africa) %&amp;gt;%
  #convert to sf
  st_as_sf()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then we can calculate the random dot position using calc_dots() from the sf.chlorodot package. This takes three arguments. The first is the df to take the data from (language_data). The second is which variables to calculate positions for. The easiest way to do this is to use names(df) and select from there, though a character vector can also be passed. Finally, n_per_dot is the number of observations (speakers of language x) for each dot on the map. This will affect the look of the map, but also the processing time (lower n_per_dot = greater time) so play around with it a bit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#calculate the dot positions using calc_dots from the sf.chlorodot package
sf_dots &amp;lt;- calc_dots(df = language_data, col_names = names(language_data)[2:14], n_per_dot = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can plot the output of this&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#stolen the background colour scheme from Paul Campbell&amp;#39;s blog
#original inspiration for this package
p &amp;lt;- ggplot() +
  geom_sf(data = south_africa, fill = &amp;quot;transparent&amp;quot;,colour = &amp;quot;white&amp;quot;) +
  geom_point(data = sf_dots, aes(lon, lat, colour = variable), size = 0.1) +
  scale_colour_discrete(name = &amp;quot;Primary Language&amp;quot;) +
  ggtitle(&amp;quot;Language Diversity in South Africa&amp;quot;) +
  theme_void() +
  guides(colour = guide_legend(override.aes = list(size = 10))) +
  theme(plot.background = element_rect(fill = &amp;quot;#212121&amp;quot;, color = NA), 
        panel.background = element_rect(fill = &amp;quot;#212121&amp;quot;, color = NA),
        legend.background = element_rect(fill = &amp;quot;#212121&amp;quot;, color = NA),
        text =  element_text(color = &amp;quot;white&amp;quot;),
        title =  element_text(color = &amp;quot;white&amp;quot;),
        legend.text=element_text(size=12),
        plot.title = element_text(size = 20),
        plot.margin = margin(1, 1, 1, 1, &amp;quot;cm&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../img/south_africa.png&#34; alt=&#34;chlorodot map of South African languages&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Enjoy!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Knowledge 4th August 2018</title>
      <link>/post/the-knowledge-4th-august-2018/</link>
      <pubDate>Sat, 04 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/the-knowledge-4th-august-2018/</guid>
      <description>


&lt;p&gt;The Guardian publish a weekly set of questions and answers on a variety of football minutiae at &lt;a href=&#34;https://www.theguardian.com/football/series/theknowledge&#34;&gt;The Knowledge&lt;/a&gt;. Forutnately, some of these are extremely tractable using R, so I thought I’d have a go at working through the archives to see if I can shed light on any of the questions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
library(dplyr)
library(magrittr)
library(data.table)
library(zoo)
library(ggplot2)
library(rvest)
library(stringr)

#jalapic/engsoccerdata
library(engsoccerdata)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;we-aint-going-to-the-town..&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;We Ain’t Going To The Town..&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/topes_lose/status/1023537060668473344?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1023537060668473344&amp;amp;ref_url=https%3A%2F%2Fwww.theguardian.com%2Ffootball%2F2018%2Faug%2F01%2Ffootballers-who-have-backed-out-of-a-transfer-for-another-late-in-the-day&#34;&gt;‘This season, Tranmere Rovers return to contest League Two alongside eight teams with the suffix Town, including six successive fixtures against these clubs over the New Year. What is the record for successive fixtures versus clubs with the same (or no) prefix or suffix?’&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For this question I decided to ignore prefixes as the dataset I’m using doesn’t have any that could be matches between teams except the ‘West’ in West Ham and West Bromwich Albion. That dataset is the excellent engsoccerdata from James Curley found at his github &lt;a href=&#34;https://github.com/jalapic/engsoccerdata&#34;&gt;here&lt;/a&gt; and on CRAN.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#take all of the english soccer data in the package and bind it together
england_data &amp;lt;- bind_rows(
    select(engsoccerdata::england,
           .data$home, .data$visitor, date = .data$Date),
    select(engsoccerdata::englandplayoffs,
           .data$home, .data$visitor, date = .data$Date),
    select(engsoccerdata::england1939,
           .data$home, .data$visitor, date = .data$Date)) %&amp;gt;%
  setDT() %&amp;gt;%
  #convert the date to date class
  .[, date := as.Date(date)]

#get a list of each unique team in the dataset
all_teams &amp;lt;- unique(c(as.character(england_data$home),
                      as.character(england_data$visitor)))

#melt the dataset by each teams matches
find_chains &amp;lt;- rbindlist(lapply(all_teams, function(team) {
  england_data %&amp;gt;%
    .[home == team | visitor == team] %&amp;gt;%
    .[, matching_team := team]
  })) %&amp;gt;%
  .[home == matching_team, other := visitor] %&amp;gt;%
  .[visitor == matching_team, other := home] %&amp;gt;%
  .[, c(&amp;quot;date&amp;quot;, &amp;quot;matching_team&amp;quot;, &amp;quot;other&amp;quot;)] %&amp;gt;%
  #get the suffixes and prefixes of the other team
  .[, other_prefix := gsub(&amp;quot; .*&amp;quot;, &amp;quot;&amp;quot;, other)] %&amp;gt;%
  .[, other_suffix := gsub(&amp;quot;.* &amp;quot;, &amp;quot;&amp;quot;, other)] %&amp;gt;%
  #arrange by team and date
  .[order(matching_team, date)] %&amp;gt;%
  #convert to an id
  .[, suffix_id := as.numeric(as.factor(other_suffix))] %&amp;gt;%
  #if playing consecutively against the same suffix id (ignoring prefixes for now) put in same &amp;#39;chain&amp;#39;
  .[, match := suffix_id - lead(suffix_id), by = &amp;quot;matching_team&amp;quot;] %&amp;gt;%
  .[match == 0 &amp;amp; lead(match) != 0, chain_id := 1:.N] %&amp;gt;%
  .[match == 0] %&amp;gt;%
  .[, chain_id := na.locf(chain_id, fromLast = TRUE)] %&amp;gt;%
  .[, chain_length := .N, by = chain_id] %&amp;gt;%
  #take only chains at least as long as Tranmere&amp;#39;s run (6)
  .[chain_length &amp;gt; 5] %&amp;gt;%
  .[order(chain_length)] %&amp;gt;%
  .[, c(&amp;quot;date&amp;quot;, &amp;quot;matching_team&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;chain_length&amp;quot;)]

#print the chains of equal length to Tranmere&amp;#39;s run
print(find_chains)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           date  matching_team               other chain_length
##  1: 1950-12-30   Chesterfield      Leicester City            6
##  2: 1951-01-13   Chesterfield     Manchester City            6
##  3: 1951-01-20   Chesterfield       Coventry City            6
##  4: 1951-02-03   Chesterfield        Cardiff City            6
##  5: 1951-02-17   Chesterfield     Birmingham City            6
##  6: 1951-02-24   Chesterfield        Swansea City            6
##  7: 2009-03-21 Leicester City   Colchester United            6
##  8: 2009-03-28 Leicester City Peterborough United            6
##  9: 2009-04-04 Leicester City     Carlisle United            6
## 10: 2009-04-11 Leicester City     Hereford United            6
## 11: 2009-04-13 Leicester City        Leeds United            6
## 12: 2009-04-18 Leicester City     Southend United            6
## 13: 1921-05-02         Fulham           Hull City            7
## 14: 1921-05-07         Fulham           Hull City            7
## 15: 1921-08-27         Fulham       Coventry City            7
## 16: 1921-08-29         Fulham      Leicester City            7
## 17: 1921-09-03         Fulham       Coventry City            7
## 18: 1921-09-05         Fulham      Leicester City            7
## 19: 1921-09-10         Fulham           Hull City            7
## 20: 1920-04-17  Leyton Orient     Birmingham City            7
## 21: 1920-04-24  Leyton Orient     Birmingham City            7
## 22: 1920-04-26  Leyton Orient      Leicester City            7
## 23: 1920-05-01  Leyton Orient      Leicester City            7
## 24: 1920-08-28  Leyton Orient      Leicester City            7
## 25: 1920-08-30  Leyton Orient        Cardiff City            7
## 26: 1920-09-04  Leyton Orient      Leicester City            7
## 27: 1920-10-09   Notts County          Stoke City            7
## 28: 1920-10-16   Notts County          Stoke City            7
## 29: 1920-10-23   Notts County        Cardiff City            7
## 30: 1920-10-30   Notts County        Cardiff City            7
## 31: 1920-11-06   Notts County       Coventry City            7
## 32: 1920-11-13   Notts County       Coventry City            7
## 33: 1920-11-20   Notts County      Leicester City            7
##           date  matching_team               other chain_length&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;so In fact an identical length chain on matching suffixes has occured twice, with Chesterfield playing a range of cities at the start of 1951 in League Two, and much more recently, Leicester playing 6 different Uniteds in a row at the tail end of the 2008/2009 season. This is also the season that saw them recover from being relegated from the Chmapionship and start moving towards winning the title in 2015-2016 season.&lt;/p&gt;
&lt;p&gt;Some longer chains involving cities happened in the 1920-1921 seasons in the Second Division, but it seems like the scheduling worked differently then and teams played back to back more, so doesn’t really count.&lt;/p&gt;
&lt;p&gt;Having originally misread the question, I also wanted to find out the longest chain of a team playing teams that matched &lt;em&gt;their own&lt;/em&gt; suffix. We can do this using a similar method&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matching_fixtures &amp;lt;- england_data %&amp;gt;%
  #get only matches between teams with matching prefix/suffixes
  .[, home_suffix := gsub(&amp;quot;.* &amp;quot;, &amp;quot;&amp;quot;, home)] %&amp;gt;%
  .[, away_suffix := gsub(&amp;quot;.* &amp;quot;, &amp;quot;&amp;quot;, visitor)] %&amp;gt;%
  .[home_suffix == away_suffix, match := home_suffix] %&amp;gt;%
  .[!is.na(match)] %&amp;gt;%
  #remove matches where teams from the same city play each other
  .[!match %in% c(&amp;quot;Bradford&amp;quot;, &amp;quot;Bristol&amp;quot;, &amp;quot;Burton&amp;quot;, &amp;quot;Manchester&amp;quot;, &amp;quot;Sheffield&amp;quot;)]

#get all the teams that have played teams with matching suffixes
matching_teams &amp;lt;- unique(c(as.character(matching_fixtures$home),
                           as.character(matching_fixtures$visitor)))

#elongate the data and look for chains
find_chains &amp;lt;- rbindlist(lapply(matching_teams, function(team) {
  england_data %&amp;gt;%
    .[home == team | visitor == team] %&amp;gt;%
    .[order(date)] %&amp;gt;%
    .[, matching_team := team]
  })) %&amp;gt;%
  .[home == matching_team, other := visitor] %&amp;gt;%
  .[visitor == matching_team, other := home] %&amp;gt;%
  #id matches and remove matches not involving teams with identical suffixes
  .[, match_id := 1:.N, by = matching_team] %&amp;gt;%
  .[!is.na(match)] %&amp;gt;%
  #find chains of identical suffixed matches
  .[, chain := match_id - lag(match_id)] %&amp;gt;%
  .[chain == 1 &amp;amp; lag(chain) != 1, chain_id := 1:.N] %&amp;gt;%
  .[chain == 1] %&amp;gt;%
  .[, chain_id := na.locf(chain_id)] %&amp;gt;%
    .[, chain_length := .N, by = chain_id] %&amp;gt;%
  #take only chains at least as long as Tranmere&amp;#39;s run (6)
  .[chain_length &amp;gt; 4] %&amp;gt;%
  .[order(chain_length)] %&amp;gt;%
  .[, c(&amp;quot;date&amp;quot;, &amp;quot;matching_team&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;chain_length&amp;quot;)]

print(find_chains)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           date   matching_team             other chain_length
##  1: 1919-12-13      Stoke City   Birmingham City            5
##  2: 1919-12-20      Stoke City    Leicester City            5
##  3: 1919-12-25      Stoke City     Coventry City            5
##  4: 1919-12-26      Stoke City     Coventry City            5
##  5: 1919-12-27      Stoke City    Leicester City            5
##  6: 1919-09-01       Hull City        Stoke City            5
##  7: 1919-09-06       Hull City   Birmingham City            5
##  8: 1919-09-08       Hull City        Stoke City            5
##  9: 1919-09-13       Hull City        Leeds City            5
## 10: 1919-09-20       Hull City        Leeds City            5
## 11: 1988-09-24 Carlisle United  Rotherham United            5
## 12: 1988-09-30 Carlisle United  Cambridge United            5
## 13: 1988-10-04 Carlisle United Colchester United            5
## 14: 1988-10-08 Carlisle United   Hereford United            5
## 15: 1988-10-15 Carlisle United    Torquay United            5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the record for that is only slightly shorter! with Stoke and Hull City playing a range of cities in the 1919-1920 season (but see above for scheduling differences) and Carlisle United playing 5 other different Uniteds in a row in the old Fourth Division.&lt;/p&gt;
&lt;div id=&#34;answer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Answer&lt;/h2&gt;
&lt;p&gt;The record is 7 matches set by Notts County, Leyton Orient, and Fulham in 1920/1921 playing 7 teams with the suffix ‘city’ in a row. The Leyton Orient and Fulham chains stretch over the end of one season and into the next, so only Notts County really satisifies the question. However, the scheduling in these years involved a lot of back to back matches and so is cheating a bit.&lt;/p&gt;
&lt;p&gt;More recently Chesterfield played 6 different teams with the suffix ‘city’ in a row in 1950/1951, and Leceister played 6 different ’united’s in a row in their promotion season from League One in 2008/2009.&lt;/p&gt;
&lt;p&gt;Even more bizarre, Carlisle United played 5 other different United’s at the start of the 1988/1989 old Fourth Division season.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;youth-of-the-nation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Youth Of The Nation&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.theguardian.com/football/2018/aug/01/footballers-who-have-backed-out-of-a-transfer-for-another-late-in-the-day&#34;&gt;“If Lucas Hernández was born a year and a half later, his age would be a lower than his shirt number (21). Have any World Cup winners achieved this?” muses Edward Gibson.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The easiest way to check this is just to scrape all of the squads off of the wiki pages for the World Cups. I only did from 1954 onwards as before this the squad no and birthdate data is a bit patchy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#links to the world cup squads pages
wiki_cup_squads &amp;lt;- sprintf(&amp;quot;https://en.wikipedia.org/wiki/%s_FIFA_World_Cup_squads&amp;quot;,
                           seq(1954, 2018, by = 4))

#scrape all the player data we need
world_cup_squads &amp;lt;- rbindlist(lapply(wiki_cup_squads[1:17], function(link) {
  year &amp;lt;- gsub(&amp;quot;.*\\/wiki\\/&amp;quot;, &amp;quot;&amp;quot;, gsub(&amp;quot;_FIFA_World.*&amp;quot;, &amp;quot;&amp;quot;, link))
  read &amp;lt;- read_html(link)
  
  sq_no &amp;lt;- read %&amp;gt;% 
    html_nodes(&amp;quot;.plainrowheaders td:nth-child(1)&amp;quot;) %&amp;gt;%
    html_text() %&amp;gt;%
    as.numeric()
  sq_names &amp;lt;- read %&amp;gt;%
    html_nodes(&amp;quot;.plainrowheaders a:nth-child(1)&amp;quot;) %&amp;gt;% 
    html_text() %&amp;gt;%
    .[. != &amp;quot;&amp;quot;] %&amp;gt;%
    .[!grepl(&amp;quot;^\\[&amp;quot;, .)] %&amp;gt;%
    .[. != &amp;quot;Unattached&amp;quot;] %&amp;gt;% 
    .[!grepl(&amp;quot;captain&amp;quot;, .)]
  sq_dobs &amp;lt;- read %&amp;gt;% 
    html_nodes(&amp;quot;.plainrowheaders td:nth-child(4)&amp;quot;) %&amp;gt;%
    html_text() %&amp;gt;%
    str_extract(., &amp;quot;[0-9]{4}-[0-9]{2}-[0-9]{2}&amp;quot;) %&amp;gt;% 
    as.Date()
  countries &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;h3 .mw-headline&amp;quot;) %&amp;gt;% 
    html_text() %&amp;gt;% 
    trimws()

  if(year &amp;gt; 2006) countries &amp;lt;- countries[1:32]
  
  squad_data &amp;lt;- data.frame(name = sq_names,
                           no = sq_no,
                           dob = sq_dobs,
                           year= year) %&amp;gt;%
    setDT() %&amp;gt;%
    .[!grepl(&amp;quot;Nery Pumpido&amp;quot;, name)] %&amp;gt;%
    .[no == 1, country := countries] %&amp;gt;%
    .[, country := na.locf(country)] %&amp;gt;%
    .[, c(&amp;quot;name&amp;quot;, &amp;quot;no&amp;quot;, &amp;quot;dob&amp;quot;, &amp;quot;year&amp;quot;, &amp;quot;country&amp;quot;)]
}))

#find all world cup squad players with shirt numbers greater than their age in years
young_players &amp;lt;- world_cup_squads %&amp;gt;%
  .[, age := as.numeric(difftime(as.Date(paste0(year, &amp;quot;-07-01&amp;quot;)), dob)) / 365] %&amp;gt;%
  .[age &amp;lt; no]

print(young_players)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                        name no        dob year    country      age
##   1:   Aleksandar Petakovic 22 1932-08-06 1954 Yugoslavia 21.91507
##   2:         Ranulfo Cortés 22 1934-07-09 1954     Mexico 19.99178
##   3:             Coskun Tas 22 1935-04-23 1954     Turkey 19.20274
##   4:            Omar Méndez 20 1934-08-07 1954    Uruguay 19.91233
##   5:          Johnny Haynes 21 1934-10-17 1954    England 19.71781
##  ---                                                              
## 110: Trent Alexander-Arnold 22 1998-10-07 2018    England 19.74521
## 111:    José Luis Rodríguez 21 1998-06-19 2018     Panama 20.04658
## 112:       Dávinson Sánchez 23 1996-06-12 2018   Colombia 22.06575
## 113:         Dawid Kownacki 23 1997-03-14 2018     Poland 21.31233
## 114:           Moussa Wagué 22 1998-10-04 2018    Senegal 19.75342&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Overall 114 players are found. England actually have the most players with shirt numbers higher than their age with 9: Haynes, Hooper, Owen, Ferdinand, Carson, Walcott, Barkeley, Shaw, Alexander-Arnold. Surprisingly, most of these young English callups are pretty recent.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(data = young_players, aes(year)) +
  geom_bar() +
  ggtitle(&amp;quot;Number of Players in World Cup Squads With Nos &amp;gt; Age&amp;quot;) +
  xlab(&amp;quot;World Cup Year&amp;quot;) +
  ylab(&amp;quot;Number&amp;quot;)

print(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-The_Knowledge_1_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It seems that the real high point for this was the turn of the century with young players being given a shot at the tail end of squads, which is returning to pre-1998 levels by 2018.&lt;/p&gt;
&lt;p&gt;The data on these squad players is then merged with the data on the winning teams to find those who played for nations who went on to win the world cup.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wc_winners &amp;lt;- data.frame(winner = c(&amp;quot;West Germany&amp;quot;,&amp;quot;Brazil&amp;quot;,&amp;quot;Brazil&amp;quot;,&amp;quot;England&amp;quot;,
                                    &amp;quot;Brazil&amp;quot;,&amp;quot;West Germany&amp;quot;,&amp;quot;Argentina&amp;quot;,&amp;quot;Italy&amp;quot;,
                                    &amp;quot;Argentina&amp;quot;,&amp;quot;West Germany&amp;quot;,&amp;quot;Brazil&amp;quot;,&amp;quot;France&amp;quot;,
                                    &amp;quot;Brazil&amp;quot;,&amp;quot;Italy&amp;quot;,&amp;quot;Spain&amp;quot;,&amp;quot;Germany&amp;quot;,&amp;quot;France&amp;quot;),
                         year = seq(1954, 2018, 4))

#merge data with winners and find matches
young_players %&amp;lt;&amp;gt;% .[, year := as.numeric(as.character(year))] %&amp;gt;%
  .[, country := gsub(&amp;quot;(^\\s+)|(\\s+$)&amp;quot;, &amp;quot;&amp;quot;, country)] %&amp;gt;%
  merge(., wc_winners, by = &amp;quot;year&amp;quot;) %&amp;gt;%
  .[winner == country]

#kaka only one to have played as per https://en.wikipedia.org/wiki/List_of_FIFA_World_Cup_winners#By_year
print(young_players)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    year    name no        dob country      age winner
## 1: 1970    Leão 22 1949-07-11  Brazil 20.98630 Brazil
## 2: 1994 Ronaldo 20 1976-09-22  Brazil 17.78356 Brazil
## 3: 2002    Kaká 23 1982-04-22  Brazil 20.20548 Brazil&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So only the great &lt;a href=&#34;https://en.wikipedia.org/wiki/%C3%89merson_Le%C3%A3o&#34;&gt;Émerson Leão&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Ronaldo_(Brazilian_footballer)&#34;&gt;Ronaldo&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Kak%C3%A1&#34;&gt;Kaka&lt;/a&gt; satisfy the question. However, of these only Kaka played any part during the tournament, which only amounted to 25 minutes vs Costa Rica.&lt;/p&gt;
&lt;p&gt;Which players &lt;em&gt;could&lt;/em&gt; have satisfied this if they had a larger squad number?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;youngest_players &amp;lt;- world_cup_squads %&amp;gt;%
  .[, age := as.numeric(difftime(as.Date(paste0(year, &amp;quot;-07-01&amp;quot;)), dob)) / 365] %&amp;gt;%
  .[age &amp;lt; 23] %&amp;gt;%
  .[, country := gsub(&amp;quot;(^\\s+)|(\\s+$)&amp;quot;, &amp;quot;&amp;quot;, country)] %&amp;gt;%
  .[, year := as.numeric(as.character(year))] %&amp;gt;%
  merge(., wc_winners, by = &amp;quot;year&amp;quot;) %&amp;gt;%
  .[winner == country] %&amp;gt;%
  .[, dob := NULL]

#gives 53 potential results with world cup winners under the age of 23
print(youngest_players)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     year                 name no      country      age       winner
##  1: 1954          Horst Eckel  6 West Germany 22.40822 West Germany
##  2: 1954     Ulrich Biesinger 18 West Germany 20.91507 West Germany
##  3: 1958                 Pelé 10       Brazil 17.69863       Brazil
##  4: 1958               Moacir 13       Brazil 22.13425       Brazil
##  5: 1958              Orlando 15       Brazil 22.79452       Brazil
##  6: 1958              Mazzola 18       Brazil 19.95068       Brazil
##  7: 1962             Coutinho  9       Brazil 19.06849       Brazil
##  8: 1962                 Pelé 10       Brazil 21.70137       Brazil
##  9: 1962             Jurandir 14       Brazil 21.64658       Brazil
## 10: 1962            Mengálvio 17       Brazil 22.55342       Brazil
## 11: 1962        Jair da Costa 18       Brazil 21.99178       Brazil
## 12: 1966            Alan Ball  7      England 21.15068      England
## 13: 1966        Martin Peters 16      England 22.66027      England
## 14: 1966        Norman Hunter 18      England 22.68767      England
## 15: 1970            Clodoaldo  5       Brazil 20.77534       Brazil
## 16: 1970        Marco Antônio  6       Brazil 19.41096       Brazil
## 17: 1970          Paulo Cézar 18       Brazil 21.05479       Brazil
## 18: 1970                  Edu 19       Brazil 20.91507       Brazil
## 19: 1970             Zé Maria 21       Brazil 21.13425       Brazil
## 20: 1970                 Leão 22       Brazil 20.98630       Brazil
## 21: 1974        Paul Breitner  3 West Germany 22.83562 West Germany
## 22: 1974           Uli Hoeneß 14 West Germany 22.50137 West Germany
## 23: 1974        Rainer Bonhof 16 West Germany 22.27123 West Germany
## 24: 1978    Alberto Tarantini 20    Argentina 22.59178    Argentina
## 25: 1978 José Daniel Valencia 21    Argentina 22.75890    Argentina
## 26: 1982        Franco Baresi  2        Italy 22.16164        Italy
## 27: 1982     Giuseppe Bergomi  3        Italy 18.53699        Italy
## 28: 1982      Daniele Massaro 17        Italy 21.12055        Italy
## 29: 1986       Claudio Borghi  4    Argentina 21.76986    Argentina
## 30: 1986           Luis Islas 15    Argentina 20.53699    Argentina
## 31: 1990       Andreas Möller 17 West Germany 22.84384 West Germany
## 32: 1994              Ronaldo 20       Brazil 17.78356       Brazil
## 33: 1998       Patrick Vieira  4       France 22.03562       France
## 34: 1998        Thierry Henry 12       France 20.88493       France
## 35: 1998      David Trezeguet 20       France 20.72329       France
## 36: 2002           Ronaldinho 11       Brazil 22.29315       Brazil
## 37: 2002                 Kaká 23       Brazil 20.20548       Brazil
## 38: 2006     Daniele De Rossi  4        Italy 22.95342        Italy
## 39: 2010            Juan Mata 13        Spain 22.18904        Spain
## 40: 2010      Sergio Busquets 16        Spain 21.97260        Spain
## 41: 2010                Pedro 18        Spain 22.94247        Spain
## 42: 2010        Javi Martínez 20        Spain 21.84110        Spain
## 43: 2014      Matthias Ginter  3      Germany 20.46027      Germany
## 44: 2014       Julian Draxler 14      Germany 20.79178      Germany
## 45: 2014            Erik Durm 15      Germany 22.15068      Germany
## 46: 2014          Mario Götze 19      Germany 22.09041      Germany
## 47: 2014     Shkodran Mustafi 21      Germany 22.21918      Germany
## 48: 2018      Benjamin Pavard  2       France 22.27397       France
## 49: 2018     Presnel Kimpembe  3       France 22.89863       France
## 50: 2018         Thomas Lemar  8       France 22.64932       France
## 51: 2018        Kylian Mbappé 10       France 19.54247       France
## 52: 2018      Ousmane Dembélé 11       France 21.14247       France
## 53: 2018      Lucas Hernández 21       France 22.39178       France
##     year                 name no      country      age       winner&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#most of these young players actually played at their world cups and many appeared in finals
youngest_players_appeared &amp;lt;- youngest_players[c(1, 3:6, 8, 12:13, 15:18, 21:23, 24:25, 27, 29, 31, 33:35, 36:37, 38, 39:42, 44, 46:47, 48:53)]

#find nearest matches
youngest_players_appeared %&amp;lt;&amp;gt;% .[, diff := age - no]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The closest other players to make it are David Trezeguet (1998, 20.7years no 20), Shkodran Mustafi (2014, 22.2years, no 21) and then Lucas Hernandez (22.4years, no 21). Hernandez is the closest one to actually play in the World Cup final. Alberto Tarantini is his closest competition at 22.6 years old and wearing shirt number 20 in the 1978 final.&lt;/p&gt;
&lt;div id=&#34;answer-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Answer&lt;/h2&gt;
&lt;p&gt;Yes, three winners have appeared in World Cups with an age less than their shirt number. All Brazilians: Émerson Leão in 1970, Ronaldo in 1994, and Kaka in 2002. However only Kaka actually played (for 25 minutes vs. Costa Rica) in the finals.&lt;/p&gt;
&lt;p&gt;Other close calls are David Trezeguet (20.7, no 20 in 1998) and Shkodran Mustafi (22.2, no 21 in 2014).&lt;/p&gt;
&lt;p&gt;Hernandez &lt;em&gt;is&lt;/em&gt; the closest to acheiving this having played in the final itself, with only Alberto Tarantini (22.5, no 20 in 1978) and Mario Goetze (22.1, no 19 in 2014) in close competition.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Could an Independent Yorkshire Win the World Cup - Rest of the World/UK</title>
      <link>/post/yorkshire_world_cup_6/</link>
      <pubDate>Sun, 10 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/yorkshire_world_cup_6/</guid>
      <description>


&lt;p&gt;Recently, a Yorkshire national football team &lt;a href=&#34;https://www.theguardian.com/uk-news/2018/jan/28/yorkshire-football-team-makes-debut-in-world-league-of-stateless-peoples&#34;&gt;appeared in a league of national teams for stateless people&lt;/a&gt;. This got me wondering how the historic counties of the UK would do at the world cup. Could any of them compete with full international teams?&lt;/p&gt;
&lt;p&gt;I &lt;a href=&#34;http://www.robert-hickman.eu/post/yorkshire_world_cup_1/&#34;&gt;published&lt;/a&gt; the complete code for that article on this blog this week. However, one question which I kept being asked was how a ‘All of the UK’ team would do (i.e. if the country wasn’t split up into England, Wales, Scotland, and Northern Ireland). Listening to the latest &lt;a href=&#34;https://twitter.com/doublepivotpod?lang=en&#34;&gt;Double Pivot Podcast&lt;/a&gt;, drafting plyers not going to the World Cup, I also wondered what a ‘Rest of the World’ 11 would look like/fare.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(magrittr)
library(data.table)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;building-teams&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Building Teams&lt;/h1&gt;
&lt;p&gt;To save time, I’m gonig to used saved versions of the datasets I built up over the 5 blog posts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#world rankings
world_rankings &amp;lt;- readRDS(&amp;quot;national_rankings.rds&amp;quot;)

#player data
all_players_data &amp;lt;- readRDS(&amp;quot;all_players_position_data.rds&amp;quot;)
#all British players
british_player_birthplaces &amp;lt;- readRDS(&amp;quot;british_player_birthplaces.rds&amp;quot;)

#the countries going to the world cup
world_cup_countries &amp;lt;- c(&amp;quot;Russia&amp;quot;, &amp;quot;Saudi Arabia&amp;quot;, &amp;quot;Egypt&amp;quot;, &amp;quot;Uruguay&amp;quot;,
                         &amp;quot;Portugal&amp;quot;, &amp;quot;Spain&amp;quot;, &amp;quot;Morocco&amp;quot;, &amp;quot;Iran&amp;quot;,
                         &amp;quot;France&amp;quot;, &amp;quot;Australia&amp;quot;, &amp;quot;Peru&amp;quot;, &amp;quot;Denmark&amp;quot;,
                         &amp;quot;Argentina&amp;quot;, &amp;quot;Iceland&amp;quot;, &amp;quot;Croatia&amp;quot;, &amp;quot;Nigeria&amp;quot;,
                         &amp;quot;Brazil&amp;quot;, &amp;quot;Switzerland&amp;quot;, &amp;quot;Costa Rica&amp;quot;, &amp;quot;Serbia&amp;quot;,
                         &amp;quot;Germany&amp;quot;, &amp;quot;Mexico&amp;quot;, &amp;quot;Sweden&amp;quot;, &amp;quot;Korea Republic&amp;quot;,
                         &amp;quot;Belgium&amp;quot;, &amp;quot;Panama&amp;quot;, &amp;quot;Tunisia&amp;quot;, &amp;quot;England&amp;quot;,
                         &amp;quot;Poland&amp;quot;, &amp;quot;Senegal&amp;quot;, &amp;quot;Colombia&amp;quot;, &amp;quot;Japan&amp;quot;)

#load data to save having to recalculate optimal teams
optimal_national_teams &amp;lt;- readRDS(&amp;quot;optimal_national_teams.rds&amp;quot;)
national_teams &amp;lt;- readRDS(&amp;quot;national_teams.rds&amp;quot;)

#the formations for selecting teams
formations_df &amp;lt;- readRDS(&amp;quot;formations_df.rds&amp;quot;)
formation_coords &amp;lt;- readRDS(&amp;quot;player_position_coords.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I won’t include the functions in this blog post either, but the article uses (at most very slight modified) functions from the previous 5 posts.&lt;/p&gt;
&lt;p&gt;We first need to sort the players into either the UK vs. the rest of the World* and finding the optimal teams for each, as we did prviously.&lt;/p&gt;
&lt;p&gt;*it’s possible Welsh (especially Gareth Bale), Northern Irish, or Scottish players might make the rest of the World team, but I’ll ignore that possibility for simplicity&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get the names of each player to merge in
player_lookup &amp;lt;- all_players_data %&amp;gt;%
  select(id, name, nationality) %&amp;gt;%
  mutate(original_nation = as.character(nationality))

#sort the data for finding teams
nationalised_players &amp;lt;- all_players_data %&amp;gt;%
  setDT() %&amp;gt;%
  #convert british players nationality to UK
  .[id %in% british_player_birthplaces$id, nationality := &amp;quot;UK&amp;quot;] %&amp;gt;%
  #filter out players from countries at the world cup
  .[!nationality %in% world_cup_countries] %&amp;gt;%
  #convert non-UK players nationality to &amp;quot;Rest of World&amp;quot;
  .[!id %in% british_player_birthplaces$id, nationality := &amp;quot;RoW&amp;quot;]

#find the optimal teams for both these nations
extranational_teams &amp;lt;- rbindlist(lapply(unique(nationalised_players$nationality), find_optimal_team, 
                                           select(nationalised_players, id, nationality, 49:60), replicates = 100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These can then be plotted to show the teams as before.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#select the best 4 county teams by total ability
extranational_teams %&amp;lt;&amp;gt;%
  setDT() %&amp;gt;%
  .[, unique_position := make.unique(as.character(position)), by = &amp;quot;nation&amp;quot;] %&amp;gt;%
  merge(., formation_coords, by = c(&amp;quot;formation&amp;quot;, &amp;quot;unique_position&amp;quot;)) %&amp;gt;%
  merge(player_lookup, by = &amp;quot;id&amp;quot;) 

#plot the data
p &amp;lt;- ggplot(data = extranational_teams)
p &amp;lt;- p %&amp;gt;%
  #custom pitch aesthetic function
  draw_pitch()
p &amp;lt;- p + 
  geom_text(aes(x = player_x, y = player_y, label = gsub(&amp;quot; &amp;quot;, &amp;quot;\n&amp;quot;, name), colour = original_nation), fontface = &amp;quot;bold&amp;quot;) +
  scale_colour_manual(values = c(&amp;quot;darkred&amp;quot;, &amp;quot;white&amp;quot;, &amp;quot;yellow&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;darkblue&amp;quot;, &amp;quot;orange&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;white&amp;quot;, &amp;quot;red&amp;quot;),
                      guide = FALSE) +
  facet_wrap(~nation)

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_6_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-ability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Calculating Ability&lt;/h1&gt;
&lt;p&gt;As previously, we can calculate the expected ELO of such teams via linear regression of the FIFA18 ability vs. ELO of actual national teams.&lt;/p&gt;
&lt;p&gt;This time, let’s predict the ability of the extranational teams based on this regression before plotting, just to save on plots/time/code/etc.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#merge in the world rankings for each fieldable national team
national_teams %&amp;lt;&amp;gt;% merge(., world_rankings, by = &amp;quot;nation&amp;quot;) %&amp;gt;%
  #merge in the optimal team total_ability for each nation
  merge(., unique(select(optimal_national_teams, nation, total_ability)), by = &amp;quot;nation&amp;quot;)

#regress ELO against total_ability (as judged by selection of FIFA18 players)
ability_regression &amp;lt;- lm(data = national_teams, ELO ~ total_ability)

#munge the extranational teams df to predict the ELO
extranational_teams &amp;lt;- data.frame(nation = c(&amp;quot;UK&amp;quot;, &amp;quot;RoW&amp;quot;)) %&amp;gt;%
  merge(., select(extranational_teams, nation, total_ability), by = &amp;quot;nation&amp;quot;) %&amp;gt;%
  #predict the ELO of each county using the previous regression
  mutate(predicted_ELO = predict(ability_regression, .)) %&amp;gt;%
  unique()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can plot this regression and see where the RoW and UK fall in terms of actual nations&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot ELO vs. total_ability
p &amp;lt;- ggplot(data = national_teams, aes(x = total_ability, y = ELO)) +
  geom_text(aes(label = nation), colour = &amp;quot;grey60&amp;quot;) +
  #add in the linear regression line + confidence intervals
  stat_smooth(method = &amp;quot;lm&amp;quot;, colour = &amp;quot;darkred&amp;quot;) +
  geom_text(data = extranational_teams, aes(label = nation, x = total_ability, y = predicted_ELO), colour = &amp;quot;darkblue&amp;quot;) +
  xlab(&amp;quot;FIFA18 Optimal Team Ability&amp;quot;) +
  ylab(&amp;quot;National Team ELO&amp;quot;) +
  ggtitle(&amp;quot;FIFA18 ability vs. ELO for National Teams&amp;quot;) +
  theme_minimal()

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_6_files/figure-html/plot_regression-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What’s quite nice about the graph is it shows the limitation of this approach. By definition, a UK team should be &lt;em&gt;at least&lt;/em&gt; as good as the English national team, but because England overperform their ‘FIFA ability’, the UK is actually ranked a fair bit lower in terms of ELO&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#show the ELOs of the English national football team
#and predicted ELO of a UK team
national_teams$ELO[national_teams$nation == &amp;quot;England&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1941&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extranational_teams$predicted_ELO[extranational_teams$nation == &amp;quot;UK&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       12 
## 1910.421&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The RoW team is similarly probably undervalued in terms of ELO. FIFA18 ranks the players as a lot better than teams like Germany and Brazil, but with much lower ELO&lt;/p&gt;
&lt;p&gt;We can then run the simulations, swapping the UK/RoW in for countries. The obvious substitute for the UK is England. For the RoW I decided to remove the team with the lowest ELO, which turns out to be Saudi Arabia&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#merge the ELOs with the world cup draw information
wc_teams %&amp;lt;&amp;gt;% merge(., select(national_teams, nation, ELO) %&amp;gt;%
                      rbind(., data.frame(nation = &amp;quot;Panama&amp;quot;, ELO = 1669)), by = &amp;quot;nation&amp;quot;)
wc_teams$nation &amp;lt;- as.character(wc_teams$nation)

simulate_counties &amp;lt;- function(extranation, simulations, replace_country) {
  #replace Englands ELO with that of the county team replacing them
  wc_teams$ELO[wc_teams$nation == replace_country] &amp;lt;- extranational_teams$predicted_ELO[extranational_teams$nation == extranation]
  wc_teams$nation[wc_teams$nation == replace_country] &amp;lt;- extranation
  
  #run x number of simulations
  for(simulation in 1:simulations) {
    winner &amp;lt;- simulate_tournament(wc_teams, knockout_matches, group_matches)
    if(simulation == 1) {
      winners &amp;lt;- winner
    } else {
      winners &amp;lt;- append(winners, winner)
    }
  }
  
  #spit out a df with each winner and the number of times they win
  simulation_df &amp;lt;- data.frame(table(winners))
  names(simulation_df) &amp;lt;- c(&amp;quot;nation&amp;quot;, &amp;quot;championships&amp;quot;)
  
  #work out the percentage chane of each nation/county winning
  simulation_df$percentage &amp;lt;- simulation_df$championships / (simulations/100)
  return(simulation_df)
}

#run the simulations
UK_simulation &amp;lt;- simulate_counties(&amp;quot;UK&amp;quot;, 1000, &amp;quot;England&amp;quot;) %&amp;gt;% 
  mutate(simulation = &amp;quot;UK&amp;quot;)
RoW_simulation &amp;lt;- simulate_counties(&amp;quot;RoW&amp;quot;, 1000, wc_teams$nation[which.min(wc_teams$ELO)]) %&amp;gt;%
  mutate(simulation = &amp;quot;RoW&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simulation_results &amp;lt;- rbind(UK_simulation, RoW_simulation) %&amp;gt;%
  setDT() %&amp;gt;%
    .[, perc_chance := mean(percentage), by = &amp;quot;nation&amp;quot;] %&amp;gt;%
  .[, c(&amp;quot;nation&amp;quot;, &amp;quot;perc_chance&amp;quot;)] %&amp;gt;%
  unique(.) %&amp;gt;%
  .[nation %in% c(&amp;quot;RoW&amp;quot;, &amp;quot;UK&amp;quot;), nation_status := &amp;quot;simulation&amp;quot;] %&amp;gt;%
  .[!nation %in% c(&amp;quot;RoW&amp;quot;, &amp;quot;UK&amp;quot;), nation_status := &amp;quot;nation&amp;quot;] %&amp;gt;%
  #order by percentage chance of winning the WC
  .[, nation := factor(nation, levels = nation[order(-.$perc_chance)])]

#plot the results
p &amp;lt;- ggplot(data = simulation_results, aes(x = nation, y = perc_chance)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, aes(fill = nation_status)) +
  scale_fill_manual(values = c(&amp;quot;darkblue&amp;quot;, &amp;quot;darkred&amp;quot;), name = &amp;quot;Nation Status&amp;quot;) +
  xlab(&amp;quot;Team&amp;quot;) +
  ylab(&amp;quot;World Cup Win Percentage Chance&amp;quot;) +
  ggtitle(&amp;quot;Percetange Chance of Winning the World Cup from 1000 Simulations&amp;quot;,
          subtitle = &amp;quot;UK/RoW Substituted for England/Saudi Arabia Respectively&amp;quot;) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1.2))

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_6_files/figure-html/munge_simulation_results-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The team for the RoW tend to do fairly well. In fact only Brazil, Germany, or Spain (3 of the tournament favourites) tend to win more simulated World Cups than them. The team for the whole of the UK disappoints as much as the English national team, winning about the same as the original, and other similarly ranked nations, such as Colombia, or Peru.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Could an Independent Yorkshire Win the World Cup - Simulate World Cups</title>
      <link>/post/yorkshire_world_cup_5/</link>
      <pubDate>Sat, 09 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/yorkshire_world_cup_5/</guid>
      <description>


&lt;p&gt;Recently, a Yorkshire national football team &lt;a href=&#34;https://www.theguardian.com/uk-news/2018/jan/28/yorkshire-football-team-makes-debut-in-world-league-of-stateless-peoples&#34;&gt;appeared in a league of national teams for stateless people&lt;/a&gt;. This got me wondering how the historic counties of the UK would do at the world cup. Could any of them compete with full international teams?&lt;/p&gt;
&lt;p&gt;This is the complete script for an short article I wrote for &lt;a href=&#34;https://www.citymetric.com/horizons/football-could-independent-yorkshire-win-world-cup-3961&#34;&gt;CityMetric&lt;/a&gt; on the topic. It’s split over 5 separate parts and is pretty hefty but contains pretty much everything you need to clone the article. Now that we’ve picked the teams for each nation and county, it’s finally time to make predictions about the World Cup.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(magrittr)
library(data.table)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;get-county-rankings&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Get County Rankings&lt;/h1&gt;
&lt;p&gt;Now that we have the teams for each county, we want to work out how well they would do at a world cup. For this, we need to know roughly what their ranking would be compared to actual nations.&lt;/p&gt;
&lt;p&gt;Two sources of rankings of nations are the official FIFA world rankings, and also the world ELO ratings of each nation at www.eloratings.net.&lt;/p&gt;
&lt;p&gt;I scraped both of these (accurate to mid-May) and cleaned the data to match the nation names to those in the player dataset we’re using.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#scraped world rankings from FIFA and world ELO
#http://www.fifa.com/fifa-world-ranking/ranking-table/men/index.html
#https://www.eloratings.net/
#accurate for mid-May
#have matched country names between world rankings and FIFA player data
world_rankings &amp;lt;- readRDS(&amp;quot;national_rankings.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#glimpse the data
head(world_rankings)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      nation  ELO FIFA
## 1    Brazil 2131    2
## 2   Germany 2092    1
## 3     Spain 2049    8
## 4 Argentina 1985    5
## 5    France 1984    7
## 6  Portugal 1975    4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ELO is a chess rating mechanism which can be used to make predictions about which team would win in a matchup. If we compare it to the FIFA rankings, we can see there’s a clear negative correlation (the lower the ranking (e.g. top 10 teams in the world), the higher the ELO)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot FIFA rankings vs. ELO
p &amp;lt;- ggplot(data = world_rankings, aes(x = FIFA, y = ELO)) +
  geom_text(aes(label = nation)) +
  xlab(&amp;quot;FIFA Ability&amp;quot;) +
  ylab(&amp;quot;ELO Rankings&amp;quot;) +
  ggtitle(&amp;quot;The FIFA World Rankings and ELO Rankings for Countries&amp;quot;)

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_5_files/figure-html/plot_world_rankings-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To validate our method, the total ability of each team from their players in FIFA18 should correlate with this ELO rating.&lt;/p&gt;
&lt;p&gt;If we merge in the optimal team data and plot it against ELO we see nice linear positive correlation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#merge in the world rankings for each fieldable national team
national_teams %&amp;lt;&amp;gt;% merge(., world_rankings, by = &amp;quot;nation&amp;quot;) %&amp;gt;%
  #merge in the optimal team total_ability for each nation
  merge(., unique(select(optimal_national_teams, nation, total_ability)), by = &amp;quot;nation&amp;quot;)

head(national_teams)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      nation players gks  ELO FIFA total_ability
## 1   Albania      36   2 1596   56      73.44459
## 2   Algeria      58   3 1524   64      77.86387
## 3    Angola      16   1 1259  138      69.03657
## 4 Argentina     875 100 1985    5      84.97171
## 5 Australia     199  33 1714   40      74.21456
## 6   Austria     226  39 1726   26      78.65603&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot ELO vs. total_ability
p &amp;lt;- ggplot(data = national_teams, aes(x = total_ability, y = ELO)) +
  geom_text(aes(label = nation), colour = &amp;quot;grey60&amp;quot;) +
  xlab(&amp;quot;FIFA18 Optimal Team Ability&amp;quot;) +
  ylab(&amp;quot;National Team ELO&amp;quot;) +
  ggtitle(&amp;quot;FIFA18 ability vs. ELO for National Teams&amp;quot;) +
  theme_minimal()

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_5_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can quanitfy this correlation by creating a linear model using lm() and see that the adjusted R-squared is rather high- 0.7354.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#regress ELO against total_ability (as judged by selection of FIFA18 players)
ability_regression &amp;lt;- lm(data = national_teams, ELO ~ total_ability)

#summary
summary(ability_regression)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = ELO ~ total_ability, data = national_teams)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -222.229  -58.773    2.228   48.415  274.785 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   -755.764    160.669  -4.704 1.02e-05 ***
## total_ability   32.133      2.111  15.221  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 97.31 on 82 degrees of freedom
## Multiple R-squared:  0.7386, Adjusted R-squared:  0.7354 
## F-statistic: 231.7 on 1 and 82 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also plot this regression to further convince ourselves that predicting ELO from FIFA18 ability is a fairly valid move.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot ELO vs. total_ability
p &amp;lt;- ggplot(data = national_teams, aes(x = total_ability, y = ELO)) +
  geom_text(aes(label = nation), colour = &amp;quot;grey60&amp;quot;) +
  #add in the linear regression line + confidence intervals
  stat_smooth(method = &amp;quot;lm&amp;quot;, colour = &amp;quot;darkred&amp;quot;) +
  xlab(&amp;quot;FIFA18 Optimal Team Ability&amp;quot;) +
  ylab(&amp;quot;National Team ELO&amp;quot;) +
  ggtitle(&amp;quot;FIFA18 ability vs. ELO for National Teams&amp;quot;) +
  theme_minimal()

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_5_files/figure-html/plot_regression-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we have a predictor for ELO based on FIFA18 ability, we can now predict the ELO of each county team. We simply feed the model back into our df of optimal county teams.&lt;/p&gt;
&lt;p&gt;If we plot this over the previous plot we can see the counties have ELOs which fall within a range of national team abilities. The best counties (Yorkshire and Lancashire) are about as good as teams which generally qualify for world cups (e.g. Sweden and Serbia) whereas some counties (e.g. ) are much less proficient and would probably struggle to qualify.&lt;/p&gt;
&lt;p&gt;Given the teams we saw that were selected earlier, this makes sense- Yorkshire and Lancashire can field generally pretty solid teams of international/near-international level footballers and so would be expected to be competitive.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county_teams %&amp;lt;&amp;gt;% merge(., select(optimal_county_teams, county = nation, total_ability), by = &amp;quot;county&amp;quot;) %&amp;gt;%
  #predict the ELO of each county using the previous regression
  mutate(predicted_ELO = predict(ability_regression, .))

#add these to the plots of ELO ~ FIFA team ability
plot(p + geom_text(data = county_teams, aes(x = total_ability, y = predicted_ELO, label = county), colour = &amp;quot;darkblue&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_5_files/figure-html/predict_county_ELO-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulate-world-cups&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulate World Cups&lt;/h1&gt;
&lt;p&gt;Finally, we want to know if any of these counties have a shot at winning the World Cup.&lt;/p&gt;
&lt;p&gt;To do this, the best method is simply to simulate lots of World Cups and see what the percentage chance for each team is. This is possible as ELO gives us a quantifiable measure of how likely a given team is to beat another.&lt;/p&gt;
&lt;p&gt;Before we can simulate the World Cup however, we need some information about the draw.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wc_teams &amp;lt;- data.frame(nation = c(&amp;quot;Russia&amp;quot;, &amp;quot;Saudi Arabia&amp;quot;, &amp;quot;Egypt&amp;quot;, &amp;quot;Uruguay&amp;quot;,
                                  &amp;quot;Portugal&amp;quot;, &amp;quot;Spain&amp;quot;, &amp;quot;Morocco&amp;quot;, &amp;quot;Iran&amp;quot;,
                                  &amp;quot;France&amp;quot;, &amp;quot;Australia&amp;quot;, &amp;quot;Peru&amp;quot;, &amp;quot;Denmark&amp;quot;,
                                  &amp;quot;Argentina&amp;quot;, &amp;quot;Iceland&amp;quot;, &amp;quot;Croatia&amp;quot;, &amp;quot;Nigeria&amp;quot;,
                                  &amp;quot;Brazil&amp;quot;, &amp;quot;Switzerland&amp;quot;, &amp;quot;Costa Rica&amp;quot;, &amp;quot;Serbia&amp;quot;,
                                  &amp;quot;Germany&amp;quot;, &amp;quot;Mexico&amp;quot;, &amp;quot;Sweden&amp;quot;, &amp;quot;Korea Republic&amp;quot;,
                                  &amp;quot;Belgium&amp;quot;, &amp;quot;Panama&amp;quot;, &amp;quot;Tunisia&amp;quot;, &amp;quot;England&amp;quot;,
                                  &amp;quot;Poland&amp;quot;, &amp;quot;Senegal&amp;quot;, &amp;quot;Colombia&amp;quot;, &amp;quot;Japan&amp;quot;),
                       group = c(rep(letters[1:8], each = 4)),
                       draw = rep(1:4, 8))

group_matches &amp;lt;- data.frame(match = 1:6,
                            team1 = c(1,3,1,4,4,2),
                            team2 = c(2,4,3,2,1,3))

knockout_matches &amp;lt;- data.frame(round = c(rep(&amp;quot;R16&amp;quot;, 8), rep(&amp;quot;QF&amp;quot;, 4), rep(&amp;quot;SF&amp;quot;, 2), &amp;quot;F&amp;quot;),
                               team1 = c(&amp;quot;a1&amp;quot;, &amp;quot;c1&amp;quot;, &amp;quot;e1&amp;quot;, &amp;quot;g1&amp;quot;, &amp;quot;b1&amp;quot;, &amp;quot;d1&amp;quot;, &amp;quot;f1&amp;quot;, &amp;quot;h1&amp;quot;,
                                         &amp;quot;m49&amp;quot;, &amp;quot;m53&amp;quot;, &amp;quot;m51&amp;quot;, &amp;quot;m55&amp;quot;, &amp;quot;m57&amp;quot;, &amp;quot;m59&amp;quot;, &amp;quot;m61&amp;quot;),
                               team2 = c(&amp;quot;b2&amp;quot;, &amp;quot;d2&amp;quot;, &amp;quot;f2&amp;quot;, &amp;quot;h2&amp;quot;, &amp;quot;a2&amp;quot;, &amp;quot;c2&amp;quot;, &amp;quot;e2&amp;quot;, &amp;quot;g2&amp;quot;,
                                         &amp;quot;m50&amp;quot;, &amp;quot;m54&amp;quot;, &amp;quot;m52&amp;quot;, &amp;quot;m56&amp;quot;, &amp;quot;m58&amp;quot;, &amp;quot;m60&amp;quot;, &amp;quot;m62&amp;quot;),
                               match_id = c(&amp;quot;m49&amp;quot;, &amp;quot;m50&amp;quot;, &amp;quot;m53&amp;quot;, &amp;quot;m54&amp;quot;, &amp;quot;m51&amp;quot;, &amp;quot;m52&amp;quot;, &amp;quot;m55&amp;quot;, &amp;quot;m56&amp;quot;,
                                            &amp;quot;m57&amp;quot;, &amp;quot;m58&amp;quot;, &amp;quot;m59&amp;quot;, &amp;quot;m60&amp;quot;, &amp;quot;m61&amp;quot;, &amp;quot;m62&amp;quot;, &amp;quot;FINAL&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then we need to write functions to do the simulation.&lt;/p&gt;
&lt;p&gt;The first of these simply takes the ELO of the two teams and works out the win percentage for teamA (for teamB = 1 - p(teamA)).&lt;/p&gt;
&lt;p&gt;This is used in two further functions which simulate the group stages, and then the knockout stages respectively.&lt;/p&gt;
&lt;p&gt;For the groups, teams are drawn against each other as they will be in Russia and their ELOs compared. A random number generator is used to decided which teams wins (if p(teamA wins based on ELO) &amp;gt; random_number, teamA wins). I also included the chance to draw if the difference between the win_chance and the random_number is less than 0.1 in either direction.&lt;/p&gt;
&lt;p&gt;The points each team is predicted to win in the groups is then summed and the top two teams from each group progresses to the knockout stage.&lt;/p&gt;
&lt;p&gt;The knockout stage is easier to simulate as we don’t need to worry about points/draws. The same method as above is used to predict the winning team and that team progresses, whilst we remove the other from a df. Eventually only one team is left- the winner of the tournament.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#uses ELO to calculate the chance of team A winning
calc_win_chance &amp;lt;- function(ratingA, ratingB) {
  win_chance &amp;lt;- 1/ (1+10^((ratingB-ratingA)/400))
}

#simulate the group stages of the tournament
simulate_groups &amp;lt;- function(group_letter, national_teams, group_matches) {
  group &amp;lt;- national_teams %&amp;gt;%
    filter(group == group_letter) %&amp;gt;%
    mutate(points = 0) %&amp;gt;%
    mutate(av_difference = 0) %&amp;gt;%
    arrange(draw)
  
  #six matches per group
  for(match in 1:6){
    team1 &amp;lt;- group$nation[group_matches$team1[match]]
    team2 &amp;lt;- group$nation[group_matches$team2[match]]
    
    #calculate winner using a random number generator and comparing to the ELO win percentages
    random_number_draw &amp;lt;- runif(1)
    win_chance &amp;lt;- calc_win_chance(group$ELO[group$nation == team1], group$ELO[group$nation == team2])
    
    #update ELOs and assign group stage points
    if(win_chance - random_number_draw &amp;gt; 0.1) {
      group$points[group$nation == team1] &amp;lt;- group$points[group$nation == team1] + 3
      group$points[group$nation == team2] &amp;lt;- group$points[group$nation == team2] + 0
      
      group$ELO[group$nation == team1] &amp;lt;- group$ELO[group$nation == team1] + 50*(1-win_chance)
      group$ELO[group$nation == team2] &amp;lt;- group$ELO[group$nation == team2] + 50*(0-(1-win_chance))
      
    } else if(win_chance - random_number_draw &amp;lt; -0.1) {
      group$points[group$nation == team1] &amp;lt;- group$points[group$nation == team1] + 0
      group$points[group$nation == team2] &amp;lt;- group$points[group$nation == team2] + 3
      
      group$ELO[group$nation == team1] &amp;lt;- group$ELO[group$nation == team1] + 50*(0-win_chance)
      group$ELO[group$nation == team2] &amp;lt;- group$ELO[group$nation == team2] + 50*(1-(1-win_chance))

    } else {
      group$points[group$nation == team1] &amp;lt;- group$points[group$nation == team1] + 1
      group$points[group$nation == team2] &amp;lt;- group$points[group$nation == team2] + 1
      
      group$ELO[group$nation == team1] &amp;lt;- group$ELO[group$nation == team1] + 50*(0.5-win_chance)
      group$ELO[group$nation == team2] &amp;lt;- group$ELO[group$nation == team2] + 50*(0.5-(1-win_chance))
    }
    
    group$av_difference[group$nation == team1] &amp;lt;- group$av_difference[group$nation == team1] + 
      (group$ELO[group$nation == team1] - group$ELO[group$nation == team2])
    group$av_difference[group$nation == team2] &amp;lt;- group$av_difference[group$nation == team2] - 
      (group$ELO[group$nation == team1] - group$ELO[group$nation == team2])
  }
  
  #arrange the groups by points per team, then by the ELO difference between a team and it&amp;#39;s rivals
  #use ELO difference as secondary sorter as proxy for goal difference
  group &amp;lt;- arrange(group, -points, -av_difference) %&amp;gt;%
    mutate(position = 1:4)
  return(group)
}

#simulate the knockout rounds
simulate_knockout_rounds &amp;lt;- function(national_teams, knockout_matches) {
  for(match in seq(nrow(knockout_matches))) {
    #get the teams and the match id
    team1 &amp;lt;- as.character(national_teams$nation[which(national_teams$id == knockout_matches$team1[match])])
    team2 &amp;lt;- as.character(national_teams$nation[which(national_teams$id == knockout_matches$team2[match])])
    match_id &amp;lt;- as.character(knockout_matches$match_id[match])
    
    national_teams$id[which(national_teams$nation %in% c(team1, team2))] &amp;lt;- match_id
    
    #use a random number generator to decide the winner
    random_number_draw &amp;lt;- runif(1)
    
    #use ELO chances vs. the random number to work out which team wins
    win_chance &amp;lt;- calc_win_chance(national_teams$ELO[national_teams$nation == team1], national_teams$ELO[national_teams$nation == team2])
    
    #update ELOs and remove losing team
    if(win_chance &amp;gt; random_number_draw) {
      national_teams$ELO[national_teams$nation == team1] &amp;lt;- national_teams$ELO[national_teams$nation == team1] + 50*(1-win_chance)
      national_teams &amp;lt;- national_teams[-which(national_teams$nation == team2),]
    } else {
      national_teams$ELO[national_teams$nation == team2] &amp;lt;- national_teams$ELO[national_teams$nation == team2] + 50*(1-(1-win_chance))
      national_teams &amp;lt;- national_teams[-which(national_teams$nation == team1),]
    }
  }
  #returns the nation from the last remain row of the df == the winner of the tournament
  return(national_teams$nation)
}

#simulate the whole tournament
simulate_tournament &amp;lt;- function(national_teams, knockout_matches, group_matches) {
  #simulate the group stages
  knockout_rounds &amp;lt;- rbindlist(lapply(letters[1:8], simulate_groups,  
                                      national_teams = national_teams, group_matches = group_matches)) %&amp;gt;%
    #filter the top two teams from each group
    filter(position &amp;lt; 3) %&amp;gt;%
    mutate(id = paste0(group, position)) %&amp;gt;%
    select(nation, ELO, id)
  
  #simulate the knockout rounds until only 1 team remains
  winner &amp;lt;- simulate_knockout_rounds(national_teams = knockout_rounds, knockout_matches = knockout_matches) %&amp;gt;%
    as.character()
  return(winner)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To simulate the world cups, first we merge the ELO data with the world cup draw information. We also have to add Panama as they were missing from the teams based on our player data.&lt;/p&gt;
&lt;p&gt;Then here I run 10 simulations of the tournament and print the winners. Generally the clear favourites of the simulation are Brazil, then Germany, Spain and Argentina. This makes sense- they have the highest ELOs of all the nations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#merge the ELOs with the world cup draw information
wc_teams %&amp;lt;&amp;gt;% merge(., select(national_teams, nation, ELO) %&amp;gt;%
                      rbind(., data.frame(nation = &amp;quot;Panama&amp;quot;, ELO = 1669)), by = &amp;quot;nation&amp;quot;)

#run 10 simulations of the world cup choosing winners via ELO
for(simulation in 1:10) {
    winner &amp;lt;- simulate_tournament(wc_teams, knockout_matches, group_matches)
    if(simulation == 1) {
      winners &amp;lt;- winner
    } else {
      winners &amp;lt;- append(winners, winner)
    }
}

#list the winners of these 10 simulations
winners&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Belgium&amp;quot;  &amp;quot;Brazil&amp;quot;   &amp;quot;Germany&amp;quot;  &amp;quot;Brazil&amp;quot;   &amp;quot;Mexico&amp;quot;   &amp;quot;Brazil&amp;quot;  
##  [7] &amp;quot;Spain&amp;quot;    &amp;quot;France&amp;quot;   &amp;quot;Spain&amp;quot;    &amp;quot;Colombia&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can substitute in each county for the English national team and run x simulations (I use 10000 as anything more would take an unrealistic amount of processing time) to see what the chance of them winning the world cup would be.&lt;/p&gt;
&lt;p&gt;I iterate this through each county and then get a df of the chances for every nation (-England) and that county to win.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simulate_counties &amp;lt;- function(county, simulations) {
  #replace Englands ELO with that of the county team replacing them
  wc_teams$ELO[wc_teams$nation == &amp;quot;England&amp;quot;] &amp;lt;- county_teams$predicted_ELO[county_teams$county == county]
  
  #run x number of simulations
  for(simulation in 1:simulations) {
    winner &amp;lt;- simulate_tournament(wc_teams, knockout_matches, group_matches)
    #if &amp;#39;England&amp;#39; wins, replace England with the county
    if(winner == &amp;quot;England&amp;quot;) {
      winner &amp;lt;- county
    }
    if(simulation == 1) {
      winners &amp;lt;- winner
    } else {
      winners &amp;lt;- append(winners, winner)
    }
  }
  
  #spit out a df with each winner and the number of times they win
  simulation_df &amp;lt;- data.frame(table(winners))
  names(simulation_df) &amp;lt;- c(&amp;quot;nation&amp;quot;, &amp;quot;championships&amp;quot;)
  
  #work out the percentage chane of each nation/county winning
  simulation_df$percentage &amp;lt;- simulation_df$championships / (simulations/100)
  simulation_df$county_test &amp;lt;- county
  return(simulation_df)
}

#run for many simulations
#TAKES A LOT OF TIME
simulations_results &amp;lt;- rbindlist(lapply(county_team_rankings$nation, simulate_counties, 10000))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have that data out, some munging is necessary to get the average chance of winning the World Cup for each nation and label the counties and nations separately.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#munge the simulation_results
simulation_results %&amp;lt;&amp;gt;% setDT() %&amp;gt;%
  #get the average wc winning chance per nation across all simulations
  .[, perc_chance := mean(percentage), by = &amp;quot;nation&amp;quot;] %&amp;gt;%
  .[, perc_var := var(percentage), by = &amp;quot;nation&amp;quot;] %&amp;gt;%
  .[, c(&amp;quot;nation&amp;quot;, &amp;quot;perc_chance&amp;quot;, &amp;quot;perc_var&amp;quot;)] %&amp;gt;%
  unique(.) %&amp;gt;%
  #bind in the nations which never win the world cup in any simulation
  rbind(., unique(data.frame(nation = county_teams$county[which(!county_teams$county %in% .$nation)],
                      perc_chance = 0,
                      perc_var = NA))) %&amp;gt;%
  #is the team a nation or a county
  .[nation %in% county_teams$county, nation_status := &amp;quot;county&amp;quot;] %&amp;gt;%
  .[!nation %in% county_teams$county, nation_status := &amp;quot;nation&amp;quot;] %&amp;gt;%
  #order by percentage chance of winning the WC
  .[, nation := factor(nation, levels = nation[order(-.$perc_chance)])]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And can then plot the results…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot the results
p &amp;lt;- ggplot(data = simulation_results, aes(x = nation, y = perc_chance)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, aes(fill = nation_status)) +
  geom_errorbar(aes(ymax = perc_chance + perc_var, ymin = perc_chance - perc_var)) +
  scale_fill_manual(values = c(&amp;quot;darkred&amp;quot;, &amp;quot;darkblue&amp;quot;), name = &amp;quot;Nation Status&amp;quot;) +
  xlab(&amp;quot;Team&amp;quot;) +
  ylab(&amp;quot;World Cup Win Percentage Chance&amp;quot;) +
  ggtitle(&amp;quot;Percetange Chance of Winning the World Cup from 10000 Simulations&amp;quot;,
          subtitle = &amp;quot;Historic UK Counties Substituted in for England&amp;quot;) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1.2))

p2 &amp;lt;- ggplot(data = filter(simulation_results, nation_status == &amp;quot;county&amp;quot;), aes(x = nation, y = perc_chance)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, aes(fill = nation_status)) +
  geom_errorbar(aes(ymax = perc_chance + perc_var, ymin = perc_chance - perc_var)) +
  scale_fill_manual(values = c(&amp;quot;darkred&amp;quot;, &amp;quot;darkblue&amp;quot;), name = &amp;quot;Nation Status&amp;quot;) +
  xlab(&amp;quot;Team&amp;quot;) +
  ylab(&amp;quot;World Cup Win Percentage Chance&amp;quot;) +
  ggtitle(&amp;quot;Percetange Chance of Winning the World Cup from 10000 Simulations&amp;quot;,
          subtitle = &amp;quot;Historic UK Counties Substituted in for England&amp;quot;) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1.2))

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_5_files/figure-html/plot_results-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(p2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_5_files/figure-html/plot_results-2.png&#34; width=&#34;672&#34; /&gt; The bad news is, the real-life favourites tend to dominate the simulations. Brazil or Germany were predicted to win the tournament in almost half of all the simulations. On the graph, it;s just possible to make out the red bars of Yorkshire and Lancashire, both of which won 41 out of 10000 simulations (a 0.41 per cent chance of winning any random World Cup).&lt;/p&gt;
&lt;p&gt;This seems pretty low – but is comparable to pretty respectable teams like Denmark (0.775 per cent), Senegal (0.217 per cent), and even higher than the Iceland team which knocked england out of Euro2016 (0.339 per cent). It’s way higher than the chances the simulation gives the Russian hosts (0.07 per cent).&lt;/p&gt;
&lt;p&gt;Scaling down to just these pretty hopeless nations/counties really shows how little hope the independent British counties would have at an international tournament. However, the best four counties (Lancashire, Yorkshire, Essex, and Surrey) all have about a 0.2 per cent or higher chance, or 500-1 odds, at winning the 2018 World Cup were they to replace England at the last minute. This is an order of magnitude greater than the 5000-1 odds given to Leicester City at the start of 2015-2016 Premier League season, so there’s always a chance.&lt;/p&gt;
&lt;p&gt;And that’s it! All the code for my article over at &lt;a href=&#34;https://www.citymetric.com/horizons/football-could-independent-yorkshire-win-world-cup-3961&#34;&gt;CityMetric&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
