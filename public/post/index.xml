<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Robert Hickman</title>
    <link>/post/</link>
    <description>Recent content in Posts on Robert Hickman</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Which English County Has Won the Most Points</title>
      <link>/post/counties_league_points/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/counties_league_points/</guid>
      <description>&lt;p&gt;Every so often a question on The Guardian’s &lt;a href=&#34;https://www.theguardian.com/football/series/theknowledge&#34;&gt;The Knowledge&lt;/a&gt; football trivia section piques my interest and is amenable to analysis using R. Previously, I looked at &lt;a href=&#34;http://www.robert-hickman.eu/post/the-knowledge-4th-august-2018/&#34;&gt;club name suffixes and young World Cup winners&lt;/a&gt; last August. This week (give or take), a question posed on twitter caught my attention:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
&lt;a href=&#34;https://twitter.com/TheKnowledge_GU?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@TheKnowledge_GU&lt;/span&gt;&lt;/a&gt; was just chatting to some colleagues in the kitchen at work about why Essex doesn&#39;t have many big football clubs and it got me thinking. If you combined all the points from every league team in the ceremonial counties in England, which county would be on top?
&lt;/p&gt;
— BoxBoron (&lt;span class=&#34;citation&#34;&gt;@Rutland_Walker&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/Rutland_Walker/status/1082641231853899781?ref_src=twsrc%5Etfw&#34;&gt;January 8, 2019&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;To start with as always load the libraries needed to analyse this&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get data
library(engsoccerdata)
library(rvest)
#munging
library(tidyverse)
library(magrittr)
#spatial analysis
library(sf)
library(rgdal)
#for plotting maps
library(ggthemes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The easiest way to get a total of points is using the engsoccerdata:: packages database of every English football match from the top four divisions (this does not include data from the 2017-2018, or 2018-2019 seasons). We can work out the points easily from the goals scored for each team&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load the data
match_data &amp;lt;- engsoccerdata::england %&amp;gt;%
  #select only the necessary columns and melt
  select(season = Season, home, visitor, hgoal, vgoal, tier) %&amp;gt;%
  reshape2::melt(id.vars = c(&amp;quot;season&amp;quot;, &amp;quot;hgoal&amp;quot;, &amp;quot;vgoal&amp;quot;, &amp;quot;tier&amp;quot;),
                 variable.name = &amp;quot;location&amp;quot;,
                 value.name = &amp;quot;team&amp;quot;) %&amp;gt;%
  #will need to match this to location data so some club names need cleaning
  mutate(team_subbed = case_when(
    team == &amp;quot;Yeovil&amp;quot; ~ &amp;quot;Yeovil Town&amp;quot;,
    team == &amp;quot;AFC Bournemouth&amp;quot; ~ &amp;quot;A.F.C. Bournemouth&amp;quot;,
    team == &amp;quot;Halifax Town&amp;quot; ~ &amp;quot;F.C. Halifax Town&amp;quot;,
    team == &amp;quot;Aldershot&amp;quot; ~ &amp;quot;Aldershot Town F.C&amp;quot;,
    team == &amp;quot;Wimbledon&amp;quot; ~ &amp;quot;A.F.C. Wimbledon&amp;quot;,
    team == &amp;quot;AFC Wimbledon&amp;quot; ~ &amp;quot;A.F.C. Wimbledon&amp;quot;,
    team == &amp;quot;Macclesfield&amp;quot; ~ &amp;quot;Macclesfield Town&amp;quot;,
    team == &amp;quot;Rushden &amp;amp; Diamonds&amp;quot; ~ &amp;quot;A.F.C. Rushden &amp;amp; Diamonds&amp;quot;,
    team == &amp;quot;Milton Keynes Dons&amp;quot; ~ &amp;quot;Milton Keynes&amp;quot;,
    team == &amp;quot;Dagenham and Redbridge&amp;quot; ~ &amp;quot;Dagenham &amp;amp; Redbridge&amp;quot;,
    team == &amp;quot;Stevenage Borough&amp;quot; ~ &amp;quot;Stevenage&amp;quot;
  )) %&amp;gt;%
  #if cleaning isnt required, take original
  mutate(team_subbed = ifelse(is.na(team_subbed), team, team_subbed))

#peek at the data
head(match_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   season hgoal vgoal tier location            team     team_subbed
## 1   1888     1     1    1     home Accrington F.C. Accrington F.C.
## 2   1888     0     2    1     home Accrington F.C. Accrington F.C.
## 3   1888     2     3    1     home Accrington F.C. Accrington F.C.
## 4   1888     5     1    1     home Accrington F.C. Accrington F.C.
## 5   1888     6     2    1     home Accrington F.C. Accrington F.C.
## 6   1888     3     1    1     home Accrington F.C. Accrington F.C.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 388k (194k matches) data.frame seems daunting, but actually only results in many fewer unique teams that have played at least one match in the top 4 divisions in England&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(unique(match_data$team_subbed))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 141&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The location of each club can then be found using the wikipedia pages for them/their stadia. This matches 121 of the 141 clubs pretty nicely which is a fairly good percentage all things considered&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the links to each clubs wikipedia page
wiki &amp;lt;- read_html(&amp;quot;https://en.wikipedia.org/wiki/List_of_football_clubs_in_England&amp;quot;) %&amp;gt;%
  html_nodes(&amp;quot;td:nth-child(1)&amp;quot;) %&amp;gt;%
  .[which(grepl(&amp;quot;href&amp;quot;, .))]

#get the names for each club
wiki_clubs &amp;lt;- wiki %&amp;gt;% html_text() %&amp;gt;% gsub(&amp;quot; \\(.*\\)$&amp;quot;, &amp;quot;&amp;quot;, .)

#can match 121/141 right off the bat
(unique(match_data$team_subbed) %in% wiki_clubs) %&amp;gt;%
  which() %&amp;gt;%
  length()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 121&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can find the location of these matching clubs by finding the page for their stadia and then finding the coordinates. It’s a bit of a messy function because I was just jamming stuff together to get data out as best as possible. This takes ~1 minute to run through all 121 teams (for the blog post I actually saved an RDS of the output from this and load it just to save time/server calls)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matching_club_locations &amp;lt;- wiki %&amp;gt;% 
  #take only the matching clubs
  .[which(wiki_clubs %in% unique(match_data$team_subbed))] %&amp;gt;%
  html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;%
  #get the wiki page link
  html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
  paste0(&amp;quot;https://en.wikipedia.org&amp;quot;, .) %&amp;gt;%
  #for each club page find the stadium and its coordinates
  lapply(., function(team) {
    link &amp;lt;- read_html(team) %&amp;gt;%
      html_nodes(&amp;quot;.label a&amp;quot;) %&amp;gt;%
      .[1] %&amp;gt;%
      html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% 
      paste0(&amp;quot;https://en.wikipedia.org&amp;quot;,. )
    coords &amp;lt;- link %&amp;gt;%
      read_html() %&amp;gt;% 
      html_nodes(&amp;quot;#coordinates a&amp;quot;) %&amp;gt;%
      html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
      .[2]
    #if coords not found use NA
    if(is.na(coords)) {
      coord_df &amp;lt;- data.frame(lat = NA,
                             lon = NA)
    } else {
      coords &amp;lt;- coords %&amp;gt;%
        paste0(&amp;quot;https:&amp;quot;, .) %&amp;gt;%
        read_html() %&amp;gt;%
        html_nodes(&amp;quot;.geo&amp;quot;) %&amp;gt;%
        html_text() %&amp;gt;%
        strsplit(., split = &amp;quot;, &amp;quot;)
      coord_df &amp;lt;- data.frame(lat = as.numeric(coords[[1]][1]),
                             lon = as.numeric(coords[[1]][2]))
    }
    return(coord_df)
  })  %&amp;gt;%
  #bind everything together
  do.call(rbind, .) %&amp;gt;%
  #add the club name as a new column
  mutate(team = wiki_clubs[
    which(wiki_clubs %in% unique(match_data$team_subbed))
    ]) %&amp;gt;%
  #filter out missing data
  filter(!is.na(lat) &amp;amp; !is.na(lon))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which gives us the location of 114 of our 141 clubs. Most of the remaining ones are now-defunct clubs (e.g. Middlesbrough Ironopolis, Leeds City etc.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;missing_teams &amp;lt;- unique(match_data$team_subbed)[which(!unique(match_data$team_subbed) %in% matching_club_locations$team)]
missing_teams&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Accrington F.C.&amp;quot;           &amp;quot;Darwen&amp;quot;                   
##  [3] &amp;quot;Burton Swifts&amp;quot;             &amp;quot;Port Vale&amp;quot;                
##  [5] &amp;quot;Middlesbrough Ironopolis&amp;quot;  &amp;quot;Rotherham Town&amp;quot;           
##  [7] &amp;quot;Burton Wanderers&amp;quot;          &amp;quot;Loughborough&amp;quot;             
##  [9] &amp;quot;Blackpool&amp;quot;                 &amp;quot;New Brighton Tower&amp;quot;       
## [11] &amp;quot;Burton United&amp;quot;             &amp;quot;Leeds City&amp;quot;               
## [13] &amp;quot;Rotherham County&amp;quot;          &amp;quot;Bristol Rovers&amp;quot;           
## [15] &amp;quot;Darlington&amp;quot;                &amp;quot;Wigan Borough&amp;quot;            
## [17] &amp;quot;Aberdare Athletic&amp;quot;         &amp;quot;New Brighton&amp;quot;             
## [19] &amp;quot;Thames&amp;quot;                    &amp;quot;Aldershot Town F.C&amp;quot;       
## [21] &amp;quot;Hereford United&amp;quot;           &amp;quot;Scarborough&amp;quot;              
## [23] &amp;quot;Cheltenham&amp;quot;                &amp;quot;A.F.C. Rushden &amp;amp; Diamonds&amp;quot;
## [25] &amp;quot;Accrington&amp;quot;                &amp;quot;Crawley Town&amp;quot;             
## [27] &amp;quot;Fleetwood Town&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given it was a Saturday morning where I had nothing better to do, I simply located these clubs home grounds manually and created a data.frame for their locations. It’s not really great practice but whatever.&lt;/p&gt;
&lt;p&gt;These are then all bound together and converted to an sf spatial object with the correct projection&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#add in the missing locations
missing_locations &amp;lt;- data.frame(
  lat = c(53.7646, 53.711772, 52.799, 53.049722, 54.5641, 53.42644, 52.8146,
          52.7743, 53.804722, 53.4359, 52.799, 53.7778, 53.428367, 51.48622,
          54.508425, 53.554914, 51.7127, 53.4292, 51.514431, 51.248386,
          52.060719, 54.265478, 51.906158, 52.328033, 53.7646, 51.405083, 53.9165),
  lon = c(-2.358, -2.477292, -1.6354, -2.1925, -1.2456, -1.34377, -1.6335, -1.1992,
          -3.048056, -3.0377, -1.6354, -1.5722, -1.370231, -2.583134, -1.534394,
          -2.650661, -3.4374, -3.0407, 0.034739, -0.754789, -2.717711, -0.418247,
          -2.060211, -0.5999, -2.358, -0.281944, -3.0247),
    team = as.character(missing_teams)
)

#bind together and convert to sf
all_locations &amp;lt;- rbind(matching_club_locations,
                       missing_locations) %&amp;gt;%
  st_as_sf(coords = c(&amp;quot;lon&amp;quot;, &amp;quot;lat&amp;quot;), crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;)) %T&amp;gt;%
  #make a quick plot of locations for sanity check
  plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-21-The_Knowledge_2_files/figure-html/bind_missing_locations-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that we have all the teams, we need the English historical county boundaries to group them by. I’d actually already used these for football analysis, looknig at &lt;a href=&#34;https://www.citymetric.com/horizons/football-could-independent-yorkshire-win-world-cup-3961&#34;&gt;if an independent Yorkshire could win the World Cup&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Load the data (the boundary file can be download from the &lt;a href=&#34;https://www.ordnancesurvey.co.uk/business-and-government/products/boundaryline.html&#34;&gt;Ordnance Survey&lt;/a&gt;) and make a quick plot of the boundaries and teams&lt;/p&gt;
&lt;p&gt;(I also created an sf object engwal which is just the counties from England and Wales selected out for background plotting)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile 
## Source: &amp;quot;C:\Users\Alaa\Desktop\geo_data\boundary\Data\Supplementary_Ceremonial&amp;quot;, layer: &amp;quot;Boundary-line-ceremonial-counties_region&amp;quot;
## with 91 features
## It has 2 fields&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load the boundary file
counties &amp;lt;- readOGR(dsn = &amp;quot;path/to/file&amp;quot;,
                    layer = &amp;quot;county_boundaries&amp;quot;) %&amp;gt;%
  #convert to sf and project as northing/easting
  st_as_sf(., crs = st_crs(&amp;quot;+init=epsg:27700&amp;quot;)) %&amp;gt;%
  #only interested in the county name
  select(county = NAME) %&amp;gt;%
  #transform the projection to match that of the club locations
  st_transform(., crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;))

engwal &amp;lt;- counties %&amp;gt;%
  .[c(1:54, 88, 90),]

#make a quick plot of counties and teams
ggplot() +
  geom_sf(data = counties, fill = NA) +
  geom_sf(data = all_locations) +
  ggtitle(&amp;quot;Location of All Teams to have Played in Top 4 English Football Divisions&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(by the way the artifacts around Bristol and the Wirral are from the OS dataset- it’s very annoying)&lt;/p&gt;
&lt;p&gt;Then we need to determine which teams are within which counties. The easiest way to do this is to use a spatial join of the team names in all_locations by which county they fall into (using st_contains from the sf package)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#bind the team names to each county
counties %&amp;lt;&amp;gt;%
  st_join(., all_locations, join = st_contains) %&amp;gt;%
  #remove counties that contain zero teams
  filter(!is.na(team)) %&amp;gt;%
  mutate(county = as.character(county))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## although coordinates are longitude/latitude, st_contains assumes that they are planar&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#quick plot of number of teams per county (missing = 0)
counties %&amp;gt;%
  group_by(county) %&amp;gt;%
  summarise(n_clubs = n()) %&amp;gt;%
  ggplot(data = .) +
  geom_sf(data = engwal) +
  geom_sf(aes(fill = n_clubs), colour = &amp;quot;black&amp;quot;) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;, name = &amp;quot;# clubs&amp;quot;) +
  ggtitle(&amp;quot;Number of Top 4 Division Playing Teams in each Ceremonial County&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-21-The_Knowledge_2_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which shows that most English historic counties (and a few Welsh ones due to teams like Cardiff City/ Swansea City etc.) have at least 1 team that has competed in the top 4 flights of English football at some point (those that do not are: Isle of Wight, Rutland, Surrey, Warwickshire, West Sussex and Cornwall).&lt;/p&gt;
&lt;p&gt;To finally get the total number of points won by these teams, the county data needs to be joined back onto the match data from the top. First I clean it up a bit then make the left_join by team name. Finally the number of points per match is calculated using case_when and points are grouped by county and summed&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county_match_data &amp;lt;- match_data %&amp;gt;% 
  mutate(team = team_subbed) %&amp;gt;%
  select(-team_subbed) %&amp;gt;%
  left_join(., counties, by = &amp;quot;team&amp;quot;) %&amp;gt;%
  mutate(points = case_when(
    location == &amp;quot;home&amp;quot; &amp;amp; hgoal &amp;gt; vgoal ~ 3,
    location == &amp;quot;visitor&amp;quot; &amp;amp; vgoal &amp;gt; hgoal ~ 3,
    location == &amp;quot;home&amp;quot; &amp;amp; hgoal &amp;lt; vgoal ~ 0,
    location == &amp;quot;visitor&amp;quot; &amp;amp; vgoal &amp;lt; hgoal ~ 0,
    hgoal == vgoal ~ 1
  ))

county_points &amp;lt;- county_match_data %&amp;gt;%
  group_by(county) %&amp;gt;%
  summarise(total_points = sum(points))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perhaps unsurprisingly, the county with the most points is Greater London, with Greater Manchester following and other footballing hotspots/ large counties in the West Midlands, Lancashire and around Yorkshire in the trailing group&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(arrange(county_points, -total_points))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   county             total_points
##   &amp;lt;chr&amp;gt;                     &amp;lt;dbl&amp;gt;
## 1 Greater London            67189
## 2 Greater Manchester        47203
## 3 West Midlands             37413
## 4 Lancashire                30808
## 5 South Yorkshire           30061
## 6 West Yorkshire            24947&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By contrast, Worcestshire and Northumberland barely have any points, with a few Welsh counties also struggling&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(arrange(county_points, total_points))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   county          total_points
##   &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;
## 1 Worcestershire           275
## 2 Northumberland           398
## 3 Mid Glamorgan            744
## 4 Somerset                 813
## 5 Gloucestershire          994
## 6 Herefordshire           1739&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we group by tier as well as county, it’s possible to see how well each county has done at specific tiers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county_match_data %&amp;gt;%
  group_by(county, tier) %&amp;gt;%
  summarise(total_points = sum(points)) %&amp;gt;%
  left_join(.,
            select(counties, county),
            by = &amp;quot;county&amp;quot;) %&amp;gt;%
  ggplot(data = .) +
  geom_sf(data = engwal) +
  geom_sf(aes(fill = total_points), colour = &amp;quot;black&amp;quot;) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;, name = &amp;quot;total points&amp;quot;) +
  ggtitle(&amp;quot;Number of Points Won by each Team in each Tier of English Football by Ceremonial County&amp;quot;) +
  facet_wrap(~tier) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-21-The_Knowledge_2_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And for the Premier League era this clears up to&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county_match_data %&amp;gt;%
  filter(season &amp;gt; 1991) %&amp;gt;%
  group_by(county, tier) %&amp;gt;%
  summarise(total_points = sum(points)) %&amp;gt;%
  left_join(.,
            select(counties, county),
            by = &amp;quot;county&amp;quot;) %&amp;gt;%
  ggplot(data = .) +
  geom_sf(data = engwal) +
  geom_sf(aes(fill = total_points), colour = &amp;quot;black&amp;quot;, name = &amp;quot;total points&amp;quot;) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;) +
  ggtitle(&amp;quot;Number of Points Won by each Team in each Tier of English Football by Ceremonial County&amp;quot;,
          subtitle = &amp;quot;From Begining of 1992/1993 Season&amp;quot;) +
  facet_wrap(~tier) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Ignoring unknown parameters: name&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-21-The_Knowledge_2_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which shows just how dominant London has been in the top division of English football (especially as it is only competitive at lower levels).&lt;/p&gt;
&lt;p&gt;I had wanted to weight points by the average ELO of that league and see which county has the most weight-adjusted points but got bored for this small blog post.&lt;/p&gt;
&lt;p&gt;Best,&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting the 2018-19 Women&#39;s Super League Using xG and Dixon-Coles</title>
      <link>/post/wsl-prediction-1/</link>
      <pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/wsl-prediction-1/</guid>
      <description>&lt;p&gt;Over the last few years since I started coding I’d always been interested in how data science could help predict football results/ identify footballing talents, and just generally ‘solve’ football.&lt;/p&gt;
&lt;p&gt;One of the major problems with analysing football had been the availability of data. Though there’s a lot of great published stuff freely available to read, a lot of the cutting edge work revolves around &lt;a href=&#34;https://www.optasports.com/services/analytics/advanced-metrics/&#34;&gt;advanced metrics&lt;/a&gt;, such as expected goals, which it’s hard to get the data for.&lt;/p&gt;
&lt;p&gt;Over the summer StatsBomb committed to sharing &lt;a href=&#34;https://statsbomb.com/resource-centre/&#34;&gt;free data&lt;/a&gt; on (amongst others) the &lt;a href=&#34;http://www.fawsl.com/index.html&#34;&gt;Women’s Super League&lt;/a&gt; (the top women’s competition in England), and I’d been interested in looking into this since then.&lt;/p&gt;
&lt;p&gt;This post is basically just a reproduction of two blogs by the excellent &lt;a href=&#34;https://twitter.com/torvaney?lang=en&#34;&gt;Ben Torvaney&lt;/a&gt;, using the &lt;a href=&#34;https://dashee87.github.io/football/python/predicting-football-results-with-statistical-modelling-dixon-coles-and-time-weighting/&#34;&gt;Dixon-Coles method&lt;/a&gt; to predict the the final positions of teams at the end of a football season.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;http://www.statsandsnakeoil.com/2018/06/22/dixon-coles-and-xg-together-at-last/&#34;&gt;first of these&lt;/a&gt; published over the summer combines this method with xG data and the &lt;a href=&#34;http://www.statsandsnakeoil.com/2019/01/01/predicting-the-premier-league-with-dixon-coles/&#34;&gt;second&lt;/a&gt; from this week is a simple and fully reproducible tutorial on implementing Dixon-Coles.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(as both of these posts are pretty comprehensive I’m going to be sparse with commenting/explaining for this post- any questions will probably be answered by the above two articles)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;First, loading the libraries needed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

#Ben Torvaney&amp;#39;s soccer analysis packages
#devtools::install_github(&amp;quot;torvaney/footballdatr&amp;quot;)
library(footballdatr)
#devtools::install_github(&amp;quot;torvaney/regista&amp;quot;)
library(regista)

#women&amp;#39;s football data
#devtools::install_github(statsbomb/StatsBombR)
library(StatsBombR)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can use the StatsBombR package to download the data we need. First we grab a tibble of every match so far in the WSL season, and then use this to get another tibble of every shot from every game which we bind to the original as a new column.&lt;/p&gt;
&lt;p&gt;At the end, we save this, so we don’t have to bombard the API every time we want to rerun the script&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#only want to run this once to avoid overloading the API
#save data after pulling and set chunk eval = FALSE

#get free match info from the StatsBombR package
wsl_matches &amp;lt;- StatsBombR::FreeCompetitions() %&amp;gt;%
  #only interested in WSL
  filter(competition_name == &amp;quot;FA Women&amp;#39;s Super League&amp;quot;) %&amp;gt;%
  #find free matches from WSL
  #(all matches played so far)
  select(competition_id) %&amp;gt;%
  StatsBombR::FreeMatches(.) %&amp;gt;%
  #only want info that helps us predict scores
  select(match_id,
         competition.competition_id,
         season.season_id,
         home = home_team.home_team_name,
         away = away_team.away_team_name,
         hgoals = home_score,
         agoals = away_score)

#get the shot information from each match and bind a tibble as a column
wsl_matches$shots &amp;lt;- wsl_matches %&amp;gt;%
  #split match info into separate rows
  split(f = 1:nrow(.)) %&amp;gt;%
  #get the shots per game
  lapply(., function(x){
    StatsBombR::get.matchFree(x) %&amp;gt;%
      select(team = possession_team.name,
             xG = shot.statsbomb_xg) %&amp;gt;%
      filter(!is.na(xG)) %&amp;gt;%
      #join home/away information
      left_join(x %&amp;gt;% 
                  select(home, away) %&amp;gt;%
                  gather(location, team),
                ., by = &amp;quot;team&amp;quot;) %&amp;gt;%
      select(-team)
  })

#save data
#ONLY WANT TO RUN THIS CHUNK ONCE
saveRDS(wsl_matches, &amp;quot;saved_wsl_matches.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For every rerun we want to load this data again. I also got rid of the LFC/WFC suffixes from each team as it’s a bit redundant and changed the team name columns to factors, which will be required when modelling later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load saved data
wsl_matches &amp;lt;- readRDS(&amp;quot;./saved_wsl_matches.rds&amp;quot;) %&amp;gt;%
  mutate(home = gsub(&amp;quot; WFC| LFC&amp;quot;, &amp;quot;&amp;quot;, home),
         away = gsub(&amp;quot; WFC| LFC&amp;quot;, &amp;quot;&amp;quot;, away)) %&amp;gt;%
  mutate(home = factor(home), away = factor(away))

#peek at what data we have
head(wsl_matches)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
##   match_id competition.com~ season.season_id home  away  hgoals agoals
##      &amp;lt;int&amp;gt;            &amp;lt;int&amp;gt;            &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;  &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;
## 1    19751               37                4 West~ Chel~      0      2
## 2    19727               37                4 Read~ Birm~      0      1
## 3    19719               37                4 West~ Read~      0      0
## 4    19731               37                4 West~ Yeov~      2      1
## 5    19730               37                4 Chel~ Brig~      2      0
## 6    19733               37                4 Birm~ Manc~      2      3
## # ... with 1 more variable: shots &amp;lt;list&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of these function comes pretty much verbatim from the &lt;a href=&#34;http://www.statsandsnakeoil.com/2019/01/01/predicting-the-premier-league-with-dixon-coles/&#34;&gt;first blog post by Ben Torvaney&lt;/a&gt;. We run through the shot data (and the expected goals for every shot over every match), and find the probability of every plausible score from these matches happening.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#http://www.statsandsnakeoil.com/2019/01/01/predicting-the-premier-league-with-dixon-coles/
add_if_missing &amp;lt;- function(data, col, fill = 0.0) {
  # Add column if not found in a dataframe
  # We need this in cases where a team has 0 shots (!)
  if (!(col %in% colnames(data))) {
    data[, col] &amp;lt;- fill
  }
  data
}

team_goal_probs &amp;lt;- function(xgs, side) {
  # Find P(Goals=G) from a set of xGs by the
  # poisson-binomial distribution
  # Use tidyeval to prefix column names with
  # the team&amp;#39;s side (&amp;quot;h&amp;quot;ome or &amp;quot;a&amp;quot;way)
  tibble(!!str_c(side, &amp;quot;goals&amp;quot;) := 0:length(xgs),
         !!str_c(side, &amp;quot;prob&amp;quot;)  := poisbinom::dpoisbinom(0:length(xgs), xgs))
}

simulate_game &amp;lt;- function(shot_xgs) {
  shot_xgs %&amp;gt;%
    split(.$location) %&amp;gt;%
    imap(~ team_goal_probs(.x$xG, .y)) %&amp;gt;%
    reduce(crossing) %&amp;gt;%
    # If there are no shots, give that team a 1.0 chance of scoring 0 goals
    add_if_missing(&amp;quot;homegoals&amp;quot;, 0) %&amp;gt;%
    add_if_missing(&amp;quot;homeprob&amp;quot;, 1) %&amp;gt;%
    add_if_missing(&amp;quot;awaygoals&amp;quot;, 0) %&amp;gt;%
    add_if_missing(&amp;quot;awayprob&amp;quot;, 1) %&amp;gt;%
    mutate(prob = homeprob * awayprob) %&amp;gt;%
    select(homegoals, awaygoals, prob)
}

simulated_games &amp;lt;- wsl_matches %&amp;gt;%
  mutate(simulated_probabilities = map(shots, simulate_game)) %&amp;gt;%
  select(match_id, home, away, simulated_probabilities) %&amp;gt;%
  unnest() %&amp;gt;%
  filter(prob &amp;gt; 0.001) %&amp;gt;%  # Keep the number of rows vaguely reasonable
  rename(hgoals = homegoals, agoals = awaygoals)

simulated_games&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,291 x 6
##    match_id home            away    hgoals agoals    prob
##       &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;           &amp;lt;fct&amp;gt;    &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
##  1    19751 West Ham United Chelsea      0      0 0.0869 
##  2    19751 West Ham United Chelsea      1      0 0.0420 
##  3    19751 West Ham United Chelsea      2      0 0.00755
##  4    19751 West Ham United Chelsea      0      1 0.212  
##  5    19751 West Ham United Chelsea      1      1 0.103  
##  6    19751 West Ham United Chelsea      2      1 0.0184 
##  7    19751 West Ham United Chelsea      3      1 0.00161
##  8    19751 West Ham United Chelsea      0      2 0.201  
##  9    19751 West Ham United Chelsea      1      2 0.0969 
## 10    19751 West Ham United Chelsea      2      2 0.0174 
## # ... with 1,281 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use this data to get estimates of each team’s offensive and defensive strengths using the Dixon-Coles method. From here on out I’m going to refer to “actual goals” as “accomplished” (the number of goals that in reality occurred either for or against a team so far this season) and use “expected goals” for xG.&lt;/p&gt;
&lt;p&gt;In the final tibble, “off” refers to the offensive strength (higher = good) and “def” the “defensive weakness” (i.e. higher is worse).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#ag_model
ag_model &amp;lt;- dixoncoles(
  hgoal = hgoals,
  agoal = agoals,
  hteam = home,
  ateam = away,
  data  = factor_teams(wsl_matches, c(&amp;quot;home&amp;quot;, &amp;quot;away&amp;quot;))
)

#xg_model
xg_model &amp;lt;- dixoncoles(
  hgoal   = hgoals,
  agoal   = agoals,
  hteam   = home,
  ateam   = away,
  weights = prob,
  data    = factor_teams(simulated_games, c(&amp;quot;home&amp;quot;, &amp;quot;away&amp;quot;))
)

#join these together
estimates &amp;lt;-
  inner_join(
    broom::tidy(ag_model),
    broom::tidy(xg_model),
    by = c(&amp;quot;parameter&amp;quot;, &amp;quot;team&amp;quot;),
    suffix = c(&amp;quot;_accomplished&amp;quot;, &amp;quot;_xg&amp;quot;)
  ) %&amp;gt;%
  mutate(value_accomplished = exp(value_accomplished),
         value_xg      = exp(value_xg))

# Preview results, ordered by the biggest difference
estimates %&amp;gt;%
  arrange(desc(abs(value_xg - value_accomplished))) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   parameter team                   value_accomplished value_xg
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;                               &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 def       Brighton &amp;amp; Hove Albion              2.65     1.57 
## 2 off       Arsenal                             2.75     2.03 
## 3 def       Yeovil Town                         3.10     2.41 
## 4 off       Chelsea                             0.788    1.42 
## 5 def       Liverpool                           1.73     1.23 
## 6 rho       &amp;lt;NA&amp;gt;                                1.48     0.996&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s quickly plot the expected and accomplished goals based strength for each team and facet by offense and defence:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot the Dixon-Coles strengths
estimates %&amp;gt;%
  arrange(desc(abs(value_xg - value_accomplished))) %&amp;gt;%
  filter(parameter %in% c(&amp;quot;def&amp;quot;, &amp;quot;off&amp;quot;)) %&amp;gt;%
  ggplot(aes(x = value_xg, y = value_accomplished)) +
  geom_abline(slope = 1, intercept = 0, linetype = &amp;quot;dotted&amp;quot;) +
  geom_text(aes(label = team), alpha = 0.7) +
  labs(title = &amp;quot;Dixon-Coles parameter estimates for WSL teams&amp;quot;,
       subtitle = &amp;quot;Based on ...&amp;quot;,
       x = &amp;quot;... expected goals&amp;quot;,
       y = &amp;quot;... accomplished goals&amp;quot;) +
  coord_equal(ratio=1) +
  theme_minimal() +
  facet_wrap(~parameter)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-04-wsl1_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt; So Arsenal who have been &lt;a href=&#34;https://www.arsenal.com/results?field_arsenal_team_target_id=5&#34;&gt;outrageously good&lt;/a&gt; so far this year have been overperforming in offense (accomplished goals-based Dixon-Coles assume they are better than they probably are), whereas relegation-candidate Yeovil have been hugely underperforming in defense.&lt;/p&gt;
&lt;p&gt;The next step is then to get a list of all unplayed fixtures and predict the probabilities of each score in these using the same models as above (re-stated in the chunk).&lt;/p&gt;
&lt;p&gt;From here we’re using code from the &lt;a href=&#34;http://www.statsandsnakeoil.com/2019/01/01/predicting-the-premier-league-with-dixon-coles/&#34;&gt;second blog post&lt;/a&gt; by Ben Torvaney.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get list of unplayed fixtures
unplayed_games &amp;lt;-
  crossing(home = wsl_matches$home,
           away = wsl_matches$home) %&amp;gt;%
  filter(home != away) %&amp;gt;%
  anti_join(wsl_matches, by = c(&amp;quot;home&amp;quot;, &amp;quot;away&amp;quot;))

#don&amp;#39;t need to reinitialise the models really 
#maybe makes it a bit easier to follow
xg_model &amp;lt;- dixoncoles(hgoals, agoals, home, away, data = simulated_games)
ag_model &amp;lt;- dixoncoles(hgoals, agoals, home, away, data = wsl_matches)

#predict future scorelines using these models
unplayed_xg_scorelines &amp;lt;-
  broom::augment(xg_model, unplayed_games, type.predict = &amp;quot;scorelines&amp;quot;) %&amp;gt;%
  unnest() %&amp;gt;%
  mutate(model = &amp;quot;expected goals&amp;quot;)

unplayed_ag_scorelines &amp;lt;-
  broom::augment(ag_model, unplayed_games, type.predict = &amp;quot;scorelines&amp;quot;) %&amp;gt;%
  unnest() %&amp;gt;%
  mutate(model = &amp;quot;accomplished goals&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bind the data from these predictions to the data from the played games in one big tibble&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;played_scorelines &amp;lt;-
  wsl_matches %&amp;gt;%
  select(home, away, hgoal = hgoals, agoal = agoals) %&amp;gt;%
  mutate(prob = 1.0)

scorelines &amp;lt;- bind_rows(
  mutate(played_scorelines, model = &amp;quot;expected goals&amp;quot;),
  mutate(played_scorelines, model = &amp;quot;accomplished goals&amp;quot;),
  unplayed_xg_scorelines,
  unplayed_ag_scorelines
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These functions again are pretty much verbatim copied from Ben’s work. They simulate the season using Monte-Carlo sampling and build a final predicted WSL table&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simulate_season &amp;lt;- function(scoreline_probabilities) {
  scoreline_probabilities %&amp;gt;%
    nest(hgoal, agoal, prob, .key = &amp;quot;scorelines&amp;quot;) %&amp;gt;%
    mutate(sampled = map(scorelines, ~ sample_n(., 1, weight = prob))) %&amp;gt;%
    select(-scorelines) %&amp;gt;%
    unnest()
}

calculate_table &amp;lt;- function(games) {
  games_augmented &amp;lt;-
    games %&amp;gt;%
    mutate(
      hpoints = case_when(
        hgoal &amp;gt; agoal  ~ 3,
        hgoal == agoal ~ 1,
        agoal &amp;gt; hgoal  ~ 0
      ),
      apoints = case_when(
        hgoal &amp;gt; agoal  ~ 0,
        hgoal == agoal ~ 1,
        agoal &amp;gt; hgoal  ~ 3
      )
    )

  games_home &amp;lt;-
    games_augmented %&amp;gt;%
    select(
      team   = home,
      gf     = hgoal,
      ga     = agoal,
      points = hpoints
    )

  games_away &amp;lt;-
    games_augmented %&amp;gt;%
    select(
      team   = away,
      gf     = agoal,
      ga     = hgoal,
      points = apoints
    )

  bind_rows(games_home, games_away) %&amp;gt;%
    group_by(team) %&amp;gt;%
    summarise(w  = sum(gf &amp;gt; ga),
              d  = sum(gf == ga),
              l  = sum(gf &amp;lt; ga),
              gf = sum(gf),
              ga = sum(ga),
              gd = gf - ga,
              points = sum(points)) %&amp;gt;%
    arrange(desc(points), desc(gd), desc(gf)) %&amp;gt;%
    mutate(position = row_number())
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then it’s simply a case of running these function n times. 1000 for each model is a nice balance of predictive power and runtime.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_simulations &amp;lt;- 1000

simulated_tables &amp;lt;- scorelines %&amp;gt;%
  split(.$model) %&amp;gt;%
  lapply(., function(sim_data) {
    rerun(n_simulations, simulate_season(sim_data)) %&amp;gt;%
      map(calculate_table) %&amp;gt;%
      bind_rows(.id = &amp;quot;simulation_id&amp;quot;) %&amp;gt;%
      #add which model we&amp;#39;re using to each sim
      mutate(model = unique(sim_data$model))
  }) %&amp;gt;%
  do.call(rbind, .)

simulated_tables&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 22,000 x 11
##    simulation_id team      w     d     l    gf    ga    gd points position
##  * &amp;lt;chr&amp;gt;         &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;
##  1 1             Arse~    17     0     3    81    14    67     51        1
##  2 1             Manc~    15     4     1    67     9    58     49        2
##  3 1             Chel~    12     7     1    33     5    28     43        3
##  4 1             Birm~    14     1     5    35    15    20     43        4
##  5 1             Read~    10     4     6    30    24     6     34        5
##  6 1             West~     7     2    11    28    42   -14     23        6
##  7 1             Bris~     7     2    11    17    36   -19     23        7
##  8 1             Brig~     5     1    14    15    46   -31     16        8
##  9 1             Ever~     4     3    13    15    43   -28     15        9
## 10 1             Live~     4     2    14    13    36   -23     14       10
## # ... with 21,990 more rows, and 1 more variable: model &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally we can see how many points each team is predicted to achieve at the end of the season for each model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_colour &amp;lt;- &amp;quot;#1a4990&amp;quot;
n_teams &amp;lt;- length(unique(wsl_matches$home))

simulated_tables %&amp;gt;%
  count(team, points, model) %&amp;gt;%
  ggplot(aes(x = points, y = reorder(team, points))) +
  geom_tile(aes(alpha = n / n_simulations),
            fill = plot_colour) +
  scale_alpha_continuous(range = c(0, 1), name = &amp;quot;probability&amp;quot;) +
  labs(title = &amp;quot;Points total probabilities for the Women&amp;#39;s Super League&amp;quot;,
       subtitle = paste(&amp;quot;n =&amp;quot;, n_simulations, &amp;quot;simulations each&amp;quot;),
       x = &amp;quot;final season points&amp;quot;,
       y = NULL) +
  theme_minimal() +
  facet_wrap(~model)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-04-wsl1_files/figure-html/plot_season-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both models show similar results (most likely Arsenal winning and Yeovil relegated). However, at roughly the season halfway point there are some interesting discrepancies. For instance the xG model is more bullish on Chelsea than the accomplished goals one, and more bearish on Bristol.&lt;/p&gt;
&lt;p&gt;There’s a lot of analysis that can be done here (and if I have time will get round to looking at) but for now I’m pretty satisfied with this as just a very slight synthesis of xG Dixon-Coles and Dixon-Coles tutorial.&lt;/p&gt;
&lt;p&gt;As one final addendum we can look at the probabilities each model assumes for the team to be relegated this year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simulated_tables %&amp;gt;%
  group_by(team, model) %&amp;gt;%
  summarise(p_rel = mean(position == 11)) %&amp;gt;%
  filter(p_rel &amp;gt; 0.01) %&amp;gt;%
  arrange(desc(p_rel))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
## # Groups:   team [3]
##   team                   model              p_rel
##   &amp;lt;fct&amp;gt;                  &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;
## 1 Yeovil Town            expected goals     0.898
## 2 Yeovil Town            accomplished goals 0.818
## 3 Brighton &amp;amp; Hove Albion accomplished goals 0.17 
## 4 Brighton &amp;amp; Hove Albion expected goals     0.064
## 5 Everton                expected goals     0.034
## 6 Everton                accomplished goals 0.012&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which shows broad agreement between the two models. Yeovil are in real trouble. The expected goals model might give some relief to Brighton fans though.&lt;/p&gt;
&lt;p&gt;Doing the same for the eventual champion:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simulated_tables %&amp;gt;%
  group_by(team, model) %&amp;gt;%
  summarise(p_rel = mean(position == 1)) %&amp;gt;%
  filter(p_rel &amp;gt; 0.01) %&amp;gt;%
  arrange(desc(p_rel))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 3
## # Groups:   team [2]
##   team            model              p_rel
##   &amp;lt;fct&amp;gt;           &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;
## 1 Arsenal         expected goals     0.891
## 2 Arsenal         accomplished goals 0.801
## 3 Manchester City accomplished goals 0.199
## 4 Manchester City expected goals     0.099&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Chelsea only sneak into the mix for the expected goals model but it doesn’t look likely they’ll win the league. In the accomplished goals model there may be a slight title race (3/4 that Arsenal win, 1/4 that Man City overtake them), but using expected goals, Arsenal should be pretty confident of winning the league this season.&lt;/p&gt;
&lt;p&gt;That’s all for now. Will hopefully do a lot more football analysis in the coming year(s) first to expand on this post then look at other stuff.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TidyTuesday Week One</title>
      <link>/post/tidytuesday-2019-1/</link>
      <pubDate>Wed, 02 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday-2019-1/</guid>
      <description>&lt;p&gt;Given it’s the new year, I decided to try and get back onto more regular posting on this blog (mostly just to build up a portfolio of work).&lt;/p&gt;
&lt;p&gt;A quick way to get something to work with that can be published unpolished is &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#TidyTuesday&lt;/a&gt; on twitter which (as far as I know/can tell) is organised by &lt;a href=&#34;https://twitter.com/thomas_mock&#34;&gt;Thomas Mock&lt;/a&gt; from RStudio.&lt;/p&gt;
&lt;p&gt;This week, the data comes in the form of a massive corpus of every tweet using the #rstats hashtag, curated by rtweet package creator Mike Kearney.&lt;/p&gt;
&lt;p&gt;I’m only going to leave sparse notes as this is just a post from some lunchtime work cleaned up and published after. I probably won’t fully spellcheck it either.&lt;/p&gt;
&lt;p&gt;First, libraries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#for data #tidytuesday data manipulation
library(tidyverse)
#used for clustering later
library(lsa)
library(e1071)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When loading the data, the first thing I decided to look at was the evolution of the hashtags use over time. As far as I can tell, first used in spring 2009 by &lt;a href=&#34;https://twitter.com/gappy3000&#34;&gt;Giuseppe Paleologo&lt;/a&gt;. Since then, it’s grown pretty exponentially.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#data at https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-01
#rstats_data &amp;lt;- readRDS(&amp;quot;../../Downloads/rstats_tweets.rds&amp;quot;)

#quickly plot tweets over time
p &amp;lt;- rstats_data %&amp;gt;%
  select(created_at) %&amp;gt;%
  arrange(created_at) %&amp;gt;%
  mutate(total_tweets = row_number()) %&amp;gt;%
  ggplot(., aes(x = created_at, y = total_tweets)) +
  geom_line() +
  xlab(&amp;quot;Date&amp;quot;) +
  ylab(&amp;quot;Total #rstats Tweets&amp;quot;) +
  ggtitle(&amp;quot;#rstats Tweets Over Time&amp;quot;) +
  theme_minimal()

p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-02-tidytuesday1_files/figure-html/peek_data-1.png&#34; width=&#34;672&#34; /&gt; I decided only to work with the most prolific #rstats tweeters, mostly to save space in plots as the corpus contains over 26k unique persons and 430k tweets&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#filter out people who tweet about rstats &amp;gt;=500 times
rstats_data %&amp;lt;&amp;gt;% 
  group_by(user_id) %&amp;gt;%
  mutate(tweet_count = n()) %&amp;gt;%
  filter(tweet_count &amp;gt; 499) %&amp;gt;%
  ungroup() %&amp;gt;%
  arrange(-tweet_count) %&amp;gt;%
  #also filter out feeds
  filter(!screen_name %in% c(&amp;quot;CRANberriesFeed&amp;quot;, &amp;quot;Rbloggers&amp;quot;, &amp;quot;rweekly_live&amp;quot;, &amp;quot;tidyversetweets&amp;quot;))

#plot the number of tweets per person
p2 &amp;lt;- rstats_data %&amp;gt;%
  ggplot(., aes(x = reorder(user_id, tweet_count))) +
  geom_bar(stat = &amp;quot;count&amp;quot;) +
  ggtitle(&amp;quot;Rstats Tweets By Person&amp;quot;) +
  xlab(&amp;quot;User&amp;quot;) +
  ylab(&amp;quot;Tweets&amp;quot;) +
  theme_minimal() +
  theme(axis.text.x = element_blank())

p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-02-tidytuesday1_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt; Lets see the most prolific tweeters&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#show the most prolific retweeters
rstats_users &amp;lt;- rstats_data %&amp;gt;%
  select(screen_name, tweet_count) %&amp;gt;%
  unique()

head(rstats_users)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   screen_name tweet_count
##   &amp;lt;chr&amp;gt;             &amp;lt;int&amp;gt;
## 1 AndySugs           8216
## 2 dataandme          4113
## 3 gp_pulipaka        3237
## 4 DerFredo           3091
## 5 revodavid          2640
## 6 MangoTheCat        2523&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I had been interested in recreating some analyses from &lt;a href=&#34;https://www.jtimm.net/2018/11/03/twitter-political-ideology-and-the-115-us-senate/&#34; class=&#34;uri&#34;&gt;https://www.jtimm.net/2018/11/03/twitter-political-ideology-and-the-115-us-senate/&lt;/a&gt; recently, and thought this gave a good oppurtunity.&lt;/p&gt;
&lt;p&gt;First I needed the top levels domains of links in #rstats tweets&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#try to find only top level domains for grouping
domain_patterns &amp;lt;- &amp;quot;\\.com.*|\\.org.*|\\.me.*|\\.gl.*|\\.li.*|\\..appspot|\\.blogspot|\\.io.*&amp;quot;
links &amp;lt;- data.frame(url = unlist(rstats_data$urls_url)) %&amp;gt;%
  mutate(domain = gsub(domain_patterns, &amp;quot;&amp;quot;, url)) %&amp;gt;%
  filter(!is.na(domain)) %&amp;gt;%
  group_by(domain) %&amp;gt;%
  mutate(share_count = n()) %&amp;gt;%
  ungroup()

#which are the most tweeted links by the top tweeters
head(links %&amp;gt;% select(-url) %&amp;gt;% unique() %&amp;gt;% arrange(-share_count))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   domain         share_count
##   &amp;lt;chr&amp;gt;                &amp;lt;int&amp;gt;
## 1 goo                   4724
## 2 wp                    4110
## 3 github                3430
## 4 twitter               3201
## 5 cran.r-project        2878
## 6 r-bloggers            2708&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;some of these (e.g. the google/wp/fb/bit.ly) ones seem a bit more to be quick links to pictures and so were removed. I also cut out links to amazon, google, facebook, and youtube, which I was less certain about doing and would probably analyse in a deeper cut.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#remove non-data sciencey links
links %&amp;gt;%
  filter(!grepl(&amp;quot;goo|wp|tweetedtimes|fb|htl|facebook|youtube|amazon|google&amp;quot;, domain)) %&amp;gt;%
  filter(!grepl(&amp;quot;activevoice.us|ift.tt|rviv.ly|bit.ly&amp;quot;, domain)) %&amp;gt;%
  select(-url) %&amp;gt;%
  unique() %&amp;gt;%
  arrange(-share_count) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   domain                   share_count
##   &amp;lt;chr&amp;gt;                          &amp;lt;int&amp;gt;
## 1 github                          3430
## 2 twitter                         3201
## 3 cran.r-project                  2878
## 4 r-bloggers                      2708
## 5 link.rweekly                    2415
## 6 blog.revolutionanalytics        1225&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we need to create a matrix of each domain vs. each user with a value of how many tweets from that user link to that domain.&lt;/p&gt;
&lt;p&gt;I selected 3 users to illustrate the finished matrix (from here on out I’m freely stealing code from the blogpost &lt;a href=&#34;https://www.jtimm.net/2018/11/03/twitter-political-ideology-and-the-115-us-senate/&#34;&gt;linked above&lt;/a&gt;)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find which domains each tweeted link belong to
rstats_domains_shared &amp;lt;- rstats_data %&amp;gt;%
  select(user_id, screen_name, url = urls_url, date = created_at) %&amp;gt;%
  #remove tweets without links
  filter(!is.na(url)) %&amp;gt;%
  #unlist the links
  #can be multiple per tweet
  splitstackshape::listCol_l(., listcol = &amp;quot;url&amp;quot;) %&amp;gt;%
  #merge with domain information
  merge(., unique(select(links, domain, url_ul = url, domain_shares = share_count)), by = &amp;quot;url_ul&amp;quot;) %&amp;gt;%
  #select only domains shared 100 or more times
  filter(domain_shares &amp;gt; 99) %&amp;gt;%
  #remove uninteresting domains
  filter(!grepl(&amp;quot;goo|wp|tweetedtimes|fb|htl|facebook|youtube|amazon|google&amp;quot;, domain)) %&amp;gt;%
  filter(!grepl(&amp;quot;activevoice.us|ift.tt|rviv.ly|bit.ly&amp;quot;, domain)) %&amp;gt;%
  #limit to only frequent tweeters
  filter(screen_name %in% rstats_users$screen_name)

#get a matrix of domains shared vs. users
rstats_shares_by_user &amp;lt;- rstats_domains_shared %&amp;gt;%
  #find the number of times each user tweets a link to a domain
  group_by(screen_name, domain) %&amp;gt;%
  summarize(share_count = n()) %&amp;gt;%
  #filter out those that are untweets
  filter(share_count &amp;gt; 0) %&amp;gt;%
  spread(screen_name, share_count) %&amp;gt;%
  replace(is.na(.), 0)  %&amp;gt;%
  ungroup()

#quickly glance at this
#has many columns so selecting only a few users
users &amp;lt;- c(&amp;quot;hadleywickham&amp;quot;, &amp;quot;drob&amp;quot;, &amp;quot;JennyBryan&amp;quot;)
rstats_shares_by_user %&amp;gt;%
  .[c(1, which(names(rstats_shares_by_user) %in% users))] %&amp;gt;%
  .[1:10,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 4
##    domain                    drob hadleywickham JennyBryan
##    &amp;lt;chr&amp;gt;                    &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
##  1 analyticsvidhya              0             0          0
##  2 andrewgelman                 0             0          0
##  3 arilamstein                  0             0          0
##  4 asbcllc                      0             0          0
##  5 bl.ocks                      0             0          0
##  6 blog.revolutionanalytics     0             5          0
##  7 blog.rstudio                 1           115          6
##  8 cran.r-project               6            12         21
##  9 cran.rstudio                 0             1          1
## 10 datasciencecentral           0             0          0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we use cosine from the lsa package to get a matrix of user-user similarity. This is then crushed down to two dimensions X1 and X2&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the cosine similarity between all users
cosine_rstats &amp;lt;- rstats_shares_by_user %&amp;gt;%
  select(2:ncol(.)) %&amp;gt;%
  data.matrix() %&amp;gt;%
  lsa::cosine(.)

#sort this into two dimensions
#X1 and X2
rstats_clustering &amp;lt;- cmdscale(1-cosine_rstats, eig = TRUE, k = 2)$points %&amp;gt;% 
  data.frame() %&amp;gt;%
  mutate(screen_name = rownames(cosine_rstats)) %&amp;gt;%
  merge(rstats_users, by = &amp;quot;screen_name&amp;quot;)

head(rstats_clustering)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       screen_name         X1         X2 tweet_count
## 1       _ColinFay -0.1192867 -0.2821199         989
## 2        abresler -0.1712703 -0.3224543        1443
## 3     AnalyticsFr -0.3210288  0.4201589        1386
## 4 AnalyticsFrance -0.3210288  0.4201589        1989
## 5 AnalyticsVidhya -0.2969805  0.4152374        1814
## 6        AndySugs  0.1371950  0.2465780        8216&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we plot this we get a nice graph of the top #rstats users qhich fall neatly into two dimensions. The first X1 seems to be ‘social’ vs. ‘professional’. People further to the left are users I recognise off the top of my head for sharing amateur data analyses/package building (e.g. JennyBryan) whereas those on the right seem to be more industrial users (e.g. MangoTheCat).&lt;/p&gt;
&lt;p&gt;The second dimension is a bit harder to gauge but strikes me as sort of software vs. data science divide with more package creators/rstudio employees towards the bottom and people doing analysis of data towards the top (but this is only a gut feeling).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot the users by their cosine similarity and number of tweets
rstats_clustering %&amp;gt;%
  ggplot(aes(X1,X2)) +
  geom_text(aes(label= screen_name, size = tweet_count), alpha = 0.3) +
  scale_size_continuous(range = c(2,5), guide = FALSE) +
  xlab(&amp;quot;Dimension X1&amp;quot;) +
  ylab(&amp;quot;Dimension X2&amp;quot;) +
  ggtitle(&amp;quot;#rstats Tweeters Arranged by Links Shared&amp;quot;,
          subtitle = &amp;quot;position based on cosine similarity between users&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-02-tidytuesday1_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To investigate a bit further I decided to see what each person was sharing. First I used c-means clustering as it’s something else I was working on in a separate project recently to cluster each use based on their cosine similarity (mostly just to have something to order the final plot by).&lt;/p&gt;
&lt;p&gt;I then used geom_tile to show how often each user was sending links from which domains. Roughly, I would say that the ‘industrial’ (green) cluster makes shows a concentration of links to sites such as &lt;a href=&#34;https://www.r-bloggers.com/&#34;&gt;r-bloggers&lt;/a&gt; and &lt;a href=&#34;https://blog.revolutionanalytics.com/&#34;&gt;revolutionanalytics’ blog&lt;/a&gt;, whereas the ‘social data science’ cluster (blue) links much more to twitter itself, github, and CRAN. The red (‘software’) cluster links to these too, but especially much more to the r-project blog in particular.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(22081992)
#use fuzzy c means to find clusters based on cosine similarity
#chose 3 as seems to be 3 clear nodes
c_grouping &amp;lt;- cmeans(select(rstats_clustering, X1, X2) %&amp;gt;% as.matrix(), 3, iter.max = 1000)

#merge this data in
rstats_clustering %&amp;lt;&amp;gt;%
  mutate(cluster = c_grouping$cluster) %&amp;gt;%
  cbind(as.data.frame(c_grouping$membership)) %&amp;gt;%
  mutate(cluster_membership = apply(.[, (ncol(.)-(max(.$cluster)-1)):ncol(.)], 1, max))

#plot a heatmap of links shared vs. cluster grouping
#remember cluster grouping is related to cosine similarity
rstats_shares_by_user %&amp;gt;%
  reshape2::melt(id.vars = &amp;quot;domain&amp;quot;, variable.name = &amp;quot;screen_name&amp;quot;, value.name = &amp;quot;shares&amp;quot;) %&amp;gt;%
  merge(rstats_clustering, by = &amp;quot;screen_name&amp;quot;) %&amp;gt;%
  filter(shares &amp;gt; 0) %&amp;gt;%
  ggplot(., aes(x = domain, y = reorder(screen_name, cluster + cluster_membership))) +
  geom_tile(aes(fill = log(shares), colour = factor(cluster)), size = 0.5) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;, guide = FALSE) +
  scale_colour_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;purple&amp;quot;), guide = FALSE) +
  xlab(&amp;quot;Domain Shared&amp;quot;) +
  ylab(&amp;quot;Screen Name&amp;quot;) +
  ggtitle(&amp;quot;Domains Shared by #rstats Tweeters Coloured by User Cluster&amp;quot;) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-02-tidytuesday1_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, I wanted to recreate the previous cosine similarity graph but with the clusters highlighted just because I think it makes a pretty graph.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#replot our initial plot of cosine similarity with the cluster information
#alpha of screen_name indicates group membership strength
rstats_clustering %&amp;gt;%
  ggplot(aes(X1, X2)) +
  geom_label(aes(label= screen_name, fill = factor(cluster), colour = cluster_membership, size = tweet_count), alpha = 0.3) +
  scale_colour_gradient(high = &amp;quot;black&amp;quot;, low = &amp;quot;white&amp;quot;, guide = FALSE) +
  scale_fill_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;purple&amp;quot;), guide = FALSE) +
  scale_size_continuous(range = c(2,5), guide = FALSE) +
  xlab(&amp;quot;Dimension X1&amp;quot;) +
  ylab(&amp;quot;Dimension X2&amp;quot;) +
  ggtitle(&amp;quot;#rstats Tweeters Grouped by Links Shared&amp;quot;,
          subtitle = &amp;quot;grouping based on cosine similarity between users&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-02-tidytuesday1_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s all for this post. I think I’ll keep on throwing up quick #TidyTuesday posts throughout the year which will be as sparse as this, but hopefully be interesting to one or two people.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Planning a Pub Crawl Using R</title>
      <link>/post/cambridge_pub_crawl/</link>
      <pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/cambridge_pub_crawl/</guid>
      <description>&lt;p&gt;A few weeks ago I went on the first pub crawl I’d been on in years around my city of Cambridge. Around the same time I had also been visiting &lt;a href=&#34;https://www.google.co.uk/maps/@52.2046202,0.1289874,18z&#34;&gt;4 very good pubs within ~200m of each other&lt;/a&gt; tucked away in a quiet neighbourhood of the town. Together, I wondered if it was possible with freely avaiable data to plan an optimal pub crawl around any town/area of the UK, and also, if it would be feasbile to visit every pub within the city in a single day if travelling optimally.&lt;/p&gt;
&lt;p&gt;Once again, I found that the simple features library (sf) for R basically can do all of this pretty simply, with igraph picking up most of the networking slack. In fact, overall it was much much simpler than I thought it would be. In total, I only needed 9 libraries (though granted tidyverse is one).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#data munging
library(tidyverse)
library(magrittr)

#scrape pub data
library(rvest)
library(googleway)

#spatial manipulation
#almost all done in sf
library(sf)
library(rgdal)

#networking the pubs
library(igraph)
library(TSP)
library(geosphere)

rm(list=ls())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the data, I was able to get by using &lt;a href=&#34;https://www.ordnancesurvey.co.uk/opendatadownload/products.html#BDLINE&#34;&gt;Ordnance Survey data&lt;/a&gt; data for the spatial work. I used Boundary Line for the city boundaries (taking the Cambridge Boro Westminster constituency limits as the city boundaries), and OS OpenRoads for all the road work. These are freely avaiable via email using the link above.&lt;/p&gt;
&lt;p&gt;For reproducibility, these are both presented as if saved in path/to/os/data with folders called ./roads and ./boundary containing the extracted files from both of these.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;file_path &amp;lt;- &amp;quot;path/to/os/data&amp;quot;

#load the westminster constituency boundaries
cambridge &amp;lt;- readOGR(dsn = file.path(file_path, &amp;quot;boundary/Data/GB&amp;quot;), 
                     layer = &amp;quot;westminster_const_region&amp;quot;) %&amp;gt;%
  #convert to sf
  st_as_sf() %&amp;gt;%
  #select only the cambridge constituency
  filter(NAME == &amp;quot;Cambridge Boro Const&amp;quot;) %&amp;gt;%
  #get rid of associated data for cleanness
  select()

#load the road link and node data
#uses the uk national grid to partion road files
#https://en.wikivoyage.org/wiki/National_Grid_(Britain)
#cambridge is in grid TL
roads &amp;lt;- readOGR(dsn = file.path(file_path, &amp;quot;roads/data&amp;quot;), 
                     layer = &amp;quot;TL_RoadLink&amp;quot;) %&amp;gt;%
  st_as_sf() %&amp;gt;%
  #transform to the crs of the city boundary
  st_transform(st_crs(cambridge)) %&amp;gt;%
  #take only the roads which cross into the city
  .[unlist(st_intersects(cambridge, .)),]

nodes &amp;lt;- readOGR(dsn = file.path(file_path, &amp;quot;roads/data&amp;quot;), 
                     layer = &amp;quot;TL_RoadNode&amp;quot;)
#converting straight to sf gives an error so munge data manually
nodes &amp;lt;- cbind(nodes@data, nodes@coords) %&amp;gt;%
  st_as_sf(coords = c(&amp;quot;coords.x1&amp;quot;, &amp;quot;coords.x2&amp;quot;),
           crs = st_crs(&amp;quot;+init=epsg:27700&amp;quot;)) %&amp;gt;%
  st_transform(st_crs(cambridge)) %&amp;gt;%
  #take only nodes which are related to the roads we previously selected
  .[which(.$identifier %in% c(as.character(roads$startNode), 
                              as.character(roads$endNode))),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have these we can make a quick plot of the layout of the city&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#quickly plot the roads and nodes data
(p1 &amp;lt;- ggplot(cambridge) +
   geom_sf() +
   geom_sf(data = roads, colour = &amp;quot;black&amp;quot;) +
   geom_sf(data = nodes, colour = &amp;quot;red&amp;quot;, alpha = 0.5, size = 0.5) +
   theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/roads_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next we need point locations for all the pubs in Cambridge. Fortunately &lt;a href=&#34;www.pubsgalore.co.uk&#34;&gt;pubsgalore.co.uk&lt;/a&gt; has us covered with a pretty extensive list. It doesn’t contain the college bars of the University which is a bit of a shame, but is still a pretty good sample of 199 pubs in/around Cambridge.&lt;/p&gt;
&lt;p&gt;We want the name and address of every open pub which this will scrape.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#page with every pub in cambridge
pub_page &amp;lt;- &amp;quot;https://www.pubsgalore.co.uk/towns/cambridge/cambridgeshire/&amp;quot; %&amp;gt;%
  read_html()


open_pubs &amp;lt;- pub_page %&amp;gt;%
  html_nodes(&amp;quot;.pubicons .pubclosed&amp;quot;) %&amp;gt;%
  html_attr(&amp;quot;src&amp;quot;) %&amp;gt;%
  grep(&amp;quot;grey&amp;quot;, .)

pub_info &amp;lt;- pub_page %&amp;gt;%
  html_nodes(&amp;quot;#pagelist a&amp;quot;) %&amp;gt;%
  html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
  .[open_pubs] %&amp;gt;%
  paste0(&amp;quot;https://www.pubsgalore.co.uk&amp;quot;, .) %&amp;gt;%
  lapply(., function(single_pub) {
    pub_page_read &amp;lt;- single_pub %&amp;gt;%
      read_html()
    
    #get the name of the pub
    pub_name &amp;lt;- pub_page_read %&amp;gt;%
      html_nodes(&amp;quot;.pubname&amp;quot;) %&amp;gt;%
      html_text()
    
    #get the address of the pub 
    line1 &amp;lt;- pub_page_read %&amp;gt;% html_nodes(&amp;quot;.address&amp;quot;) %&amp;gt;% html_text()
    line2 &amp;lt;- pub_page_read %&amp;gt;% html_nodes(&amp;quot;.town&amp;quot;) %&amp;gt;% html_text()
    line3 &amp;lt;- pub_page_read %&amp;gt;% html_nodes(&amp;quot;.postcode&amp;quot;) %&amp;gt;% html_text()
    
    pub_address &amp;lt;- paste0(line1, &amp;quot;, &amp;quot;, line2, &amp;quot;, &amp;quot;, line3)
    
    #put together into data.frame
    pub_data &amp;lt;- data.frame(name = pub_name, address = pub_address)
    return(pub_data)
  }) %&amp;gt;%
  #rbind the lapply results
  do.call(rbind, .) %&amp;gt;%
  #remove duplicated pub addresses
  filter(!duplicated(address))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then need to geocode these adresses into coordinates. Because ggmap has been playing up for me, I tend to use the googleway package with a Google API key which you can get for free &lt;a href=&#34;https://developers.google.com/maps/documentation/javascript/get-api-key&#34;&gt;here&lt;/a&gt;. My key isn’t in the code published here for obvious reasons.&lt;/p&gt;
&lt;p&gt;These coordinates are then bound back onto the pub df and we filter out only the pubs which are located within the city limits (103).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pubs &amp;lt;- pub_info$address %&amp;gt;%
  #convert to character
  as.character() %&amp;gt;%
  #find the coords of every pub address using googleway
  lapply(., function(address) {
    #get the coords
    coords &amp;lt;- google_geocode(address, key = key) %&amp;gt;%
      .$results %&amp;gt;%
      .$geometry %&amp;gt;%
      .$location %&amp;gt;%
      #covert to df and add the address back
      data.frame() %&amp;gt;%
      mutate(address = address)
  }) %&amp;gt;%
  #rbind the results
  do.call(rbind, .) %&amp;gt;%
  #merge back in the pub names
  merge(pub_info, by = &amp;quot;address&amp;quot;) %&amp;gt;%
  #convert to sf
  st_as_sf(coords = c(&amp;quot;lng&amp;quot;, &amp;quot;lat&amp;quot;),
           crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;),
           remove = FALSE) %&amp;gt;%
  #convert to the same crs as the city shapefile
  st_transform(crs = st_crs(cambridge)) %&amp;gt;%
  #only take those which fall within the city shapefile
  #the postal district is a little large and extends into Cambridgeshire
  .[unlist(st_contains(cambridge, .)),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we plot these we can see that most are in the very centre of the city, with some sparsely distributed out in Trumpington (south), Cherry Hinton (east), and Arbury (north).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#quickly plot the roads and nodes data
(p2 &amp;lt;- ggplot(cambridge) +
   geom_sf() +
   geom_sf(data = roads, colour = &amp;quot;black&amp;quot;) +
   #pubs in blue
   geom_sf(data = pubs, colour = &amp;quot;blue&amp;quot;, size = 1.5) +
   theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/pubs_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We could just take the nearest roads to each pub and then easily create a node graph using the lookup between the roads and nodes in the OS data. However, I wanted to play around with manipulating the spatial data (this is a learning exercise after all) and so decided to see how ‘accurate’ I could get the distance on the optimal pub crawl path. In reality, the point locations given by google are probably slightly off anyway, but I’m going to ignore that.&lt;/p&gt;
&lt;p&gt;In order to include ‘half-roads’ (i.e. when the pub is halfway down a road you don’t want to walk the full length of the road), I need to first find the nearest point on &lt;em&gt;any&lt;/em&gt; road to each pub. dist2Line from the geosphere package does this nicely, though it does require turning our sf objects back into SpatialDataFrames.&lt;/p&gt;
&lt;p&gt;(this is by far the longest step in the script btw)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#convert to spatial for dist2Line
pubs_spatial &amp;lt;- st_transform(pubs, crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;)) %&amp;gt;%
  as_Spatial()
roads_spatial &amp;lt;- st_transform(roads, crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;)) %&amp;gt;%
  as_Spatial()

#finds the distance to each nearest line and that point
road_distances &amp;lt;- suppressWarnings(dist2Line(pubs_spatial, roads_spatial)) %&amp;gt;%
  as.data.frame() %&amp;gt;%
  #convert to sf
  st_as_sf(coords = c(&amp;quot;lon&amp;quot;, &amp;quot;lat&amp;quot;),
           crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;),
           remove = FALSE) %&amp;gt;%
  st_transform(crs = st_crs(cambridge))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Taking a peek at these reveals the distance of each pub the nearest road, and the ID of that road and the point on the road nearest the pub.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#display the first few of these
head(road_distances)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Simple feature collection with 6 features and 4 fields
## geometry type:  POINT
## dimension:      XY
## bbox:           xmin: 544907.5 ymin: 256223.8 xmax: 548593.7 ymax: 258690.9
## epsg (SRID):    NA
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.999601272 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.1502,0.247,0.8421,-20.4894 +units=m +no_defs
##     distance       lon      lat   ID                  geometry
## 1  5.8371470 0.1422209 52.20374 2696   POINT (546488.8 258331)
## 2  0.1801197 0.1391954 52.19889 2989   POINT (546298 257784.9)
## 3 29.6403166 0.1720762 52.18425 4059 POINT (548593.7 256223.8)
## 4 10.4782939 0.1223197 52.20734 1765 POINT (545117.3 258690.9)
## 5 32.6370002 0.1203945 52.20662 1757 POINT (544988.1 258606.7)
## 6 10.9321895 0.1191027 52.20427 1751 POINT (544907.5 258343.3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then want to break up the roads that these point lie on. To illustrate this, I’ll use the 3rd pub in the dataset, which is The Robin Hood in Cherry Hinton (as the plots look better and make more sense than the first two in my opinion).&lt;/p&gt;
&lt;p&gt;First we take the pub, and all roads and their nodes within 100m of the pub:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#take the first pub
x_pub &amp;lt;- pubs[3,]
#get all roads within 100m and their nodes
x_roads &amp;lt;- roads %&amp;gt;%
  .[unlist(st_intersects(st_buffer(x_pub, 100), .)),]
x_nodes &amp;lt;- nodes %&amp;gt;%
  .[which(.$nodeid %in% c(as.character(x_roads$start), 
                          as.character(x_roads$end))),]

#plot the roads local to this pub
(p3 &amp;lt;- ggplot() +
  geom_sf(data = x_roads) +
  geom_sf(data = x_nodes, colour = &amp;quot;red&amp;quot;, alpha = 0.5) +
  geom_sf(data = x_pub, colour = &amp;quot;blue&amp;quot;) +
  theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/splitting_roads1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We then use the point on any road nearest to this pub (green) as the ‘entrance’ of the pub (this may not strictly be the case and it might be possible to instead match road names to the address, but whatever).&lt;/p&gt;
&lt;p&gt;Using this point, we split up the road it lies on into two new separate roads (in orange and purple). To get to this pub you would have to travel down one of these to the green point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the nearest point that lies on a road
x_nearest_road_point &amp;lt;- road_distances[3,]

#split that road into two over that point
x_split_roads &amp;lt;- roads %&amp;gt;%
  .[which(.$id == x_nearest_road_point$ID),] %&amp;gt;%
  st_difference(., st_buffer(x_nearest_road_point, 0.2)) %&amp;gt;%
    st_cast(&amp;quot;LINESTRING&amp;quot;) 

#add to the plot
(p3 &amp;lt;- p3 + 
    geom_sf(data = x_split_roads[1,], colour = &amp;quot;purple&amp;quot;, size = 1.5, alpha = 0.5) +
    geom_sf(data = x_split_roads[2,], colour = &amp;quot;goldenrod&amp;quot;, size = 1.5, alpha = 0.5) +
    geom_sf(data = x_nearest_road_point, colour = &amp;quot;darkgreen&amp;quot;, size = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/splitting_roads2-1.png&#34; width=&#34;672&#34; /&gt; I could have used the green ‘entrance’ nodes for the travelling salesman part of the problem, but decided also to create roads from this ‘entrance’ to the geocoded location of the pub (blue). This is probably the equivalent of travelling from the pavement to the bar of each pub and worthy of consideration*.&lt;/p&gt;
&lt;p&gt;These roads are created by binding the green and blue points together from each pub, grouping them, and then casting a line between them.&lt;/p&gt;
&lt;p&gt;*another reason to take this into account is that some pubs may appear far away from roads. One example that I visit fairly often is &lt;a href=&#34;https://www.google.co.uk/maps/place/Fort+St+George/@52.2123057,0.1272911,18.75z/data=!4m5!3m4!1s0x47d870ecb4e8556d:0x3bfb0ff82c243075!8m2!3d52.2124367!4d0.1278073&#34;&gt;Fort St George&lt;/a&gt; which is on a river footpath and does not have direct road access.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#combine the green point on the roads with the point for the pub
x_pub_entrance &amp;lt;- select(x_nearest_road_point) %&amp;gt;%
  rbind(., select(x_pub)) %&amp;gt;%
  group_by(&amp;quot;pub road&amp;quot;) %&amp;gt;%
  summarise() %&amp;gt;%
  #cast to a line (for a new road)
  st_cast(&amp;#39;LINESTRING&amp;#39;)

#plot the pub entrance
(p3 &amp;lt;- p3 +
    geom_sf(data = x_pub_entrance, colour = &amp;quot;lightblue&amp;quot;, size = 1.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/splitting_roads3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#remove all the extra objects we created in the example
rm(list=ls()[grep(&amp;quot;x_&amp;quot;, ls())])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First we create all the new nodes (the green pub entrances, and the blue pub locations)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#add information to the pubs df
pubs &amp;lt;- pubs %&amp;gt;%
  mutate(pub = 1:nrow(.), 
         id = (max(nodes$id)+1):(max(nodes$id) + nrow(.)),
         nodeid = NA,
         class = &amp;quot;pubnode&amp;quot;)

#bind the pubs to the nodes data frame
nodes &amp;lt;- rbind(nodes, select(pubs, pub, nodeid, id, class))

#add the nodes found as the nearest road point to each pub to the nodes df
new_nodes &amp;lt;- road_distances %&amp;gt;%
  select() %&amp;gt;%
  mutate(pub = pubs$pub, 
         id = (max(nodes$id)+1):(max(nodes$id)+nrow(.)),
         nodeid = NA,
         class = &amp;quot;entrancenode&amp;quot;)
nodes &amp;lt;- rbind(nodes, new_nodes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we create all of the split roads in a for loop and all of the roads from the green to the blue points. These are bound back into the original roads data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#split up the roads that have a new node for the netrance of a pub
roads_2_split &amp;lt;- roads %&amp;gt;%
  slice(unique(road_distances$ID))
#leave the rest alone
roads &amp;lt;- roads %&amp;gt;%
  slice(-unique(road_distances$ID))

#for each new node split up the road that it bisects it 
#as we did in the example
for(node in seq(nrow(new_nodes))) {
  #find the road that the pub is nearest to
  split_road &amp;lt;- st_intersects(st_buffer(new_nodes[node,], .2),
                              roads_2_split) %&amp;gt;%
    unlist() %&amp;gt;%
    roads_2_split[.,]
  
  #split this road up
  split_roads &amp;lt;- st_difference(split_road, st_buffer(new_nodes[node,], .2)) %&amp;gt;%
    st_cast(&amp;quot;LINESTRING&amp;quot;) %&amp;gt;%
    select(start_id, end_id, id)

  #keep hold of the old id
  old_id &amp;lt;- unique(split_roads$id)
  
  #get rid of this road from the df
  roads_2_split &amp;lt;- roads_2_split %&amp;gt;%
    slice(-which(roads_2_split$id == old_id))

  #get the nodes for the old road
  old_nodes &amp;lt;- filter(nodes, id %in% c(unique(split_roads$start_id),
                                       unique(split_roads$end_id)))
  
  #add the correct nodes to the newly split road
  split_roads$start_id &amp;lt;- old_nodes$id[
      unlist(st_contains(st_buffer(split_roads, .2), old_nodes))
    ]
  split_roads$end_id &amp;lt;- new_nodes$id[node]
  
  #add in new information for the new road
  split_roads %&amp;lt;&amp;gt;%
    mutate(id = max(roads_2_split$id) + seq(nrow(split_roads)),
           class = &amp;quot;split road&amp;quot;,
           start = NA, 
           end = NA)
  #bind back to the original df
  roads_2_split &amp;lt;- rbind(roads_2_split, split_roads)
}

#bind the split roads to the original roads df
roads &amp;lt;- rbind(roads, roads_2_split)

#generate paths from the nearest point on a road to the pub gecoded location
#i.e. walking from the pavement to the bar itself
pub_roads &amp;lt;- select(road_distances) %&amp;gt;%
  #add in information and bind to equivalent pub points
  mutate(name = pubs$name, start_id = pubs$id, end_id = new_nodes$id) %&amp;gt;%
  rbind(., mutate(select(pubs, name, start_id = id), end_id = new_nodes$id)) %&amp;gt;%
  #group each pub together
  group_by(name, start_id, end_id) %&amp;gt;%
  summarise() %&amp;gt;%
  #cast to a line
  st_cast(&amp;#39;LINESTRING&amp;#39;) %&amp;gt;%
  ungroup() %&amp;gt;%
  #munge required information
  mutate(id = max(roads$id)+1 + seq(nrow(.)),
         start = NA,
         end = NA,
         class = &amp;quot;pub road&amp;quot;) %&amp;gt;%
  select(class, id, start_id, end_id, start, end)

#bind these into the original road df
roads &amp;lt;- rbind(roads, pub_roads)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that I have all the pubs and roads I want to traverse, I can move onto the &lt;a href=&#34;https://en.wikipedia.org/wiki/Travelling_salesman_problem&#34;&gt;travelling salesman&lt;/a&gt; portion of the problem- what is the shortest journey between all of them.&lt;/p&gt;
&lt;p&gt;For this, I need to use the igraph package and convert my df of roads (which contains the node at each end of every road) into a weighted node graph. Once I have this, I iterate through every combination of pubs and find the shortest path between the two and the vertices that comprise it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#add in the length between each pair of nodes using st_length
nodes_df &amp;lt;- roads %&amp;gt;%
  mutate(length = as.numeric(st_length(.))) %&amp;gt;%
  #select only the node ids and length between them
  select(start_id, end_id, length)
#get rid of the geometry
st_geometry(nodes_df) = NULL

#create a node graph from this df
#uses graph.data.frame from the igraph package
node_graph &amp;lt;- graph.data.frame(nodes_df, directed=FALSE)

#to get the shortest distance between every pair of pubs
#need to create each combination of pub id number
combinations &amp;lt;- combn(1:nrow(pubs), 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The shortest path between any two pubs is found using igraph::get.shortest.paths() and then extracting the path of nodes. Each vertex of the path is then found by using pairwise combinations of the nodes, and the travelled vertices for each pub-&amp;gt;pub journey are saved into a (large) df.&lt;/p&gt;
&lt;p&gt;The whole thing is pretty quick but obviously the number of combinations grows quickly. For 103 pubs in Cambridge, it takes ~20mins on my machine.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#go through each of these pairs
journeys &amp;lt;- lapply(seq(length(combinations)/2), function(combination) {
  #the two pubs we&amp;#39;ll test
  pub1 &amp;lt;- combinations[1,][combination]
  pub2 &amp;lt;- combinations[2,][combination]
  
  #get the shortest path (node-node) between these two pubs
  travel_nodes &amp;lt;- get.shortest.paths(node_graph,
                                     from = as.character(pubs$id[pub1]),
                                     to = as.character(pubs$id[pub2]),
                                     weights = E(node_graph)$length) %&amp;gt;%
    .$vpath %&amp;gt;%
    unlist() %&amp;gt;%
    names() %&amp;gt;%
    as.numeric()
  
  #find the vertices that connect between these nodes
  connecting_vertices &amp;lt;- lapply(seq(length(travel_nodes)-1), function(node_pair) {
    between_nodes &amp;lt;- travel_nodes[c(node_pair:(node_pair+1))]
    connecting_vertex &amp;lt;- which(roads$start_id %in% between_nodes &amp;amp;
                                 roads$end_id %in% between_nodes)
    #if more than one potential vertex, take the shorter one
    if(length(connecting_vertex) &amp;gt; 1) {
      connecting_vertex &amp;lt;- connecting_vertex[
        which.min(st_length(roads[connecting_vertex,]))
      ]
    }
    
  return(connecting_vertex)
  }) %&amp;gt;%
    unlist() %&amp;gt;%
    roads[.,] %&amp;gt;%
    #id this journey between pubs
    mutate(journey = paste0(pub1, &amp;quot;-&amp;quot;, pub2))
  
  return(connecting_vertices)
}) %&amp;gt;%
  #rbind it all together
  do.call(rbind, .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s possible to check a journey between two pubs easily using this df, and show that it does seem like igraph is finding the shortest route between the two&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get only the journey between two random pubs
set.seed(3459)
x_random_numbers &amp;lt;- sample(nrow(pubs), 2) %&amp;gt;%
  sort()
x_journey &amp;lt;- journeys %&amp;gt;%
  filter(journey == paste0(x_random_numbers[1], &amp;quot;-&amp;quot;, x_random_numbers[2]))

#get the nodes of this journey
x_journey_nodes &amp;lt;- nodes %&amp;gt;%
  filter(id %in% c(as.character(x_journey$start_id),
                   as.character(x_journey$end_id)))

#find the pubs at the start and end of this journey
#random pubs as defined earlier
x_journey_pubs &amp;lt;- pubs[x_random_numbers,]

#get all roads within a kilometer of each pub
x_local_roads &amp;lt;- roads %&amp;gt;%
  .[unlist(st_intersects(
    #select all roads that at least will be between the two pubs
    st_buffer(x_journey_pubs, max(st_distance(x_journey_pubs)/1.5)
  ), .)),]

#plot the shortest path between these two pubs
(p4 &amp;lt;- ggplot() +
    geom_sf(data = x_local_roads, colour = &amp;quot;grey&amp;quot;) +
    geom_sf(data = x_journey_nodes, colour = &amp;quot;black&amp;quot;) +
    geom_sf(data = x_journey, colour = &amp;quot;black&amp;quot;) +
    geom_sf(data = x_journey_pubs, colour = &amp;quot;blue&amp;quot;, alpha = 0.5, size = 3) +
    theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/check_shortest_paths-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#remove all the extra objects we created in the example
rm(list=ls()[grep(&amp;quot;x_&amp;quot;, ls())])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I have to find the shortest combination of these journeys which visits every pub at least once. For this I find the shortest distances between each pub and every other pub and bind it into a matrix.&lt;/p&gt;
&lt;p&gt;If you just wanted to find the distance of a pub crawl (and not the path) you could just skip to here and save a lot of time.&lt;/p&gt;
&lt;p&gt;This matrix of 103x103 is then converted into TSP object using TSP::TSP() and converted to a Hamtiltonian path problem by inserting a dummy variable. The TSP is then solved given the order of nodes (in this case, just pubs) to visit.&lt;/p&gt;
&lt;p&gt;A lot of help in doing this came from the StackOverflow answer &lt;a href=&#34;https://stackoverflow.com/questions/27363653/find-shortest-path-from-x-y-coordinates-with-start-%E2%89%A0-end&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get the distances 
distances &amp;lt;- lapply(seq(nrow(pubs)), function(node) {
  distance &amp;lt;- shortest.paths(node_graph, 
                             v = as.character(pubs$id[node]),
                             to = as.character(pubs$id[seq(nrow(pubs))]), 
                             weights = E(node_graph)$length)
}) %&amp;gt;%
  do.call(rbind, .) %&amp;gt;%
  as.dist() %&amp;gt;%
  TSP()

#insert a dummy city to turn this into a Hamiltonian path question
#i.e. we do not need to return to the start at the end
tsp &amp;lt;- insert_dummy(distances, label = &amp;quot;cut&amp;quot;)

#solve the shortest Hamiltonian tour of each pub in Cambridge
#get the total distance ()
tour &amp;lt;- solve_TSP(tsp, method=&amp;quot;2-opt&amp;quot;, control=list(rep=10))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: executing %dopar% sequentially: no parallel backend registered&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tour&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## object of class &amp;#39;TOUR&amp;#39; 
## result of method &amp;#39;2-opt_rep_10&amp;#39; for 104 cities
## tour length: 41289.44&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the order of pubs to visit in an optimal Hamtilonian path
tour &amp;lt;-  unname(cut_tour(tour, &amp;quot;cut&amp;quot;))
tour&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1]  38  76  70  14   8  49  86  32  75  63  10   9   2  41   1  27  40
##  [18]  54  20  82  90  80  72   4  61  98  83  77   5  89  47   6  60  88
##  [35]  59  46  57  11 101  78  84   7  36  65  44  71  21  39 103  18 102
##  [52]  93  19  85  62  23  16  22  29 100  42  51  50  48  64  56  79  92
##  [69]  30  28  87  96  55  35  67  69  68  91  13  17  52  94  12  97  58
##  [86]  26  73  66  25  24  31  81  37  33  53  15  74  45  43  99  34   3
## [103]  95&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So to visit every pub, you’d expect to walk just under 41km, which fits with eye-testing the size of Cambridge (approx 10km diameter).&lt;/p&gt;
&lt;p&gt;In order to plot this, the pub order is converted into strings in the format we’ve used for journeys between pubs (e.g. “1-2”) and each journey is then extracted from the df of shortest journeys between all pubs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#it might be possible that doing some journeys in &amp;#39;reverse&amp;#39; is faster
#when considering the tour of all pubs
#e.g. from pub2 -&amp;gt; pub1 rather than the other way round
rev_journeys &amp;lt;- journeys
rev_journeys$journey &amp;lt;- strsplit(journeys$journey, &amp;quot;-&amp;quot;) %&amp;gt;%
  #reverse the journey tag
  lapply(., rev) %&amp;gt;%
  lapply(., paste, collapse = &amp;quot;-&amp;quot;)

#bind these together in a df of all journeys
journeys &amp;lt;- rbind(journeys, rev_journeys) %&amp;gt;%
  mutate(journey = as.character(journey))

#take the nodes from the shortest tour and arrange them as in
#the journeys tag for each path between two pubs
tour_strings &amp;lt;- paste0(tour, &amp;quot;-&amp;quot;, lead(tour)) %&amp;gt;%
  .[-grep(&amp;quot;NA&amp;quot;, .)] %&amp;gt;%
  data.frame(journey = .,
             journey_order = 1:length(.))

#use this to find each vertex that needs traversing in order to complete
#the shortest tour of all pubs
#subset this from the df of all shortest journeys between any two pubs
shortest_tour &amp;lt;- journeys[which(journeys$journey %in% tour_strings$journey),] %&amp;gt;%
  merge(., tour_strings,  by = &amp;quot;journey&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All that’s left to do is plot this shortest pub crawl&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(p5 &amp;lt;- ggplot(data = shortest_tour) +
  geom_sf(data = cambridge) +
  geom_sf(data = roads, size = 0.5) + 
  geom_sf(aes(colour = journey_order), size = 1.5, alpha = 0.7) +
  scale_colour_gradient(high = &amp;quot;darkred&amp;quot;, low = &amp;quot;orange&amp;quot;, name = &amp;quot;journey order&amp;quot;) +
  geom_sf(data = pubs, colour = &amp;quot;blue&amp;quot;, size = 1.5) +
  theme_minimal() +
  ggtitle(&amp;quot;Hamiltonian Path of Every Pub in the City of Cambridge&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/plot_shortest_tour-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As it turns out, you’d want to start in Trumpington at The Lord Byron Inn and then get into central Cambridge where you dart around a lot before heading out to the north and finally the east at The Red Lion in Cherry Hinton.&lt;/p&gt;
&lt;p&gt;A bigger image of the above can be found at imgure &lt;a href=&#34;https://i.imgur.com/r18SG2B.png&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All together this script calculates a distances of ~41km (give or take probably quite a bit) to visit every pub, which is actually kind of doable in a single day (if you forgo [at least some of] the drinking).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How I Learned To Stop Worrying and Love Heatmaps</title>
      <link>/post/getis-ord-heatmaps-tutorial/</link>
      <pubDate>Thu, 04 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/getis-ord-heatmaps-tutorial/</guid>
      <description>&lt;p&gt;Whilst &lt;del&gt;getting some work done&lt;/del&gt; browsing twitter at work today, I came across &lt;a href=&#34;https://twitter.com/jburnmurdoch/status/1047470445459644416&#34;&gt;this tweet&lt;/a&gt; from the always excellent John Burn-Murdoch on the scourge of heatmaps. What’s most frustrating about these maps is that ggplot2 (which is underrated as mapping software, especially when combined with packages like sf in R) makes it super easy to create this bland, uninformative maps.&lt;/p&gt;
&lt;p&gt;For instance, lets load some mapping libraries&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(sf)
library(rgdal)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this blog I’m going to use data of bus stops in London, because there’s an absolute ton of them and because I love the London Datastore and it was the first public, heavy, point data file I came across.&lt;/p&gt;
&lt;p&gt;Let’s grab some data to use as exemplars&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get the shapefile data of london from GADM
#downloads into a temp file
gadm_url &amp;lt;- &amp;quot;https://biogeo.ucdavis.edu/data/gadm3.6/Rsf/gadm36_GBR_2_sf.rds&amp;quot;
temp_dir &amp;lt;- tempdir()
download.file(gadm_url, destfile = file.path(temp_dir, &amp;quot;london_shapefile.rds&amp;quot;), 
              mode = &amp;quot;wb&amp;quot;, quiet = TRUE)
london &amp;lt;- sf::st_as_sf(readRDS(file.path(temp_dir, &amp;quot;london_shapefile.rds&amp;quot;))) %&amp;gt;%
  filter(grepl(&amp;quot;London&amp;quot;, NAME_2))

#get the bus stop data
#https://data.london.gov.uk/dataset/tfl-bus-stop-locations-and-routes
bus &amp;lt;- read.csv(&amp;quot;https://files.datapress.com/london/dataset/tfl-bus-stop-locations-and-routes/bus-stops-10-06-15.csv&amp;quot;,
                stringsAsFactors = FALSE) %&amp;gt;%
  filter(!is.na(Location_Easting)) %&amp;gt;%
  #convert to simple features
  st_as_sf(coords = c(&amp;quot;Location_Easting&amp;quot;, &amp;quot;Location_Northing&amp;quot;), crs = st_crs(&amp;quot;+init=epsg:27700&amp;quot;)) %&amp;gt;%
  #transform projection to match the boundary data
  st_transform(crs = st_crs(london)) %&amp;gt;%
  #remove bus stops outside of the limits of the london shapefile
  .[unlist(st_intersects(london, .)),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the data, let’s have a first pass at plotting the points on a map of London using ggplot2&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot the bus stop locations
p1 &amp;lt;- ggplot(bus) +
  geom_sf(data = london, fill = &amp;quot;grey&amp;quot;) +
  geom_sf(colour = &amp;quot;blue&amp;quot;, alpha = 0.2) +
  ggtitle(&amp;quot;Bus Stops in London&amp;quot;) +
  theme_void()

plot(p1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-04-getis-ord-tutorial_files/figure-html/plot_bus_stops-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The points here do kiiiind of work alone- it’s possible to see the route along which buses travel. However, it’s still pretty heavy and takes some cognitive effort. It’s also worth remembering this is just the first example of data I came across- if working with stuff like common incident data, even a heavy alpha on the points is going to cover the map and leave it unreadable, as in the tweet up top.&lt;/p&gt;
&lt;p&gt;But let’s say we plot it as a heatmap to try and sumamrise where the points are. In my opinion, this is even worse. For one- it stretches utside the boundaries of the shapes, even though we have filtered data to include no bus stops outside London. In this example that’s not a huge deal- but if London was (e.g.) an Island, the graph is now suggesting commutes into the sea. It’s also incredibly taxing to keep track of the various peaks and troughs- even in a simple model where bus stops generally increase as you go near to the centre&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#bind the coordinates as numeric
bus &amp;lt;- bus %&amp;gt;%
  cbind(., st_coordinates(bus))

#plot the bus stops as a density map
p2 &amp;lt;- ggplot() +
  geom_sf(data = london) +
  stat_density_2d(data = bus, aes(X, Y)) +
  ggtitle(&amp;quot;Bus Stops in London&amp;quot;) +
  theme_void()

plot(p2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-04-getis-ord-tutorial_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Instead, we can bin the point data into equi-sized containers. I’m extremely partial to this, even though it’s not super popular. To do this with hexagonal bins (the closest to circular that still tessellates perfectly), we just have to create a grid of points and connect them up&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#merge the london wards into one boundary file
london_union &amp;lt;- london %&amp;gt;%
  group_by(&amp;quot;group&amp;quot;) %&amp;gt;%
  summarise()

#generate a grid of points separated hexagonally
#no way to do this purely in sf yet afaik
hex_points &amp;lt;- spsample(as_Spatial(london_union), type = &amp;quot;hexagonal&amp;quot;, cellsize = 0.01)

#generate hexgaon polygons from these points
hex_polygons &amp;lt;- HexPoints2SpatialPolygons(hex_points) %&amp;gt;%
  st_as_sf(crs = st_crs(london_union)) %&amp;gt;%
  #clip to the london shapefile
  st_intersection(., london_union)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then find out which bin every bus stop is located in using st_intersects&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find how many bus stops are within each hexagon
hex_polygons$planning_no &amp;lt;- lengths(st_intersects(hex_polygons, bus))

#plot the number of bus stops per bin
p2 &amp;lt;- ggplot(hex_polygons) +
  geom_sf(aes(fill = planning_no)) +
  scale_fill_viridis_c(option = &amp;quot;magma&amp;quot;, &amp;quot;# Bus Stops&amp;quot;) +
  theme_void() +
  ggtitle(&amp;quot;Binned London Bus Stops&amp;quot;)

plot(p2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-04-getis-ord-tutorial_files/figure-html/bin_stops-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, this is still pretty confusing. It seems like bus stops are fairly randomly distributed as by chance one hexagon may contain multiple stops at its edges, whereas a neighbour may be juuuust missing out on these.&lt;/p&gt;
&lt;p&gt;To mitigate this effect, we can study the spatial autocorrelation of each hexagon to it’s neighbours. There are multiple ways to do this, but the one I was first introduced to and have used most is the Getis-Ord local statistic. In this example I will include.self() which means we are using the Gi* variant of the statistic.&lt;/p&gt;
&lt;p&gt;Basically- we tell R to find all the nearest neighbours of any bin (hexagon- though not necessarily so, we could e.g. use wards, but I think it looks messier). It then calculates the ratio of values within the bin and it’s neighbours, to the total number of points (bus stops). The reported Gi* value is a z statistic- it can be positive (more clustering) or negative (less) and used to find significant clusters. I’m not going to do any of that here- just accept for now that a high Getis_Ord Gi* value means a greater cluster of bus stops in that region.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(spdep)

#find the centroid of each hexagon and convert to a matrix of points
hex_points &amp;lt;- do.call(rbind, st_geometry(st_centroid(hex_polygons))) %&amp;gt;%
  unlist() %&amp;gt;%
  as.matrix.data.frame()

#use a k-nearest-neighbour algorithm to find which shape neighbour which
#super easy for the hexagons analytically obvs but important for e.g. using the ward boundaries instead
neighbouring_hexes &amp;lt;- knn2nb(knearneigh(hex_points, k = 6), 
                             row.names = rownames(hex_points)) %&amp;gt;%
  include.self()

#calculate the local G for a given variable (#bus stops) using the neihbours found previously
localGvalues &amp;lt;- localG(x = as.numeric(hex_polygons$planning_no),
                       listw = nb2listw(neighbouring_hexes, style = &amp;quot;B&amp;quot;),
                       zero.policy = TRUE)

#bind this back to the sf as a numeric variable column
hex_polygons$smooth_planning_no &amp;lt;- as.numeric(localGvalues)

#plot the statistic
#+ve indicates more than would be expected
p3 &amp;lt;- ggplot(hex_polygons) +
  geom_sf(aes(fill = smooth_planning_no)) +
  scale_fill_viridis_c(option = &amp;quot;magma&amp;quot;, name = &amp;quot;Gi* Statistic&amp;quot;) +
  theme_void() +
  ggtitle(&amp;quot;Getis-Ord Binned London Bus Stops Statistic&amp;quot;)

plot(p3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-04-getis-ord-tutorial_files/figure-html/getis_ord-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;which generates a nice plot showing smoothed autocorrelation of dense public-transportation access. I like how you can clearly see the darker regions for the Lea Valley and Richmond Park, and in contrast the hubs of Kingston and Croydon, but in a way to is much more manageable than the contour map, or the point data itself.&lt;/p&gt;
&lt;p&gt;It’s also worth bearing in mind, that this data is fairly organised (bus routes are to some extent, logically planned). When I first looked into spatial autocorrelation I was dealing with a huge number of dense points randomly dispersed over a significant proportion of England. At that level, techniques such as the finding the Getis-Ord statistic allow you to make sense of the data, AS WELL AS statistically test it. Though I haven’t ever worked with epidemiology data, apparently it’s a powerful technique to find clusters of disease outbreaks, and indeed, the data packaged with spdep is for SIDS data in North Carolina.&lt;/p&gt;
&lt;p&gt;For more on this, I first learned about Getis-Ord from an excellent &lt;a href=&#34;https://pudding.cool/process/regional_smoothing/&#34;&gt;The Pudding&lt;/a&gt; post (from which some of my code is stolen), &lt;a href=&#34;http://pro.arcgis.com/en/pro-app/tool-reference/spatial-statistics/h-how-hot-spot-analysis-getis-ord-gi-spatial-stati.htm&#34;&gt;ARCGIS has&lt;/a&gt; a pretty good write up, and of course there is the CRAN page for the &lt;a href=&#34;https://cran.r-project.org/web/packages/spdep/index.html&#34;&gt;spdep library&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Best,&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>sf.chlorodot mini-package</title>
      <link>/post/sf.schlorodot/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/sf.schlorodot/</guid>
      <description>&lt;p&gt;Recently, I’d seen two tweets with stunning examples of maps by Paul Campbell &lt;a href=&#34;https://twitter.com/PaulCampbell91/status/992043182996193280&#34;&gt;here&lt;/a&gt; and (taken inspiration from the first) by Imer Muhović &lt;a href=&#34;https://twitter.com/ImerM1/status/1037358973807210498&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The basic idea of the dot chloropleths is to visualise not only the location clustering of each variable but the number of observations (something traditional ‘filled’ chloropleths don’t do). More importantly than this, the maps also just look really really cool.&lt;/p&gt;
&lt;p&gt;I had a spare few minutes during work on Friday which I tidied up into a package to calculate the random position of dots for such maps which can be found &lt;a href=&#34;https://github.com/RobWHickman/sf.chlorodot&#34;&gt;on my github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Below, I’ll outline the code for the South African example used in the package README. Data comes from Adrian Frith’s &lt;a href=&#34;https://census2011.adrianfrith.com/&#34;&gt;very good 2011 census site&lt;/a&gt; and &lt;a href=&#34;https://gadm.org/download_country_v3.html&#34;&gt;gadm&lt;/a&gt; for the shapefiles.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sf)
library(ggplot2)
library(tidyverse)
library(data.table)
library(rvest)

devtools::install_github(&amp;#39;RobWHickman/sf.chlorodot&amp;#39;)
library(sf.chlordot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, download and scrape the data for the map&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#download the South African shapefile fom gadm
admin_url &amp;lt;- &amp;quot;https://biogeo.ucdavis.edu/data/gadm3.6/Rsf/gadm36_ZAF_3_sf.rds&amp;quot;
download.file(admin_url, destfile = &amp;quot;shapefiles.rds&amp;quot;, mode = &amp;quot;wb&amp;quot;)
south_africa &amp;lt;- readRDS(&amp;quot;shapefiles.rds&amp;quot;) %&amp;gt;%
  #convert to sf
  st_as_sf() %&amp;gt;%
  select(region = NAME_3) %&amp;gt;%
  #merge geometries that have two rows
  group_by(region) %&amp;gt;%
  summarise()

#get the links to the data from Adrian Frith&amp;#39;s site
sa_data_url &amp;lt;- &amp;quot;https://census2011.adrianfrith.com&amp;quot;
south_africa_data &amp;lt;- sa_data_url %&amp;gt;%
  read_html() %&amp;gt;% html_nodes(&amp;quot;.namecell a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% paste0(sa_data_url, .) %&amp;gt;%
  lapply(., function(x) read_html(x) %&amp;gt;% html_nodes(&amp;quot;.namecell a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% paste0(sa_data_url, .)) %&amp;gt;% unlist() %&amp;gt;%
   lapply(., function(x) read_html(x) %&amp;gt;% html_nodes(&amp;quot;.namecell a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% paste0(sa_data_url, .)) %&amp;gt;% unlist()

#scrape the data on primary language from the 2011 South African census
language_data &amp;lt;- rbindlist(lapply(south_africa_data, function(x) {
  read &amp;lt;- read_html(x)
  language_nos &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;.datacell&amp;quot;) %&amp;gt;% html_text()
  start &amp;lt;- grep(&amp;quot;Percentage&amp;quot;, language_nos)[3] + 1
  stop &amp;lt;- grep(&amp;quot;Population&amp;quot;, language_nos) - 1
  #some areas have no data
  if(!is.na(start) &amp;amp; !is.na(stop)) {
    language_nos &amp;lt;- language_nos[start:stop]
    language_nos &amp;lt;- language_nos[seq(1, length(language_nos), 2)]
  } else {
    language_nos &amp;lt;- NA
  }
  
  languages &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;tr &amp;gt; :nth-child(1)&amp;quot;) %&amp;gt;% html_text()
  start &amp;lt;- grep(&amp;quot;First language&amp;quot;, languages) + 1
  stop &amp;lt;- grep(&amp;quot;Name&amp;quot;, languages) - 1
  if(length(start) &amp;gt; 0 &amp;amp; !is.na(stop)) {
    languages &amp;lt;- languages[start:stop]
  } else {
    languages &amp;lt;- NA
  }
  
  region_names &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;.topname&amp;quot;) %&amp;gt;% html_text()
  
  #combine into a df
  df &amp;lt;- data.frame(language = languages, primary_speakers = language_nos, region = region_names)
  return(df)
}))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the lanaguage data then needs to be transformed before the dot position is calculated. It must be in ‘short’ format with variables as column names. At the same time we can do some cleaning in order to match the shape areas with the region names from the census and remove data we don’t want to plot&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;language_data %&amp;lt;&amp;gt;%
  #convert number of speakers to numeric
  mutate(primary_speakers = as.numeric(as.character(primary_speakers))) %&amp;gt;%
  #matching of area names with South African shapefile
  mutate(region = gsub(&amp;quot; NU&amp;quot;, &amp;quot;&amp;quot;, region)) %&amp;gt;%
  mutate(region = gsub(&amp;quot;Tshwane&amp;quot;, &amp;quot;City of Tshwane&amp;quot;, region)) %&amp;gt;%
  #filter only the data we want to merge
  filter(region %in% south_africa$region) %&amp;gt;%
  filter(!is.na(language)) %&amp;gt;%
  filter(language != &amp;quot;Not applicable&amp;quot;) %&amp;gt;%
  #spread the data
  dcast(., region ~ language, value.var = &amp;quot;primary_speakers&amp;quot;, fun.aggregate = sum) %&amp;gt;%
  #join in the spatial geometry
  left_join(., south_africa) %&amp;gt;%
  #convert to sf
  st_as_sf()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then we can calculate the random dot position using calc_dots() from the sf.chlorodot package. This takes three arguments. The first is the df to take the data from (language_data). The second is which variables to calculate positions for. The easiest way to do this is to use names(df) and select from there, though a character vector can also be passed. Finally, n_per_dot is the number of observations (speakers of language x) for each dot on the map. This will affect the look of the map, but also the processing time (lower n_per_dot = greater time) so play around with it a bit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#calculate the dot positions using calc_dots from the sf.chlorodot package
sf_dots &amp;lt;- calc_dots(df = language_data, col_names = names(language_data)[2:14], n_per_dot = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can plot the output of this&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#stolen the background colour scheme from Paul Campbell&amp;#39;s blog
#original inspiration for this package
p &amp;lt;- ggplot() +
  geom_sf(data = south_africa, fill = &amp;quot;transparent&amp;quot;,colour = &amp;quot;white&amp;quot;) +
  geom_point(data = sf_dots, aes(lon, lat, colour = variable), size = 0.1) +
  scale_colour_discrete(name = &amp;quot;Primary Language&amp;quot;) +
  ggtitle(&amp;quot;Language Diversity in South Africa&amp;quot;) +
  theme_void() +
  guides(colour = guide_legend(override.aes = list(size = 10))) +
  theme(plot.background = element_rect(fill = &amp;quot;#212121&amp;quot;, color = NA), 
        panel.background = element_rect(fill = &amp;quot;#212121&amp;quot;, color = NA),
        legend.background = element_rect(fill = &amp;quot;#212121&amp;quot;, color = NA),
        text =  element_text(color = &amp;quot;white&amp;quot;),
        title =  element_text(color = &amp;quot;white&amp;quot;),
        legend.text=element_text(size=12),
        plot.title = element_text(size = 20),
        plot.margin = margin(1, 1, 1, 1, &amp;quot;cm&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../img/south_africa.png&#34; alt=&#34;chlorodot map of South African languages&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Enjoy!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Knowledge 4th August 2018</title>
      <link>/post/the-knowledge-4th-august-2018/</link>
      <pubDate>Sat, 04 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/the-knowledge-4th-august-2018/</guid>
      <description>&lt;p&gt;The Guardian publish a weekly set of questions and answers on a variety of football minutiae at &lt;a href=&#34;https://www.theguardian.com/football/series/theknowledge&#34;&gt;The Knowledge&lt;/a&gt;. Forutnately, some of these are extremely tractable using R, so I thought I’d have a go at working through the archives to see if I can shed light on any of the questions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
library(dplyr)
library(magrittr)
library(data.table)
library(zoo)
library(ggplot2)
library(rvest)
library(stringr)

#jalapic/engsoccerdata
library(engsoccerdata)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;we-aint-going-to-the-town..&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;We Ain’t Going To The Town..&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/topes_lose/status/1023537060668473344?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1023537060668473344&amp;amp;ref_url=https%3A%2F%2Fwww.theguardian.com%2Ffootball%2F2018%2Faug%2F01%2Ffootballers-who-have-backed-out-of-a-transfer-for-another-late-in-the-day&#34;&gt;‘This season, Tranmere Rovers return to contest League Two alongside eight teams with the suffix Town, including six successive fixtures against these clubs over the New Year. What is the record for successive fixtures versus clubs with the same (or no) prefix or suffix?’&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For this question I decided to ignore prefixes as the dataset I’m using doesn’t have any that could be matches between teams except the ‘West’ in West Ham and West Bromwich Albion. That dataset is the excellent engsoccerdata from James Curley found at his github &lt;a href=&#34;https://github.com/jalapic/engsoccerdata&#34;&gt;here&lt;/a&gt; and on CRAN.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#take all of the english soccer data in the package and bind it together
england_data &amp;lt;- bind_rows(
    select(engsoccerdata::england,
           .data$home, .data$visitor, date = .data$Date),
    select(engsoccerdata::englandplayoffs,
           .data$home, .data$visitor, date = .data$Date),
    select(engsoccerdata::england1939,
           .data$home, .data$visitor, date = .data$Date)) %&amp;gt;%
  setDT() %&amp;gt;%
  #convert the date to date class
  .[, date := as.Date(date)]

#get a list of each unique team in the dataset
all_teams &amp;lt;- unique(c(as.character(england_data$home),
                      as.character(england_data$visitor)))

#melt the dataset by each teams matches
find_chains &amp;lt;- rbindlist(lapply(all_teams, function(team) {
  england_data %&amp;gt;%
    .[home == team | visitor == team] %&amp;gt;%
    .[, matching_team := team]
  })) %&amp;gt;%
  .[home == matching_team, other := visitor] %&amp;gt;%
  .[visitor == matching_team, other := home] %&amp;gt;%
  .[, c(&amp;quot;date&amp;quot;, &amp;quot;matching_team&amp;quot;, &amp;quot;other&amp;quot;)] %&amp;gt;%
  #get the suffixes and prefixes of the other team
  .[, other_prefix := gsub(&amp;quot; .*&amp;quot;, &amp;quot;&amp;quot;, other)] %&amp;gt;%
  .[, other_suffix := gsub(&amp;quot;.* &amp;quot;, &amp;quot;&amp;quot;, other)] %&amp;gt;%
  #arrange by team and date
  .[order(matching_team, date)] %&amp;gt;%
  #convert to an id
  .[, suffix_id := as.numeric(as.factor(other_suffix))] %&amp;gt;%
  #if playing consecutively against the same suffix id (ignoring prefixes for now) put in same &amp;#39;chain&amp;#39;
  .[, match := suffix_id - lead(suffix_id), by = &amp;quot;matching_team&amp;quot;] %&amp;gt;%
  .[match == 0 &amp;amp; lead(match) != 0, chain_id := 1:.N] %&amp;gt;%
  .[match == 0] %&amp;gt;%
  .[, chain_id := na.locf(chain_id, fromLast = TRUE)] %&amp;gt;%
  .[, chain_length := .N, by = chain_id] %&amp;gt;%
  #take only chains at least as long as Tranmere&amp;#39;s run (6)
  .[chain_length &amp;gt; 5] %&amp;gt;%
  .[order(chain_length)] %&amp;gt;%
  .[, c(&amp;quot;date&amp;quot;, &amp;quot;matching_team&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;chain_length&amp;quot;)]

#print the chains of equal length to Tranmere&amp;#39;s run
print(find_chains)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           date  matching_team               other chain_length
##  1: 1950-12-30   Chesterfield      Leicester City            6
##  2: 1951-01-13   Chesterfield     Manchester City            6
##  3: 1951-01-20   Chesterfield       Coventry City            6
##  4: 1951-02-03   Chesterfield        Cardiff City            6
##  5: 1951-02-17   Chesterfield     Birmingham City            6
##  6: 1951-02-24   Chesterfield        Swansea City            6
##  7: 2009-03-21 Leicester City   Colchester United            6
##  8: 2009-03-28 Leicester City Peterborough United            6
##  9: 2009-04-04 Leicester City     Carlisle United            6
## 10: 2009-04-11 Leicester City     Hereford United            6
## 11: 2009-04-13 Leicester City        Leeds United            6
## 12: 2009-04-18 Leicester City     Southend United            6
## 13: 1921-05-02         Fulham           Hull City            7
## 14: 1921-05-07         Fulham           Hull City            7
## 15: 1921-08-27         Fulham       Coventry City            7
## 16: 1921-08-29         Fulham      Leicester City            7
## 17: 1921-09-03         Fulham       Coventry City            7
## 18: 1921-09-05         Fulham      Leicester City            7
## 19: 1921-09-10         Fulham           Hull City            7
## 20: 1920-04-17  Leyton Orient     Birmingham City            7
## 21: 1920-04-24  Leyton Orient     Birmingham City            7
## 22: 1920-04-26  Leyton Orient      Leicester City            7
## 23: 1920-05-01  Leyton Orient      Leicester City            7
## 24: 1920-08-28  Leyton Orient      Leicester City            7
## 25: 1920-08-30  Leyton Orient        Cardiff City            7
## 26: 1920-09-04  Leyton Orient      Leicester City            7
## 27: 1920-10-09   Notts County          Stoke City            7
## 28: 1920-10-16   Notts County          Stoke City            7
## 29: 1920-10-23   Notts County        Cardiff City            7
## 30: 1920-10-30   Notts County        Cardiff City            7
## 31: 1920-11-06   Notts County       Coventry City            7
## 32: 1920-11-13   Notts County       Coventry City            7
## 33: 1920-11-20   Notts County      Leicester City            7
##           date  matching_team               other chain_length&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;so In fact an identical length chain on matching suffixes has occured twice, with Chesterfield playing a range of cities at the start of 1951 in League Two, and much more recently, Leicester playing 6 different Uniteds in a row at the tail end of the 2008/2009 season. This is also the season that saw them recover from being relegated from the Chmapionship and start moving towards winning the title in 2015-2016 season.&lt;/p&gt;
&lt;p&gt;Some longer chains involving cities happened in the 1920-1921 seasons in the Second Division, but it seems like the scheduling worked differently then and teams played back to back more, so doesn’t really count.&lt;/p&gt;
&lt;p&gt;Having originally misread the question, I also wanted to find out the longest chain of a team playing teams that matched &lt;em&gt;their own&lt;/em&gt; suffix. We can do this using a similar method&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matching_fixtures &amp;lt;- england_data %&amp;gt;%
  #get only matches between teams with matching prefix/suffixes
  .[, home_suffix := gsub(&amp;quot;.* &amp;quot;, &amp;quot;&amp;quot;, home)] %&amp;gt;%
  .[, away_suffix := gsub(&amp;quot;.* &amp;quot;, &amp;quot;&amp;quot;, visitor)] %&amp;gt;%
  .[home_suffix == away_suffix, match := home_suffix] %&amp;gt;%
  .[!is.na(match)] %&amp;gt;%
  #remove matches where teams from the same city play each other
  .[!match %in% c(&amp;quot;Bradford&amp;quot;, &amp;quot;Bristol&amp;quot;, &amp;quot;Burton&amp;quot;, &amp;quot;Manchester&amp;quot;, &amp;quot;Sheffield&amp;quot;)]

#get all the teams that have played teams with matching suffixes
matching_teams &amp;lt;- unique(c(as.character(matching_fixtures$home),
                           as.character(matching_fixtures$visitor)))

#elongate the data and look for chains
find_chains &amp;lt;- rbindlist(lapply(matching_teams, function(team) {
  england_data %&amp;gt;%
    .[home == team | visitor == team] %&amp;gt;%
    .[order(date)] %&amp;gt;%
    .[, matching_team := team]
  })) %&amp;gt;%
  .[home == matching_team, other := visitor] %&amp;gt;%
  .[visitor == matching_team, other := home] %&amp;gt;%
  #id matches and remove matches not involving teams with identical suffixes
  .[, match_id := 1:.N, by = matching_team] %&amp;gt;%
  .[!is.na(match)] %&amp;gt;%
  #find chains of identical suffixed matches
  .[, chain := match_id - lag(match_id)] %&amp;gt;%
  .[chain == 1 &amp;amp; lag(chain) != 1, chain_id := 1:.N] %&amp;gt;%
  .[chain == 1] %&amp;gt;%
  .[, chain_id := na.locf(chain_id)] %&amp;gt;%
    .[, chain_length := .N, by = chain_id] %&amp;gt;%
  #take only chains at least as long as Tranmere&amp;#39;s run (6)
  .[chain_length &amp;gt; 4] %&amp;gt;%
  .[order(chain_length)] %&amp;gt;%
  .[, c(&amp;quot;date&amp;quot;, &amp;quot;matching_team&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;chain_length&amp;quot;)]

print(find_chains)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           date   matching_team             other chain_length
##  1: 1919-12-13      Stoke City   Birmingham City            5
##  2: 1919-12-20      Stoke City    Leicester City            5
##  3: 1919-12-25      Stoke City     Coventry City            5
##  4: 1919-12-26      Stoke City     Coventry City            5
##  5: 1919-12-27      Stoke City    Leicester City            5
##  6: 1919-09-01       Hull City        Stoke City            5
##  7: 1919-09-06       Hull City   Birmingham City            5
##  8: 1919-09-08       Hull City        Stoke City            5
##  9: 1919-09-13       Hull City        Leeds City            5
## 10: 1919-09-20       Hull City        Leeds City            5
## 11: 1988-09-24 Carlisle United  Rotherham United            5
## 12: 1988-09-30 Carlisle United  Cambridge United            5
## 13: 1988-10-04 Carlisle United Colchester United            5
## 14: 1988-10-08 Carlisle United   Hereford United            5
## 15: 1988-10-15 Carlisle United    Torquay United            5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the record for that is only slightly shorter! with Stoke and Hull City playing a range of cities in the 1919-1920 season (but see above for scheduling differences) and Carlisle United playing 5 other different Uniteds in a row in the old Fourth Division.&lt;/p&gt;
&lt;div id=&#34;answer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Answer&lt;/h2&gt;
&lt;p&gt;The record is 7 matches set by Notts County, Leyton Orient, and Fulham in 1920/1921 playing 7 teams with the suffix ‘city’ in a row. The Leyton Orient and Fulham chains stretch over the end of one season and into the next, so only Notts County really satisifies the question. However, the scheduling in these years involved a lot of back to back matches and so is cheating a bit.&lt;/p&gt;
&lt;p&gt;More recently Chesterfield played 6 different teams with the suffix ‘city’ in a row in 1950/1951, and Leceister played 6 different ’united’s in a row in their promotion season from League One in 2008/2009.&lt;/p&gt;
&lt;p&gt;Even more bizarre, Carlisle United played 5 other different United’s at the start of the 1988/1989 old Fourth Division season.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;youth-of-the-nation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Youth Of The Nation&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.theguardian.com/football/2018/aug/01/footballers-who-have-backed-out-of-a-transfer-for-another-late-in-the-day&#34;&gt;“If Lucas Hernández was born a year and a half later, his age would be a lower than his shirt number (21). Have any World Cup winners achieved this?” muses Edward Gibson.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The easiest way to check this is just to scrape all of the squads off of the wiki pages for the World Cups. I only did from 1954 onwards as before this the squad no and birthdate data is a bit patchy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#links to the world cup squads pages
wiki_cup_squads &amp;lt;- sprintf(&amp;quot;https://en.wikipedia.org/wiki/%s_FIFA_World_Cup_squads&amp;quot;,
                           seq(1954, 2018, by = 4))

#scrape all the player data we need
world_cup_squads &amp;lt;- rbindlist(lapply(wiki_cup_squads[1:17], function(link) {
  year &amp;lt;- gsub(&amp;quot;.*\\/wiki\\/&amp;quot;, &amp;quot;&amp;quot;, gsub(&amp;quot;_FIFA_World.*&amp;quot;, &amp;quot;&amp;quot;, link))
  read &amp;lt;- read_html(link)
  
  sq_no &amp;lt;- read %&amp;gt;% 
    html_nodes(&amp;quot;.plainrowheaders td:nth-child(1)&amp;quot;) %&amp;gt;%
    html_text() %&amp;gt;%
    as.numeric()
  sq_names &amp;lt;- read %&amp;gt;%
    html_nodes(&amp;quot;.plainrowheaders a:nth-child(1)&amp;quot;) %&amp;gt;% 
    html_text() %&amp;gt;%
    .[. != &amp;quot;&amp;quot;] %&amp;gt;%
    .[!grepl(&amp;quot;^\\[&amp;quot;, .)] %&amp;gt;%
    .[. != &amp;quot;Unattached&amp;quot;] %&amp;gt;% 
    .[!grepl(&amp;quot;captain&amp;quot;, .)]
  sq_dobs &amp;lt;- read %&amp;gt;% 
    html_nodes(&amp;quot;.plainrowheaders td:nth-child(4)&amp;quot;) %&amp;gt;%
    html_text() %&amp;gt;%
    str_extract(., &amp;quot;[0-9]{4}-[0-9]{2}-[0-9]{2}&amp;quot;) %&amp;gt;% 
    as.Date()
  countries &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;h3 .mw-headline&amp;quot;) %&amp;gt;% 
    html_text() %&amp;gt;% 
    trimws()

  if(year &amp;gt; 2006) countries &amp;lt;- countries[1:32]
  
  squad_data &amp;lt;- data.frame(name = sq_names,
                           no = sq_no,
                           dob = sq_dobs,
                           year= year) %&amp;gt;%
    setDT() %&amp;gt;%
    .[!grepl(&amp;quot;Nery Pumpido&amp;quot;, name)] %&amp;gt;%
    .[no == 1, country := countries] %&amp;gt;%
    .[, country := na.locf(country)] %&amp;gt;%
    .[, c(&amp;quot;name&amp;quot;, &amp;quot;no&amp;quot;, &amp;quot;dob&amp;quot;, &amp;quot;year&amp;quot;, &amp;quot;country&amp;quot;)]
}))

#find all world cup squad players with shirt numbers greater than their age in years
young_players &amp;lt;- world_cup_squads %&amp;gt;%
  .[, age := as.numeric(difftime(as.Date(paste0(year, &amp;quot;-07-01&amp;quot;)), dob)) / 365] %&amp;gt;%
  .[age &amp;lt; no]

print(young_players)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                        name no        dob year    country      age
##   1:   Aleksandar Petakovic 22 1932-08-06 1954 Yugoslavia 21.91507
##   2:         Ranulfo Cortés 22 1934-07-09 1954     Mexico 19.99178
##   3:             Coskun Tas 22 1935-04-23 1954     Turkey 19.20274
##   4:            Omar Méndez 20 1934-08-07 1954    Uruguay 19.91233
##   5:          Johnny Haynes 21 1934-10-17 1954    England 19.71781
##  ---                                                              
## 110: Trent Alexander-Arnold 22 1998-10-07 2018    England 19.74521
## 111:    José Luis Rodríguez 21 1998-06-19 2018     Panama 20.04658
## 112:       Dávinson Sánchez 23 1996-06-12 2018   Colombia 22.06575
## 113:         Dawid Kownacki 23 1997-03-14 2018     Poland 21.31233
## 114:           Moussa Wagué 22 1998-10-04 2018    Senegal 19.75342&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Overall 114 players are found. England actually have the most players with shirt numbers higher than their age with 9: Haynes, Hooper, Owen, Ferdinand, Carson, Walcott, Barkeley, Shaw, Alexander-Arnold. Surprisingly, most of these young English callups are pretty recent.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(data = young_players, aes(year)) +
  geom_bar() +
  ggtitle(&amp;quot;Number of Players in World Cup Squads With Nos &amp;gt; Age&amp;quot;) +
  xlab(&amp;quot;World Cup Year&amp;quot;) +
  ylab(&amp;quot;Number&amp;quot;)

print(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-05-The_Knowledge_1_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It seems that the real high point for this was the turn of the century with young players being given a shot at the tail end of squads, which is returning to pre-1998 levels by 2018.&lt;/p&gt;
&lt;p&gt;The data on these squad players is then merged with the data on the winning teams to find those who played for nations who went on to win the world cup.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wc_winners &amp;lt;- data.frame(winner = c(&amp;quot;West Germany&amp;quot;,&amp;quot;Brazil&amp;quot;,&amp;quot;Brazil&amp;quot;,&amp;quot;England&amp;quot;,
                                    &amp;quot;Brazil&amp;quot;,&amp;quot;West Germany&amp;quot;,&amp;quot;Argentina&amp;quot;,&amp;quot;Italy&amp;quot;,
                                    &amp;quot;Argentina&amp;quot;,&amp;quot;West Germany&amp;quot;,&amp;quot;Brazil&amp;quot;,&amp;quot;France&amp;quot;,
                                    &amp;quot;Brazil&amp;quot;,&amp;quot;Italy&amp;quot;,&amp;quot;Spain&amp;quot;,&amp;quot;Germany&amp;quot;,&amp;quot;France&amp;quot;),
                         year = seq(1954, 2018, 4))

#merge data with winners and find matches
young_players %&amp;lt;&amp;gt;% .[, year := as.numeric(as.character(year))] %&amp;gt;%
  .[, country := gsub(&amp;quot;(^\\s+)|(\\s+$)&amp;quot;, &amp;quot;&amp;quot;, country)] %&amp;gt;%
  merge(., wc_winners, by = &amp;quot;year&amp;quot;) %&amp;gt;%
  .[winner == country]

#kaka only one to have played as per https://en.wikipedia.org/wiki/List_of_FIFA_World_Cup_winners#By_year
print(young_players)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    year    name no        dob country      age winner
## 1: 1970    Leão 22 1949-07-11  Brazil 20.98630 Brazil
## 2: 1994 Ronaldo 20 1976-09-22  Brazil 17.78356 Brazil
## 3: 2002    Kaká 23 1982-04-22  Brazil 20.20548 Brazil&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So only the great &lt;a href=&#34;https://en.wikipedia.org/wiki/%C3%89merson_Le%C3%A3o&#34;&gt;Émerson Leão&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Ronaldo_(Brazilian_footballer)&#34;&gt;Ronaldo&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Kak%C3%A1&#34;&gt;Kaka&lt;/a&gt; satisfy the question. However, of these only Kaka played any part during the tournament, which only amounted to 25 minutes vs Costa Rica.&lt;/p&gt;
&lt;p&gt;Which players &lt;em&gt;could&lt;/em&gt; have satisfied this if they had a larger squad number?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;youngest_players &amp;lt;- world_cup_squads %&amp;gt;%
  .[, age := as.numeric(difftime(as.Date(paste0(year, &amp;quot;-07-01&amp;quot;)), dob)) / 365] %&amp;gt;%
  .[age &amp;lt; 23] %&amp;gt;%
  .[, country := gsub(&amp;quot;(^\\s+)|(\\s+$)&amp;quot;, &amp;quot;&amp;quot;, country)] %&amp;gt;%
  .[, year := as.numeric(as.character(year))] %&amp;gt;%
  merge(., wc_winners, by = &amp;quot;year&amp;quot;) %&amp;gt;%
  .[winner == country] %&amp;gt;%
  .[, dob := NULL]

#gives 53 potential results with world cup winners under the age of 23
print(youngest_players)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     year                 name no      country      age       winner
##  1: 1954          Horst Eckel  6 West Germany 22.40822 West Germany
##  2: 1954     Ulrich Biesinger 18 West Germany 20.91507 West Germany
##  3: 1958                 Pelé 10       Brazil 17.69863       Brazil
##  4: 1958               Moacir 13       Brazil 22.13425       Brazil
##  5: 1958              Orlando 15       Brazil 22.79452       Brazil
##  6: 1958              Mazzola 18       Brazil 19.95068       Brazil
##  7: 1962             Coutinho  9       Brazil 19.06849       Brazil
##  8: 1962                 Pelé 10       Brazil 21.70137       Brazil
##  9: 1962             Jurandir 14       Brazil 21.64658       Brazil
## 10: 1962            Mengálvio 17       Brazil 22.55342       Brazil
## 11: 1962        Jair da Costa 18       Brazil 21.99178       Brazil
## 12: 1966            Alan Ball  7      England 21.15068      England
## 13: 1966        Martin Peters 16      England 22.66027      England
## 14: 1966        Norman Hunter 18      England 22.68767      England
## 15: 1970            Clodoaldo  5       Brazil 20.77534       Brazil
## 16: 1970        Marco Antônio  6       Brazil 19.41096       Brazil
## 17: 1970          Paulo Cézar 18       Brazil 21.05479       Brazil
## 18: 1970                  Edu 19       Brazil 20.91507       Brazil
## 19: 1970             Zé Maria 21       Brazil 21.13425       Brazil
## 20: 1970                 Leão 22       Brazil 20.98630       Brazil
## 21: 1974        Paul Breitner  3 West Germany 22.83562 West Germany
## 22: 1974           Uli Hoeneß 14 West Germany 22.50137 West Germany
## 23: 1974        Rainer Bonhof 16 West Germany 22.27123 West Germany
## 24: 1978    Alberto Tarantini 20    Argentina 22.59178    Argentina
## 25: 1978 José Daniel Valencia 21    Argentina 22.75890    Argentina
## 26: 1982        Franco Baresi  2        Italy 22.16164        Italy
## 27: 1982     Giuseppe Bergomi  3        Italy 18.53699        Italy
## 28: 1982      Daniele Massaro 17        Italy 21.12055        Italy
## 29: 1986       Claudio Borghi  4    Argentina 21.76986    Argentina
## 30: 1986           Luis Islas 15    Argentina 20.53699    Argentina
## 31: 1990       Andreas Möller 17 West Germany 22.84384 West Germany
## 32: 1994              Ronaldo 20       Brazil 17.78356       Brazil
## 33: 1998       Patrick Vieira  4       France 22.03562       France
## 34: 1998        Thierry Henry 12       France 20.88493       France
## 35: 1998      David Trezeguet 20       France 20.72329       France
## 36: 2002           Ronaldinho 11       Brazil 22.29315       Brazil
## 37: 2002                 Kaká 23       Brazil 20.20548       Brazil
## 38: 2006     Daniele De Rossi  4        Italy 22.95342        Italy
## 39: 2010            Juan Mata 13        Spain 22.18904        Spain
## 40: 2010      Sergio Busquets 16        Spain 21.97260        Spain
## 41: 2010                Pedro 18        Spain 22.94247        Spain
## 42: 2010        Javi Martínez 20        Spain 21.84110        Spain
## 43: 2014      Matthias Ginter  3      Germany 20.46027      Germany
## 44: 2014       Julian Draxler 14      Germany 20.79178      Germany
## 45: 2014            Erik Durm 15      Germany 22.15068      Germany
## 46: 2014          Mario Götze 19      Germany 22.09041      Germany
## 47: 2014     Shkodran Mustafi 21      Germany 22.21918      Germany
## 48: 2018      Benjamin Pavard  2       France 22.27397       France
## 49: 2018     Presnel Kimpembe  3       France 22.89863       France
## 50: 2018         Thomas Lemar  8       France 22.64932       France
## 51: 2018        Kylian Mbappé 10       France 19.54247       France
## 52: 2018      Ousmane Dembélé 11       France 21.14247       France
## 53: 2018      Lucas Hernández 21       France 22.39178       France
##     year                 name no      country      age       winner&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#most of these young players actually played at their world cups and many appeared in finals
youngest_players_appeared &amp;lt;- youngest_players[c(1, 3:6, 8, 12:13, 15:18, 21:23, 24:25, 27, 29, 31, 33:35, 36:37, 38, 39:42, 44, 46:47, 48:53)]

#find nearest matches
youngest_players_appeared %&amp;lt;&amp;gt;% .[, diff := age - no]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The closest other players to make it are David Trezeguet (1998, 20.7years no 20), Shkodran Mustafi (2014, 22.2years, no 21) and then Lucas Hernandez (22.4years, no 21). Hernandez is the closest one to actually play in the World Cup final. Alberto Tarantini is his closest competition at 22.6 years old and wearing shirt number 20 in the 1978 final.&lt;/p&gt;
&lt;div id=&#34;answer-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Answer&lt;/h2&gt;
&lt;p&gt;Yes, three winners have appeared in World Cups with an age less than their shirt number. All Brazilians: Émerson Leão in 1970, Ronaldo in 1994, and Kaka in 2002. However only Kaka actually played (for 25 minutes vs. Costa Rica) in the finals.&lt;/p&gt;
&lt;p&gt;Other close calls are David Trezeguet (20.7, no 20 in 1998) and Shkodran Mustafi (22.2, no 21 in 2014).&lt;/p&gt;
&lt;p&gt;Hernandez &lt;em&gt;is&lt;/em&gt; the closest to acheiving this having played in the final itself, with only Alberto Tarantini (22.5, no 20 in 1978) and Mario Goetze (22.1, no 19 in 2014) in close competition.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Could an Independent Yorkshire Win the World Cup - Rest of the World/UK</title>
      <link>/post/yorkshire_world_cup_6/</link>
      <pubDate>Sun, 10 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/yorkshire_world_cup_6/</guid>
      <description>&lt;p&gt;Recently, a Yorkshire national football team &lt;a href=&#34;https://www.theguardian.com/uk-news/2018/jan/28/yorkshire-football-team-makes-debut-in-world-league-of-stateless-peoples&#34;&gt;appeared in a league of national teams for stateless people&lt;/a&gt;. This got me wondering how the historic counties of the UK would do at the world cup. Could any of them compete with full international teams?&lt;/p&gt;
&lt;p&gt;I &lt;a href=&#34;http://www.robert-hickman.eu/post/yorkshire_world_cup_1/&#34;&gt;published&lt;/a&gt; the complete code for that article on this blog this week. However, one question which I kept being asked was how a ‘All of the UK’ team would do (i.e. if the country wasn’t split up into England, Wales, Scotland, and Northern Ireland). Listening to the latest &lt;a href=&#34;https://twitter.com/doublepivotpod?lang=en&#34;&gt;Double Pivot Podcast&lt;/a&gt;, drafting plyers not going to the World Cup, I also wondered what a ‘Rest of the World’ 11 would look like/fare.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(magrittr)
library(data.table)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;building-teams&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Building Teams&lt;/h1&gt;
&lt;p&gt;To save time, I’m gonig to used saved versions of the datasets I built up over the 5 blog posts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#world rankings
world_rankings &amp;lt;- readRDS(&amp;quot;national_rankings.rds&amp;quot;)

#player data
all_players_data &amp;lt;- readRDS(&amp;quot;all_players_position_data.rds&amp;quot;)
#all British players
british_player_birthplaces &amp;lt;- readRDS(&amp;quot;british_player_birthplaces.rds&amp;quot;)

#the countries going to the world cup
world_cup_countries &amp;lt;- c(&amp;quot;Russia&amp;quot;, &amp;quot;Saudi Arabia&amp;quot;, &amp;quot;Egypt&amp;quot;, &amp;quot;Uruguay&amp;quot;,
                         &amp;quot;Portugal&amp;quot;, &amp;quot;Spain&amp;quot;, &amp;quot;Morocco&amp;quot;, &amp;quot;Iran&amp;quot;,
                         &amp;quot;France&amp;quot;, &amp;quot;Australia&amp;quot;, &amp;quot;Peru&amp;quot;, &amp;quot;Denmark&amp;quot;,
                         &amp;quot;Argentina&amp;quot;, &amp;quot;Iceland&amp;quot;, &amp;quot;Croatia&amp;quot;, &amp;quot;Nigeria&amp;quot;,
                         &amp;quot;Brazil&amp;quot;, &amp;quot;Switzerland&amp;quot;, &amp;quot;Costa Rica&amp;quot;, &amp;quot;Serbia&amp;quot;,
                         &amp;quot;Germany&amp;quot;, &amp;quot;Mexico&amp;quot;, &amp;quot;Sweden&amp;quot;, &amp;quot;Korea Republic&amp;quot;,
                         &amp;quot;Belgium&amp;quot;, &amp;quot;Panama&amp;quot;, &amp;quot;Tunisia&amp;quot;, &amp;quot;England&amp;quot;,
                         &amp;quot;Poland&amp;quot;, &amp;quot;Senegal&amp;quot;, &amp;quot;Colombia&amp;quot;, &amp;quot;Japan&amp;quot;)

#load data to save having to recalculate optimal teams
optimal_national_teams &amp;lt;- readRDS(&amp;quot;optimal_national_teams.rds&amp;quot;)
national_teams &amp;lt;- readRDS(&amp;quot;national_teams.rds&amp;quot;)

#the formations for selecting teams
formations_df &amp;lt;- readRDS(&amp;quot;formations_df.rds&amp;quot;)
formation_coords &amp;lt;- readRDS(&amp;quot;player_position_coords.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I won’t include the functions in this blog post either, but the article uses (at most very slight modified) functions from the previous 5 posts.&lt;/p&gt;
&lt;p&gt;We first need to sort the players into either the UK vs. the rest of the World* and finding the optimal teams for each, as we did prviously.&lt;/p&gt;
&lt;p&gt;*it’s possible Welsh (especially Gareth Bale), Northern Irish, or Scottish players might make the rest of the World team, but I’ll ignore that possibility for simplicity&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get the names of each player to merge in
player_lookup &amp;lt;- all_players_data %&amp;gt;%
  select(id, name, nationality) %&amp;gt;%
  mutate(original_nation = as.character(nationality))

#sort the data for finding teams
nationalised_players &amp;lt;- all_players_data %&amp;gt;%
  setDT() %&amp;gt;%
  #convert british players nationality to UK
  .[id %in% british_player_birthplaces$id, nationality := &amp;quot;UK&amp;quot;] %&amp;gt;%
  #filter out players from countries at the world cup
  .[!nationality %in% world_cup_countries] %&amp;gt;%
  #convert non-UK players nationality to &amp;quot;Rest of World&amp;quot;
  .[!id %in% british_player_birthplaces$id, nationality := &amp;quot;RoW&amp;quot;]

#find the optimal teams for both these nations
extranational_teams &amp;lt;- rbindlist(lapply(unique(nationalised_players$nationality), find_optimal_team, 
                                           select(nationalised_players, id, nationality, 49:60), replicates = 100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These can then be plotted to show the teams as before.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#select the best 4 county teams by total ability
extranational_teams %&amp;lt;&amp;gt;%
  setDT() %&amp;gt;%
  .[, unique_position := make.unique(as.character(position)), by = &amp;quot;nation&amp;quot;] %&amp;gt;%
  merge(., formation_coords, by = c(&amp;quot;formation&amp;quot;, &amp;quot;unique_position&amp;quot;)) %&amp;gt;%
  merge(player_lookup, by = &amp;quot;id&amp;quot;) 

#plot the data
p &amp;lt;- ggplot(data = extranational_teams)
p &amp;lt;- p %&amp;gt;%
  #custom pitch aesthetic function
  draw_pitch()
p &amp;lt;- p + 
  geom_text(aes(x = player_x, y = player_y, label = gsub(&amp;quot; &amp;quot;, &amp;quot;\n&amp;quot;, name), colour = original_nation), fontface = &amp;quot;bold&amp;quot;) +
  scale_colour_manual(values = c(&amp;quot;darkred&amp;quot;, &amp;quot;white&amp;quot;, &amp;quot;yellow&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;darkblue&amp;quot;, &amp;quot;orange&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;white&amp;quot;, &amp;quot;red&amp;quot;),
                      guide = FALSE) +
  facet_wrap(~nation)

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_6_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-ability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Calculating Ability&lt;/h1&gt;
&lt;p&gt;As previously, we can calculate the expected ELO of such teams via linear regression of the FIFA18 ability vs. ELO of actual national teams.&lt;/p&gt;
&lt;p&gt;This time, let’s predict the ability of the extranational teams based on this regression before plotting, just to save on plots/time/code/etc.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#merge in the world rankings for each fieldable national team
national_teams %&amp;lt;&amp;gt;% merge(., world_rankings, by = &amp;quot;nation&amp;quot;) %&amp;gt;%
  #merge in the optimal team total_ability for each nation
  merge(., unique(select(optimal_national_teams, nation, total_ability)), by = &amp;quot;nation&amp;quot;)

#regress ELO against total_ability (as judged by selection of FIFA18 players)
ability_regression &amp;lt;- lm(data = national_teams, ELO ~ total_ability)

#munge the extranational teams df to predict the ELO
extranational_teams &amp;lt;- data.frame(nation = c(&amp;quot;UK&amp;quot;, &amp;quot;RoW&amp;quot;)) %&amp;gt;%
  merge(., select(extranational_teams, nation, total_ability), by = &amp;quot;nation&amp;quot;) %&amp;gt;%
  #predict the ELO of each county using the previous regression
  mutate(predicted_ELO = predict(ability_regression, .)) %&amp;gt;%
  unique()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can plot this regression and see where the RoW and UK fall in terms of actual nations&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot ELO vs. total_ability
p &amp;lt;- ggplot(data = national_teams, aes(x = total_ability, y = ELO)) +
  geom_text(aes(label = nation), colour = &amp;quot;grey60&amp;quot;) +
  #add in the linear regression line + confidence intervals
  stat_smooth(method = &amp;quot;lm&amp;quot;, colour = &amp;quot;darkred&amp;quot;) +
  geom_text(data = extranational_teams, aes(label = nation, x = total_ability, y = predicted_ELO), colour = &amp;quot;darkblue&amp;quot;) +
  xlab(&amp;quot;FIFA18 Optimal Team Ability&amp;quot;) +
  ylab(&amp;quot;National Team ELO&amp;quot;) +
  ggtitle(&amp;quot;FIFA18 ability vs. ELO for National Teams&amp;quot;) +
  theme_minimal()

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_6_files/figure-html/plot_regression-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What’s quite nice about the graph is it shows the limitation of this approach. By definition, a UK team should be &lt;em&gt;at least&lt;/em&gt; as good as the English national team, but because England overperform their ‘FIFA ability’, the UK is actually ranked a fair bit lower in terms of ELO&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#show the ELOs of the English national football team
#and predicted ELO of a UK team
national_teams$ELO[national_teams$nation == &amp;quot;England&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1941&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extranational_teams$predicted_ELO[extranational_teams$nation == &amp;quot;UK&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       12 
## 1910.421&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The RoW team is similarly probably undervalued in terms of ELO. FIFA18 ranks the players as a lot better than teams like Germany and Brazil, but with much lower ELO&lt;/p&gt;
&lt;p&gt;We can then run the simulations, swapping the UK/RoW in for countries. The obvious substitute for the UK is England. For the RoW I decided to remove the team with the lowest ELO, which turns out to be Saudi Arabia&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#merge the ELOs with the world cup draw information
wc_teams %&amp;lt;&amp;gt;% merge(., select(national_teams, nation, ELO) %&amp;gt;%
                      rbind(., data.frame(nation = &amp;quot;Panama&amp;quot;, ELO = 1669)), by = &amp;quot;nation&amp;quot;)
wc_teams$nation &amp;lt;- as.character(wc_teams$nation)

simulate_counties &amp;lt;- function(extranation, simulations, replace_country) {
  #replace Englands ELO with that of the county team replacing them
  wc_teams$ELO[wc_teams$nation == replace_country] &amp;lt;- extranational_teams$predicted_ELO[extranational_teams$nation == extranation]
  wc_teams$nation[wc_teams$nation == replace_country] &amp;lt;- extranation
  
  #run x number of simulations
  for(simulation in 1:simulations) {
    winner &amp;lt;- simulate_tournament(wc_teams, knockout_matches, group_matches)
    if(simulation == 1) {
      winners &amp;lt;- winner
    } else {
      winners &amp;lt;- append(winners, winner)
    }
  }
  
  #spit out a df with each winner and the number of times they win
  simulation_df &amp;lt;- data.frame(table(winners))
  names(simulation_df) &amp;lt;- c(&amp;quot;nation&amp;quot;, &amp;quot;championships&amp;quot;)
  
  #work out the percentage chane of each nation/county winning
  simulation_df$percentage &amp;lt;- simulation_df$championships / (simulations/100)
  return(simulation_df)
}

#run the simulations
UK_simulation &amp;lt;- simulate_counties(&amp;quot;UK&amp;quot;, 1000, &amp;quot;England&amp;quot;) %&amp;gt;% 
  mutate(simulation = &amp;quot;UK&amp;quot;)
RoW_simulation &amp;lt;- simulate_counties(&amp;quot;RoW&amp;quot;, 1000, wc_teams$nation[which.min(wc_teams$ELO)]) %&amp;gt;%
  mutate(simulation = &amp;quot;RoW&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simulation_results &amp;lt;- rbind(UK_simulation, RoW_simulation) %&amp;gt;%
  setDT() %&amp;gt;%
    .[, perc_chance := mean(percentage), by = &amp;quot;nation&amp;quot;] %&amp;gt;%
  .[, c(&amp;quot;nation&amp;quot;, &amp;quot;perc_chance&amp;quot;)] %&amp;gt;%
  unique(.) %&amp;gt;%
  .[nation %in% c(&amp;quot;RoW&amp;quot;, &amp;quot;UK&amp;quot;), nation_status := &amp;quot;simulation&amp;quot;] %&amp;gt;%
  .[!nation %in% c(&amp;quot;RoW&amp;quot;, &amp;quot;UK&amp;quot;), nation_status := &amp;quot;nation&amp;quot;] %&amp;gt;%
  #order by percentage chance of winning the WC
  .[, nation := factor(nation, levels = nation[order(-.$perc_chance)])]

#plot the results
p &amp;lt;- ggplot(data = simulation_results, aes(x = nation, y = perc_chance)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, aes(fill = nation_status)) +
  scale_fill_manual(values = c(&amp;quot;darkblue&amp;quot;, &amp;quot;darkred&amp;quot;), name = &amp;quot;Nation Status&amp;quot;) +
  xlab(&amp;quot;Team&amp;quot;) +
  ylab(&amp;quot;World Cup Win Percentage Chance&amp;quot;) +
  ggtitle(&amp;quot;Percetange Chance of Winning the World Cup from 1000 Simulations&amp;quot;,
          subtitle = &amp;quot;UK/RoW Substituted for England/Saudi Arabia Respectively&amp;quot;) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1.2))

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_6_files/figure-html/munge_simulation_results-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The team for the RoW tend to do fairly well. In fact only Brazil, Germany, or Spain (3 of the tournament favourites) tend to win more simulated World Cups than them. The team for the whole of the UK disappoints as much as the English national team, winning about the same as the original, and other similarly ranked nations, such as Colombia, or Peru.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Could an Independent Yorkshire Win the World Cup - Simulate World Cups</title>
      <link>/post/yorkshire_world_cup_5/</link>
      <pubDate>Sat, 09 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/yorkshire_world_cup_5/</guid>
      <description>&lt;p&gt;Recently, a Yorkshire national football team &lt;a href=&#34;https://www.theguardian.com/uk-news/2018/jan/28/yorkshire-football-team-makes-debut-in-world-league-of-stateless-peoples&#34;&gt;appeared in a league of national teams for stateless people&lt;/a&gt;. This got me wondering how the historic counties of the UK would do at the world cup. Could any of them compete with full international teams?&lt;/p&gt;
&lt;p&gt;This is the complete script for an short article I wrote for &lt;a href=&#34;https://www.citymetric.com/horizons/football-could-independent-yorkshire-win-world-cup-3961&#34;&gt;CityMetric&lt;/a&gt; on the topic. It’s split over 5 separate parts and is pretty hefty but contains pretty much everything you need to clone the article. Now that we’ve picked the teams for each nation and county, it’s finally time to make predictions about the World Cup.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(magrittr)
library(data.table)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;get-county-rankings&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Get County Rankings&lt;/h1&gt;
&lt;p&gt;Now that we have the teams for each county, we want to work out how well they would do at a world cup. For this, we need to know roughly what their ranking would be compared to actual nations.&lt;/p&gt;
&lt;p&gt;Two sources of rankings of nations are the official FIFA world rankings, and also the world ELO ratings of each nation at www.eloratings.net.&lt;/p&gt;
&lt;p&gt;I scraped both of these (accurate to mid-May) and cleaned the data to match the nation names to those in the player dataset we’re using.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#scraped world rankings from FIFA and world ELO
#http://www.fifa.com/fifa-world-ranking/ranking-table/men/index.html
#https://www.eloratings.net/
#accurate for mid-May
#have matched country names between world rankings and FIFA player data
world_rankings &amp;lt;- readRDS(&amp;quot;national_rankings.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#glimpse the data
head(world_rankings)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      nation  ELO FIFA
## 1    Brazil 2131    2
## 2   Germany 2092    1
## 3     Spain 2049    8
## 4 Argentina 1985    5
## 5    France 1984    7
## 6  Portugal 1975    4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ELO is a chess rating mechanism which can be used to make predictions about which team would win in a matchup. If we compare it to the FIFA rankings, we can see there’s a clear negative correlation (the lower the ranking (e.g. top 10 teams in the world), the higher the ELO)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot FIFA rankings vs. ELO
p &amp;lt;- ggplot(data = world_rankings, aes(x = FIFA, y = ELO)) +
  geom_text(aes(label = nation)) +
  xlab(&amp;quot;FIFA Ability&amp;quot;) +
  ylab(&amp;quot;ELO Rankings&amp;quot;) +
  ggtitle(&amp;quot;The FIFA World Rankings and ELO Rankings for Countries&amp;quot;)

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_5_files/figure-html/plot_world_rankings-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To validate our method, the total ability of each team from their players in FIFA18 should correlate with this ELO rating.&lt;/p&gt;
&lt;p&gt;If we merge in the optimal team data and plot it against ELO we see nice linear positive correlation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#merge in the world rankings for each fieldable national team
national_teams %&amp;lt;&amp;gt;% merge(., world_rankings, by = &amp;quot;nation&amp;quot;) %&amp;gt;%
  #merge in the optimal team total_ability for each nation
  merge(., unique(select(optimal_national_teams, nation, total_ability)), by = &amp;quot;nation&amp;quot;)

head(national_teams)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      nation players gks  ELO FIFA total_ability
## 1   Albania      36   2 1596   56      73.44459
## 2   Algeria      58   3 1524   64      77.86387
## 3    Angola      16   1 1259  138      69.03657
## 4 Argentina     875 100 1985    5      84.97171
## 5 Australia     199  33 1714   40      74.21456
## 6   Austria     226  39 1726   26      78.65603&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot ELO vs. total_ability
p &amp;lt;- ggplot(data = national_teams, aes(x = total_ability, y = ELO)) +
  geom_text(aes(label = nation), colour = &amp;quot;grey60&amp;quot;) +
  xlab(&amp;quot;FIFA18 Optimal Team Ability&amp;quot;) +
  ylab(&amp;quot;National Team ELO&amp;quot;) +
  ggtitle(&amp;quot;FIFA18 ability vs. ELO for National Teams&amp;quot;) +
  theme_minimal()

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_5_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can quanitfy this correlation by creating a linear model using lm() and see that the adjusted R-squared is rather high- 0.7354.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#regress ELO against total_ability (as judged by selection of FIFA18 players)
ability_regression &amp;lt;- lm(data = national_teams, ELO ~ total_ability)

#summary
summary(ability_regression)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = ELO ~ total_ability, data = national_teams)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -222.229  -58.773    2.228   48.415  274.785 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   -755.764    160.669  -4.704 1.02e-05 ***
## total_ability   32.133      2.111  15.221  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 97.31 on 82 degrees of freedom
## Multiple R-squared:  0.7386, Adjusted R-squared:  0.7354 
## F-statistic: 231.7 on 1 and 82 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also plot this regression to further convince ourselves that predicting ELO from FIFA18 ability is a fairly valid move.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot ELO vs. total_ability
p &amp;lt;- ggplot(data = national_teams, aes(x = total_ability, y = ELO)) +
  geom_text(aes(label = nation), colour = &amp;quot;grey60&amp;quot;) +
  #add in the linear regression line + confidence intervals
  stat_smooth(method = &amp;quot;lm&amp;quot;, colour = &amp;quot;darkred&amp;quot;) +
  xlab(&amp;quot;FIFA18 Optimal Team Ability&amp;quot;) +
  ylab(&amp;quot;National Team ELO&amp;quot;) +
  ggtitle(&amp;quot;FIFA18 ability vs. ELO for National Teams&amp;quot;) +
  theme_minimal()

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_5_files/figure-html/plot_regression-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we have a predictor for ELO based on FIFA18 ability, we can now predict the ELO of each county team. We simply feed the model back into our df of optimal county teams.&lt;/p&gt;
&lt;p&gt;If we plot this over the previous plot we can see the counties have ELOs which fall within a range of national team abilities. The best counties (Yorkshire and Lancashire) are about as good as teams which generally qualify for world cups (e.g. Sweden and Serbia) whereas some counties (e.g. ) are much less proficient and would probably struggle to qualify.&lt;/p&gt;
&lt;p&gt;Given the teams we saw that were selected earlier, this makes sense- Yorkshire and Lancashire can field generally pretty solid teams of international/near-international level footballers and so would be expected to be competitive.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county_teams %&amp;lt;&amp;gt;% merge(., select(optimal_county_teams, county = nation, total_ability), by = &amp;quot;county&amp;quot;) %&amp;gt;%
  #predict the ELO of each county using the previous regression
  mutate(predicted_ELO = predict(ability_regression, .))

#add these to the plots of ELO ~ FIFA team ability
plot(p + geom_text(data = county_teams, aes(x = total_ability, y = predicted_ELO, label = county), colour = &amp;quot;darkblue&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_5_files/figure-html/predict_county_ELO-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulate-world-cups&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulate World Cups&lt;/h1&gt;
&lt;p&gt;Finally, we want to know if any of these counties have a shot at winning the World Cup.&lt;/p&gt;
&lt;p&gt;To do this, the best method is simply to simulate lots of World Cups and see what the percentage chance for each team is. This is possible as ELO gives us a quantifiable measure of how likely a given team is to beat another.&lt;/p&gt;
&lt;p&gt;Before we can simulate the World Cup however, we need some information about the draw.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wc_teams &amp;lt;- data.frame(nation = c(&amp;quot;Russia&amp;quot;, &amp;quot;Saudi Arabia&amp;quot;, &amp;quot;Egypt&amp;quot;, &amp;quot;Uruguay&amp;quot;,
                                  &amp;quot;Portugal&amp;quot;, &amp;quot;Spain&amp;quot;, &amp;quot;Morocco&amp;quot;, &amp;quot;Iran&amp;quot;,
                                  &amp;quot;France&amp;quot;, &amp;quot;Australia&amp;quot;, &amp;quot;Peru&amp;quot;, &amp;quot;Denmark&amp;quot;,
                                  &amp;quot;Argentina&amp;quot;, &amp;quot;Iceland&amp;quot;, &amp;quot;Croatia&amp;quot;, &amp;quot;Nigeria&amp;quot;,
                                  &amp;quot;Brazil&amp;quot;, &amp;quot;Switzerland&amp;quot;, &amp;quot;Costa Rica&amp;quot;, &amp;quot;Serbia&amp;quot;,
                                  &amp;quot;Germany&amp;quot;, &amp;quot;Mexico&amp;quot;, &amp;quot;Sweden&amp;quot;, &amp;quot;Korea Republic&amp;quot;,
                                  &amp;quot;Belgium&amp;quot;, &amp;quot;Panama&amp;quot;, &amp;quot;Tunisia&amp;quot;, &amp;quot;England&amp;quot;,
                                  &amp;quot;Poland&amp;quot;, &amp;quot;Senegal&amp;quot;, &amp;quot;Colombia&amp;quot;, &amp;quot;Japan&amp;quot;),
                       group = c(rep(letters[1:8], each = 4)),
                       draw = rep(1:4, 8))

group_matches &amp;lt;- data.frame(match = 1:6,
                            team1 = c(1,3,1,4,4,2),
                            team2 = c(2,4,3,2,1,3))

knockout_matches &amp;lt;- data.frame(round = c(rep(&amp;quot;R16&amp;quot;, 8), rep(&amp;quot;QF&amp;quot;, 4), rep(&amp;quot;SF&amp;quot;, 2), &amp;quot;F&amp;quot;),
                               team1 = c(&amp;quot;a1&amp;quot;, &amp;quot;c1&amp;quot;, &amp;quot;e1&amp;quot;, &amp;quot;g1&amp;quot;, &amp;quot;b1&amp;quot;, &amp;quot;d1&amp;quot;, &amp;quot;f1&amp;quot;, &amp;quot;h1&amp;quot;,
                                         &amp;quot;m49&amp;quot;, &amp;quot;m53&amp;quot;, &amp;quot;m51&amp;quot;, &amp;quot;m55&amp;quot;, &amp;quot;m57&amp;quot;, &amp;quot;m59&amp;quot;, &amp;quot;m61&amp;quot;),
                               team2 = c(&amp;quot;b2&amp;quot;, &amp;quot;d2&amp;quot;, &amp;quot;f2&amp;quot;, &amp;quot;h2&amp;quot;, &amp;quot;a2&amp;quot;, &amp;quot;c2&amp;quot;, &amp;quot;e2&amp;quot;, &amp;quot;g2&amp;quot;,
                                         &amp;quot;m50&amp;quot;, &amp;quot;m54&amp;quot;, &amp;quot;m52&amp;quot;, &amp;quot;m56&amp;quot;, &amp;quot;m58&amp;quot;, &amp;quot;m60&amp;quot;, &amp;quot;m62&amp;quot;),
                               match_id = c(&amp;quot;m49&amp;quot;, &amp;quot;m50&amp;quot;, &amp;quot;m53&amp;quot;, &amp;quot;m54&amp;quot;, &amp;quot;m51&amp;quot;, &amp;quot;m52&amp;quot;, &amp;quot;m55&amp;quot;, &amp;quot;m56&amp;quot;,
                                            &amp;quot;m57&amp;quot;, &amp;quot;m58&amp;quot;, &amp;quot;m59&amp;quot;, &amp;quot;m60&amp;quot;, &amp;quot;m61&amp;quot;, &amp;quot;m62&amp;quot;, &amp;quot;FINAL&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then we need to write functions to do the simulation.&lt;/p&gt;
&lt;p&gt;The first of these simply takes the ELO of the two teams and works out the win percentage for teamA (for teamB = 1 - p(teamA)).&lt;/p&gt;
&lt;p&gt;This is used in two further functions which simulate the group stages, and then the knockout stages respectively.&lt;/p&gt;
&lt;p&gt;For the groups, teams are drawn against each other as they will be in Russia and their ELOs compared. A random number generator is used to decided which teams wins (if p(teamA wins based on ELO) &amp;gt; random_number, teamA wins). I also included the chance to draw if the difference between the win_chance and the random_number is less than 0.1 in either direction.&lt;/p&gt;
&lt;p&gt;The points each team is predicted to win in the groups is then summed and the top two teams from each group progresses to the knockout stage.&lt;/p&gt;
&lt;p&gt;The knockout stage is easier to simulate as we don’t need to worry about points/draws. The same method as above is used to predict the winning team and that team progresses, whilst we remove the other from a df. Eventually only one team is left- the winner of the tournament.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#uses ELO to calculate the chance of team A winning
calc_win_chance &amp;lt;- function(ratingA, ratingB) {
  win_chance &amp;lt;- 1/ (1+10^((ratingB-ratingA)/400))
}

#simulate the group stages of the tournament
simulate_groups &amp;lt;- function(group_letter, national_teams, group_matches) {
  group &amp;lt;- national_teams %&amp;gt;%
    filter(group == group_letter) %&amp;gt;%
    mutate(points = 0) %&amp;gt;%
    mutate(av_difference = 0) %&amp;gt;%
    arrange(draw)
  
  #six matches per group
  for(match in 1:6){
    team1 &amp;lt;- group$nation[group_matches$team1[match]]
    team2 &amp;lt;- group$nation[group_matches$team2[match]]
    
    #calculate winner using a random number generator and comparing to the ELO win percentages
    random_number_draw &amp;lt;- runif(1)
    win_chance &amp;lt;- calc_win_chance(group$ELO[group$nation == team1], group$ELO[group$nation == team2])
    
    #update ELOs and assign group stage points
    if(win_chance - random_number_draw &amp;gt; 0.1) {
      group$points[group$nation == team1] &amp;lt;- group$points[group$nation == team1] + 3
      group$points[group$nation == team2] &amp;lt;- group$points[group$nation == team2] + 0
      
      group$ELO[group$nation == team1] &amp;lt;- group$ELO[group$nation == team1] + 50*(1-win_chance)
      group$ELO[group$nation == team2] &amp;lt;- group$ELO[group$nation == team2] + 50*(0-(1-win_chance))
      
    } else if(win_chance - random_number_draw &amp;lt; -0.1) {
      group$points[group$nation == team1] &amp;lt;- group$points[group$nation == team1] + 0
      group$points[group$nation == team2] &amp;lt;- group$points[group$nation == team2] + 3
      
      group$ELO[group$nation == team1] &amp;lt;- group$ELO[group$nation == team1] + 50*(0-win_chance)
      group$ELO[group$nation == team2] &amp;lt;- group$ELO[group$nation == team2] + 50*(1-(1-win_chance))

    } else {
      group$points[group$nation == team1] &amp;lt;- group$points[group$nation == team1] + 1
      group$points[group$nation == team2] &amp;lt;- group$points[group$nation == team2] + 1
      
      group$ELO[group$nation == team1] &amp;lt;- group$ELO[group$nation == team1] + 50*(0.5-win_chance)
      group$ELO[group$nation == team2] &amp;lt;- group$ELO[group$nation == team2] + 50*(0.5-(1-win_chance))
    }
    
    group$av_difference[group$nation == team1] &amp;lt;- group$av_difference[group$nation == team1] + 
      (group$ELO[group$nation == team1] - group$ELO[group$nation == team2])
    group$av_difference[group$nation == team2] &amp;lt;- group$av_difference[group$nation == team2] - 
      (group$ELO[group$nation == team1] - group$ELO[group$nation == team2])
  }
  
  #arrange the groups by points per team, then by the ELO difference between a team and it&amp;#39;s rivals
  #use ELO difference as secondary sorter as proxy for goal difference
  group &amp;lt;- arrange(group, -points, -av_difference) %&amp;gt;%
    mutate(position = 1:4)
  return(group)
}

#simulate the knockout rounds
simulate_knockout_rounds &amp;lt;- function(national_teams, knockout_matches) {
  for(match in seq(nrow(knockout_matches))) {
    #get the teams and the match id
    team1 &amp;lt;- as.character(national_teams$nation[which(national_teams$id == knockout_matches$team1[match])])
    team2 &amp;lt;- as.character(national_teams$nation[which(national_teams$id == knockout_matches$team2[match])])
    match_id &amp;lt;- as.character(knockout_matches$match_id[match])
    
    national_teams$id[which(national_teams$nation %in% c(team1, team2))] &amp;lt;- match_id
    
    #use a random number generator to decide the winner
    random_number_draw &amp;lt;- runif(1)
    
    #use ELO chances vs. the random number to work out which team wins
    win_chance &amp;lt;- calc_win_chance(national_teams$ELO[national_teams$nation == team1], national_teams$ELO[national_teams$nation == team2])
    
    #update ELOs and remove losing team
    if(win_chance &amp;gt; random_number_draw) {
      national_teams$ELO[national_teams$nation == team1] &amp;lt;- national_teams$ELO[national_teams$nation == team1] + 50*(1-win_chance)
      national_teams &amp;lt;- national_teams[-which(national_teams$nation == team2),]
    } else {
      national_teams$ELO[national_teams$nation == team2] &amp;lt;- national_teams$ELO[national_teams$nation == team2] + 50*(1-(1-win_chance))
      national_teams &amp;lt;- national_teams[-which(national_teams$nation == team1),]
    }
  }
  #returns the nation from the last remain row of the df == the winner of the tournament
  return(national_teams$nation)
}

#simulate the whole tournament
simulate_tournament &amp;lt;- function(national_teams, knockout_matches, group_matches) {
  #simulate the group stages
  knockout_rounds &amp;lt;- rbindlist(lapply(letters[1:8], simulate_groups,  
                                      national_teams = national_teams, group_matches = group_matches)) %&amp;gt;%
    #filter the top two teams from each group
    filter(position &amp;lt; 3) %&amp;gt;%
    mutate(id = paste0(group, position)) %&amp;gt;%
    select(nation, ELO, id)
  
  #simulate the knockout rounds until only 1 team remains
  winner &amp;lt;- simulate_knockout_rounds(national_teams = knockout_rounds, knockout_matches = knockout_matches) %&amp;gt;%
    as.character()
  return(winner)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To simulate the world cups, first we merge the ELO data with the world cup draw information. We also have to add Panama as they were missing from the teams based on our player data.&lt;/p&gt;
&lt;p&gt;Then here I run 10 simulations of the tournament and print the winners. Generally the clear favourites of the simulation are Brazil, then Germany, Spain and Argentina. This makes sense- they have the highest ELOs of all the nations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#merge the ELOs with the world cup draw information
wc_teams %&amp;lt;&amp;gt;% merge(., select(national_teams, nation, ELO) %&amp;gt;%
                      rbind(., data.frame(nation = &amp;quot;Panama&amp;quot;, ELO = 1669)), by = &amp;quot;nation&amp;quot;)

#run 10 simulations of the world cup choosing winners via ELO
for(simulation in 1:10) {
    winner &amp;lt;- simulate_tournament(wc_teams, knockout_matches, group_matches)
    if(simulation == 1) {
      winners &amp;lt;- winner
    } else {
      winners &amp;lt;- append(winners, winner)
    }
}

#list the winners of these 10 simulations
winners&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Belgium&amp;quot;  &amp;quot;Brazil&amp;quot;   &amp;quot;Germany&amp;quot;  &amp;quot;Brazil&amp;quot;   &amp;quot;Mexico&amp;quot;   &amp;quot;Brazil&amp;quot;  
##  [7] &amp;quot;Spain&amp;quot;    &amp;quot;France&amp;quot;   &amp;quot;Spain&amp;quot;    &amp;quot;Colombia&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can substitute in each county for the English national team and run x simulations (I use 10000 as anything more would take an unrealistic amount of processing time) to see what the chance of them winning the world cup would be.&lt;/p&gt;
&lt;p&gt;I iterate this through each county and then get a df of the chances for every nation (-England) and that county to win.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simulate_counties &amp;lt;- function(county, simulations) {
  #replace Englands ELO with that of the county team replacing them
  wc_teams$ELO[wc_teams$nation == &amp;quot;England&amp;quot;] &amp;lt;- county_teams$predicted_ELO[county_teams$county == county]
  
  #run x number of simulations
  for(simulation in 1:simulations) {
    winner &amp;lt;- simulate_tournament(wc_teams, knockout_matches, group_matches)
    #if &amp;#39;England&amp;#39; wins, replace England with the county
    if(winner == &amp;quot;England&amp;quot;) {
      winner &amp;lt;- county
    }
    if(simulation == 1) {
      winners &amp;lt;- winner
    } else {
      winners &amp;lt;- append(winners, winner)
    }
  }
  
  #spit out a df with each winner and the number of times they win
  simulation_df &amp;lt;- data.frame(table(winners))
  names(simulation_df) &amp;lt;- c(&amp;quot;nation&amp;quot;, &amp;quot;championships&amp;quot;)
  
  #work out the percentage chane of each nation/county winning
  simulation_df$percentage &amp;lt;- simulation_df$championships / (simulations/100)
  simulation_df$county_test &amp;lt;- county
  return(simulation_df)
}

#run for many simulations
#TAKES A LOT OF TIME
simulations_results &amp;lt;- rbindlist(lapply(county_team_rankings$nation, simulate_counties, 10000))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have that data out, some munging is necessary to get the average chance of winning the World Cup for each nation and label the counties and nations separately.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#munge the simulation_results
simulation_results %&amp;lt;&amp;gt;% setDT() %&amp;gt;%
  #get the average wc winning chance per nation across all simulations
  .[, perc_chance := mean(percentage), by = &amp;quot;nation&amp;quot;] %&amp;gt;%
  .[, perc_var := var(percentage), by = &amp;quot;nation&amp;quot;] %&amp;gt;%
  .[, c(&amp;quot;nation&amp;quot;, &amp;quot;perc_chance&amp;quot;, &amp;quot;perc_var&amp;quot;)] %&amp;gt;%
  unique(.) %&amp;gt;%
  #bind in the nations which never win the world cup in any simulation
  rbind(., unique(data.frame(nation = county_teams$county[which(!county_teams$county %in% .$nation)],
                      perc_chance = 0,
                      perc_var = NA))) %&amp;gt;%
  #is the team a nation or a county
  .[nation %in% county_teams$county, nation_status := &amp;quot;county&amp;quot;] %&amp;gt;%
  .[!nation %in% county_teams$county, nation_status := &amp;quot;nation&amp;quot;] %&amp;gt;%
  #order by percentage chance of winning the WC
  .[, nation := factor(nation, levels = nation[order(-.$perc_chance)])]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And can then plot the results…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot the results
p &amp;lt;- ggplot(data = simulation_results, aes(x = nation, y = perc_chance)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, aes(fill = nation_status)) +
  geom_errorbar(aes(ymax = perc_chance + perc_var, ymin = perc_chance - perc_var)) +
  scale_fill_manual(values = c(&amp;quot;darkred&amp;quot;, &amp;quot;darkblue&amp;quot;), name = &amp;quot;Nation Status&amp;quot;) +
  xlab(&amp;quot;Team&amp;quot;) +
  ylab(&amp;quot;World Cup Win Percentage Chance&amp;quot;) +
  ggtitle(&amp;quot;Percetange Chance of Winning the World Cup from 10000 Simulations&amp;quot;,
          subtitle = &amp;quot;Historic UK Counties Substituted in for England&amp;quot;) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1.2))

p2 &amp;lt;- ggplot(data = filter(simulation_results, nation_status == &amp;quot;county&amp;quot;), aes(x = nation, y = perc_chance)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, aes(fill = nation_status)) +
  geom_errorbar(aes(ymax = perc_chance + perc_var, ymin = perc_chance - perc_var)) +
  scale_fill_manual(values = c(&amp;quot;darkred&amp;quot;, &amp;quot;darkblue&amp;quot;), name = &amp;quot;Nation Status&amp;quot;) +
  xlab(&amp;quot;Team&amp;quot;) +
  ylab(&amp;quot;World Cup Win Percentage Chance&amp;quot;) +
  ggtitle(&amp;quot;Percetange Chance of Winning the World Cup from 10000 Simulations&amp;quot;,
          subtitle = &amp;quot;Historic UK Counties Substituted in for England&amp;quot;) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1.2))

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_5_files/figure-html/plot_results-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(p2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_5_files/figure-html/plot_results-2.png&#34; width=&#34;672&#34; /&gt; The bad news is, the real-life favourites tend to dominate the simulations. Brazil or Germany were predicted to win the tournament in almost half of all the simulations. On the graph, it;s just possible to make out the red bars of Yorkshire and Lancashire, both of which won 41 out of 10000 simulations (a 0.41 per cent chance of winning any random World Cup).&lt;/p&gt;
&lt;p&gt;This seems pretty low – but is comparable to pretty respectable teams like Denmark (0.775 per cent), Senegal (0.217 per cent), and even higher than the Iceland team which knocked england out of Euro2016 (0.339 per cent). It’s way higher than the chances the simulation gives the Russian hosts (0.07 per cent).&lt;/p&gt;
&lt;p&gt;Scaling down to just these pretty hopeless nations/counties really shows how little hope the independent British counties would have at an international tournament. However, the best four counties (Lancashire, Yorkshire, Essex, and Surrey) all have about a 0.2 per cent or higher chance, or 500-1 odds, at winning the 2018 World Cup were they to replace England at the last minute. This is an order of magnitude greater than the 5000-1 odds given to Leicester City at the start of 2015-2016 Premier League season, so there’s always a chance.&lt;/p&gt;
&lt;p&gt;And that’s it! All the code for my article over at &lt;a href=&#34;https://www.citymetric.com/horizons/football-could-independent-yorkshire-win-world-cup-3961&#34;&gt;CityMetric&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Could an Independent Yorkshire Win the World Cup - Picking Teams</title>
      <link>/post/yorkshire_world_cup_4/</link>
      <pubDate>Fri, 08 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/yorkshire_world_cup_4/</guid>
      <description>&lt;p&gt;Recently, a Yorkshire national football team &lt;a href=&#34;https://www.theguardian.com/uk-news/2018/jan/28/yorkshire-football-team-makes-debut-in-world-league-of-stateless-peoples&#34;&gt;appeared in a league of national teams for stateless people&lt;/a&gt;. This got me wondering how the historic counties of the UK would do at the world cup. Could any of them compete with full international teams?&lt;/p&gt;
&lt;p&gt;This is the complete script for an short article I wrote for &lt;a href=&#34;https://www.citymetric.com/horizons/football-could-independent-yorkshire-win-world-cup-3961&#34;&gt;CityMetric&lt;/a&gt; on the topic. It’s split over 6 separate parts and is pretty hefty but contains pretty much everything you need to clone the article. In the last post, we located the place and county of birth for British players, which we’ll use to pick teams for counties now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(magrittr)
library(data.table)
library(ggplot2)
library(rvest)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;putting-together-teams&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Putting Together Teams&lt;/h1&gt;
&lt;p&gt;To work out how good each nation/county is, we need to select the best team that can be picked from the available pool of players. In theory we could just select the best 11 players, but this isn’t how football works in real life. Instead, we want to pick the optimal 11 players for a set of realistic formations.&lt;/p&gt;
&lt;p&gt;First, we need a list of plausible formations, and the positions they contain. There’s a handy list of the default FIFA18 formations online which we’ll scrape.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#grab a link to all the default FIFA18 formations
link &amp;lt;- &amp;quot;https://www.fifauteam.com/fifa-18-formations-guide/#4222&amp;quot;

#get all the formations
formations &amp;lt;- read_html(link) %&amp;gt;%
  html_nodes(&amp;quot;h2&amp;quot;) %&amp;gt;%
  html_text() %&amp;gt;%
  .[2:length(.)]

#get all the positions per formation
positions &amp;lt;- read_html(link) %&amp;gt;%
  html_nodes(&amp;quot;td:nth-child(1)&amp;quot;) %&amp;gt;%
  html_text() %&amp;gt;%
  gsub(&amp;quot; .&amp;quot;, &amp;quot;&amp;quot;, .) %&amp;gt;%
  #make positions symmetric
  gsub(&amp;quot;RF|LF&amp;quot;, &amp;quot;CF&amp;quot;, .) %&amp;gt;%
  gsub(&amp;quot;CMR|CML&amp;quot;, &amp;quot;CM&amp;quot;, .) %&amp;gt;%
  gsub(&amp;quot;^R|^L&amp;quot;, &amp;quot;W&amp;quot;, .)

#df of each formation and the positions it contains
formations_df &amp;lt;- data.frame(formation = rep(formations, each = 10),
                            position = positions)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, for each nation/county, we need to work out which of these formations (and the selection of players for it), gives the highest total ability (using the ability for each position that we worked out earlier).&lt;/p&gt;
&lt;p&gt;To do this, I have two functions: - The first (find_optimal_team) selects the available players for that nation/county. It then wraps in a second function (pick_players) that takes a formation and tries to find the optimal team for that formation. Finally, we select the team that has the highest total_ability out of all the possibilities that pick_plyaers returns&lt;/p&gt;
&lt;p&gt;-pick_players itself iterates through each formation that we scraped. It then shuffles the positions each trial and pseudo-randomly picks the best* players for each position until an entire team is picked.It does this a specified (replicates) times per formation I find that doing it 100x per formation almost always gives an answer == 10000x per formation so I limit it to 100 to save on time.&lt;/p&gt;
&lt;p&gt;*it doesn’t always necessarily pick the very best player, as we can imagine that picking the best (e.g.) centre forward, might mean that player can’t be picked as a striker where they would be better. Instead it is biased towards picking the best player, though sometimes opting for the 2nd or 3rd best.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;find_optimal_team &amp;lt;- function(nation, players, replicates) {
  #find only players available to play for that nation
  players_pool &amp;lt;- players %&amp;gt;%
    filter(nationality == nation)
  
  #find the best team that can be played using these players for each default formation
  best_team &amp;lt;- rbindlist(lapply(rep(unique(formations_df$formation), replicates), pick_players, players = players_pool)) %&amp;gt;%
    #select only the formation/team with the highest total ability
    filter(total_ability == max(total_ability)) %&amp;gt;%
    #in case there are multiple best teams, take the first
    .[1:11,] %&amp;gt;%
    #add the nation as an id
    mutate(nation = nation)
  
  return(best_team)
}  

pick_players &amp;lt;- function(players, formation) {
  #get all the positions for he formation being tested
  formation_positions &amp;lt;- formations_df$position[formations_df$formation == formation]
  #randomise the order of positions to pick
  positions &amp;lt;- sample(as.character(formation_positions))
  #add the goalkeeper as the first to be picked
  positions &amp;lt;- append(&amp;quot;GK&amp;quot;, positions)
  
  #for each position that needs a player
  for(position in positions) {
    if(position != &amp;quot;GK&amp;quot;) {
      #generate a random number to determine if picking the best, second best, or third best player for that position
      #might not always be optimal to pick the best player if they are even better in another position
      randomiser &amp;lt;- runif(1)
      #pick the corresponding player
      if(randomiser &amp;lt; 0.6 | nrow(players) &amp;lt; 3) {
        id &amp;lt;- players$id[which.max(players[[position]])]
      } else if(randomiser &amp;lt; 0.9) {
        id &amp;lt;- players$id[order(-players[[position]])][2]
      } else {
        id &amp;lt;- players$id[order(-players[[position]])][3]
      }
    } else {
      #always pick the best goalkeeper available
      id &amp;lt;- players$id[which.max(players[[position]])]
    }
    
    #get the ability of that player in the position sampled
    ability &amp;lt;- players[[position]][which(players$id == id)]
    
    #create a df of all the players picked for this formation
    if(position == &amp;quot;GK&amp;quot;) {
      team &amp;lt;- data.frame(id = id, position = position, ability = ability)
    } else {
      team &amp;lt;- rbind(team, data.frame(id = id, position = position, ability = ability))
    }
    #for each player picked, remove it from further consideration for other positions
    players &amp;lt;- players[-which(players$id == id),]
  }
  
  #get the total ability of the team by averaging their position abilities
  team$total_ability &amp;lt;- sum(team$ability) / 11
  team$formation &amp;lt;- formation
  return(team)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not every nation has enough players in FIFA18 to pick a whole side so first we need to select only those who have at leat 10 outfield players and at least one goalkeeper. This leaves us with 84 nations in total (most of the top nations and few random stragglers).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the number of FIFA players for each nation
national_teams &amp;lt;- data.frame(table(all_players_data$nationality)) %&amp;gt;%
  merge(., data.frame(table(all_players_data$nationality[which(all_players_data$symmetric_position == &amp;quot;GK&amp;quot;)])), by = &amp;quot;Var1&amp;quot;)
names(national_teams) &amp;lt;- c(&amp;quot;nation&amp;quot;, &amp;quot;players&amp;quot;, &amp;quot;gks&amp;quot;)

#select only nations that can field a team
#at least 1 goalkeeper and 10 outfield players
national_teams &amp;lt;- national_teams %&amp;gt;%
  mutate(players = players - gks) %&amp;gt;%
  filter(players &amp;gt;= 10) %&amp;gt;%
  filter(gks &amp;gt;= 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then running the picking functions for each of these nations, giving us a df of each nations best possible team in FIFA18.&lt;/p&gt;
&lt;p&gt;This function takes a while to run (~1 hour total).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the optimal team for each nation
optimal_national_teams &amp;lt;- rbindlist(lapply(national_teams$nation, find_optimal_team, 
                                           select(all_players_data, id, nationality, 49:60), replicates = 100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then plot the national teams to take a look at the selections and check they make sense. I’ve only included the best 4 teams (Brazil, Germany, Spain, and Belgium) below to save space.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get the names of each player to merge in
players &amp;lt;- all_players_data %&amp;gt;%
  select(id, name)

#select the best 4 county teams by total ability
best_national_elevens &amp;lt;- optimal_national_teams %&amp;gt;%
  setDT() %&amp;gt;%
  .[, unique_position := make.unique(as.character(position)), by = &amp;quot;nation&amp;quot;] %&amp;gt;%
  merge(., formation_coords, by = c(&amp;quot;formation&amp;quot;, &amp;quot;unique_position&amp;quot;)) %&amp;gt;%
  merge(players, by = &amp;quot;id&amp;quot;) %&amp;gt;%
 .[total_ability &amp;gt;= abs(sort(unique(-.$total_ability)))[4]]

#plot the data
p &amp;lt;- ggplot(data = best_national_elevens)
p &amp;lt;- p %&amp;gt;%
  #custom pitch aesthetic function
  draw_pitch()
p &amp;lt;- p + 
  geom_text(aes(x = player_x, y = player_y, label = gsub(&amp;quot;.* &amp;quot;, &amp;quot;&amp;quot;, name)), colour = &amp;quot;black&amp;quot;) +
  facet_wrap(~nation)

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_4_files/figure-html/plot_best_national_teams-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We then need to do the same thing, but for the counties.&lt;/p&gt;
&lt;p&gt;First the player position ability for all the british players needs to be merged in.&lt;/p&gt;
&lt;p&gt;Then we select only those counties that can field a whole team, as we did before for nations. This leaves us with 20 counties overall which are plotted below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#merge the birthplace data with the playing ability data
british_player_data &amp;lt;- merge(british_player_birthplaces, select(all_players_data, id, 49:60))

#find the number of FIFA players for each county
county_teams &amp;lt;- data.frame(table(british_player_data$county)) %&amp;gt;%
  merge(., 
        data.frame(table(british_player_data$county[which(british_player_data$symmetric_position == &amp;quot;GK&amp;quot;)])),
        by = &amp;quot;Var1&amp;quot;)
names(county_teams) &amp;lt;- c(&amp;quot;county&amp;quot;, &amp;quot;players&amp;quot;, &amp;quot;gks&amp;quot;)

#select only counties that can field a team
#at least 1 goalkeeper and 10 outfield players
county_teams &amp;lt;- county_teams %&amp;gt;%
  mutate(players = players - gks) %&amp;gt;%
  filter(players &amp;gt;= 10) %&amp;gt;%
  filter(gks &amp;gt;= 1)

#plot the counties which can field a whole team
p &amp;lt;- ggplot(data = uk_counties) +
  geom_sf() +
  geom_sf(data = uk_counties[which(uk_counties$county %in%
                                     county_teams$county),], fill = &amp;quot;darkred&amp;quot;) +
  theme_void()

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_4_files/figure-html/select_fieldable_counties-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and, as before, use these to pick the optimal teams for these counties&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the optimal team for each county
optimal_county_teams &amp;lt;- rbindlist(lapply(county_teams$county, find_optimal_team, 
                                         select(british_player_data, id, nationality = county, 49:60),
                                         replicates = 100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This yields some interesting potential teams…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#merge in player names and position coordinates
county_elevens &amp;lt;- optimal_county_teams %&amp;gt;%
  setDT() %&amp;gt;%
  .[, unique_position := make.unique(as.character(position)), by = &amp;quot;nation&amp;quot;] %&amp;gt;%
  merge(., formation_coords, by = c(&amp;quot;formation&amp;quot;, &amp;quot;unique_position&amp;quot;)) %&amp;gt;%
  merge(players, by = &amp;quot;id&amp;quot;)

#plot the data
p &amp;lt;- ggplot(data = county_elevens)
p &amp;lt;- p %&amp;gt;%
  draw_pitch()
p &amp;lt;- p + 
  geom_text(aes(x = player_x, y = player_y, label = gsub(&amp;quot;.* &amp;quot;, &amp;quot;&amp;quot;, name), colour = total_ability), size = 2.5) +
  scale_colour_gradient(high = &amp;quot;darkred&amp;quot;, low = &amp;quot;darkblue&amp;quot;, guide = FALSE) +
  facet_wrap(~nation)

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_4_files/figure-html/plot_county_teams-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which is a bit too small to make out. If we just plot the 4 best teams (Yorkshire, Lancashire, Essex, and Surrey)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#select the best 4 county teams by total ability
county_elevens %&amp;lt;&amp;gt;%  .[total_ability &amp;gt;= abs(sort(unique(-county_elevens$total_ability)))[4]]

#plot the data
p &amp;lt;- ggplot(data = county_elevens)
p &amp;lt;- p %&amp;gt;%
  draw_pitch()
p &amp;lt;- p + 
  geom_text(aes(x = player_x, y = player_y, label = gsub(&amp;quot; &amp;quot;, &amp;quot;\n&amp;quot;, name)), colour = &amp;quot;black&amp;quot;, size = 2.5) +
  facet_wrap(~nation)

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_4_files/figure-html/plot_best_county_teams-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Could an Independent Yorkshire Win the World Cup - Finding British Player&#39;s Birthplaces</title>
      <link>/post/yorkshire_world_cup_3/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/yorkshire_world_cup_3/</guid>
      <description>&lt;p&gt;Recently, a Yorkshire national football team &lt;a href=&#34;https://www.theguardian.com/uk-news/2018/jan/28/yorkshire-football-team-makes-debut-in-world-league-of-stateless-peoples&#34;&gt;appeared in a league of national teams for stateless people&lt;/a&gt;. This got me wondering how the historic counties of the UK would do at the world cup. Could any of them compete with full international teams?&lt;/p&gt;
&lt;p&gt;This is the complete script for an short article I wrote for &lt;a href=&#34;https://www.citymetric.com/horizons/football-could-independent-yorkshire-win-world-cup-3961&#34;&gt;CityMetric&lt;/a&gt; on the topic. It’s split over 5 separate parts and is pretty hefty but contains pretty much everything you need to clone the article. Last time, we found the position abilities of each player using LASSO regression. This time, we’ll geolocate the birthplace of the British players in our dataset to find which county team they’d be eligible for.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(magrittr)
library(data.table)
library(ggplot2)
#use pediarr to query wikipedia to find the birthplace of players
library(pediarr)
#use googleway to geocode birthplaces
library(googleway)
#use sf to bin players into counties
library(sf)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;find-british-players-birthplaces&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Find British Players Birthplaces&lt;/h1&gt;
&lt;p&gt;To select our county teams, we need to know where each British player was born (and thus their ‘county’ nationality). Fortunately, wikipedia has an extremely detailed database of thousands of footballers, incluiding their birthplace (which we can assume is at least reasonably correct).&lt;/p&gt;
&lt;p&gt;First, the data needs to be filtered to include only players with British nationalities (English, Welsh, Scottish, or Northern Irish) or Irish. It’s very plausible that some players representing other countries would be born in England, and so eligible for the hypothetical county teams, but unlikely, and more trouble than it’s worth.&lt;/p&gt;
&lt;p&gt;When filtering, I also remove players who have no wikipedia page/birthplace listed. For some of these, I was able to manually locate their birthplace. Some players don’t get matched very well (mostly due to Australian/American footballers) and it was easiest just to manually supply the links to their wikipedia page.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#players with no wikipedia birthplace listed
players_missing_data &amp;lt;- c(&amp;quot;Liam Lindsay&amp;quot;,&amp;quot;Greg Docherty&amp;quot;,&amp;quot;Mikey Devlin&amp;quot;,&amp;quot;Josh Dacres-Cogley&amp;quot;,
                          &amp;quot;Tom Broadbent&amp;quot;,&amp;quot;Callum Gribbin&amp;quot;,&amp;quot;Sam Hughes&amp;quot;,&amp;quot;
                          James Cook&amp;quot;,&amp;quot;Daniel Jarvis&amp;quot;,&amp;quot;Zachary Dearnley&amp;quot;,&amp;quot;Ro-Shaun Williams&amp;quot;,
                          &amp;quot;Jack Fitzwater&amp;quot;,&amp;quot;Jack Hamilton&amp;quot;,&amp;quot;Lewis Banks&amp;quot;,&amp;quot;Greg Bolger&amp;quot;,&amp;quot;Chris Shields&amp;quot;,
                          &amp;quot;Conor Wilkinson&amp;quot;,&amp;quot;Barry McNamee&amp;quot;,&amp;quot;Keith Ward&amp;quot;,&amp;quot;Simon Madden&amp;quot;,&amp;quot;Dylan Connolly&amp;quot;,
                          &amp;quot;Brian Gartland&amp;quot;,&amp;quot;Dinny Corcoran&amp;quot;)

#players whose birthplace was manually found
missing_players_data &amp;lt;- readRDS(&amp;quot;missing_player_birthplaces.rds&amp;quot;)

#players whose wikipedia page is manually linked
manual_links &amp;lt;- readRDS(&amp;quot;manual_links.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function below then iterates through every player with a nationality from the British Isles and searches for a matching wikipedia page.&lt;/p&gt;
&lt;p&gt;It then looks for the birthplace of that player on their wikipedia page and returns a df containing the player and their birthplace.&lt;/p&gt;
&lt;p&gt;It also tries to match the birthdate listed from FIFA18 with that on their wikipedia page as a check and throws a warning if they don’t match. I haven’t looked into if there are mismatches there but ~50 players overall don’t match perfectly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;uk_players_info &amp;lt;- all_players_data %&amp;gt;%
  #only want data to help identify players by wiki page
  select(id, name, nationality, birthdate) %&amp;gt;%
  #only include UK nations (+Ireland)
  filter(nationality %in% c(&amp;quot;England&amp;quot;, &amp;quot;Scotland&amp;quot;, &amp;quot;Wales&amp;quot;, &amp;quot;Northern Ireland&amp;quot;, &amp;quot;Republic of Ireland&amp;quot;)) %&amp;gt;%
  #remove duplicated names
  #might lose some players here but they&amp;#39;re all so far down the pecking order effect should be minimal
  filter(!duplicated(name)) %&amp;gt;%
  #remove players who have no wikipedia birthplace
  filter(!name %in% players_missing_data)

#function to find the wikipedia page of each player
#returns a df with the player name and birthplace scraped from wikipedia
get_info &amp;lt;- function(row) {
  #get player info
  name &amp;lt;- uk_players_info$name[row]
  birthday &amp;lt;- uk_players_info$birthdate[row]
  id &amp;lt;- uk_players_info$id[row]
  
  #search wikipedia using the player name
  search &amp;lt;- pediasearch(name, extract = TRUE, limit = 10)
  #if a troublesome search use manual link
  if(name %in% manual_links$name) {
    wiki_suffix &amp;lt;- manual_links$link[which(manual_links$name == as.character(name))]
  } else {
    #else find the wikipedia page suffix for the player
    if(search[1] == &amp;quot;&amp;quot; &amp;amp; length(search) == 1) {
      wiki_suffix &amp;lt;- name %&amp;gt;%
        gsub(&amp;quot; &amp;quot;, &amp;quot;_&amp;quot;, .)
    } else {
      footballer &amp;lt;- grep(&amp;quot;football&amp;quot;, search)[1]
      wiki_suffix &amp;lt;- names(search)[footballer] %&amp;gt;%
        gsub(&amp;quot; &amp;quot;, &amp;quot;_&amp;quot;, .)
    }
  }
  
  #read the info card from the players wikipedia page
  info_card &amp;lt;- read_html(paste0(&amp;quot;https://en.wikipedia.org/wiki/&amp;quot;, wiki_suffix)) %&amp;gt;%
    html_nodes(&amp;quot;.vcard&amp;quot;) %&amp;gt;%
    .[1] %&amp;gt;%
    html_table(fill = TRUE) %&amp;gt;%
    data.frame()
  
  names(info_card) &amp;lt;- paste0(&amp;quot;X&amp;quot;, 1:ncol(info_card))
  info_card$X1 &amp;lt;- tolower(info_card$X1)
  
  #check if the wikipedia birthdate matches the FIFA one
  birthdate &amp;lt;- info_card %&amp;gt;%
    filter(X1 == &amp;quot;date of birth&amp;quot;)
  
  birthdate &amp;lt;- birthdate$X2 %&amp;gt;%
    as.character() %&amp;gt;%
    gsub(&amp;quot; .*&amp;quot;, &amp;quot;&amp;quot;, .) %&amp;gt;%
    gsub(&amp;quot;\\(|\\)&amp;quot;, &amp;quot;&amp;quot;, .) %&amp;gt;%
    as.Date()
  
  if(birthdate != birthday){
    warning(paste(row, &amp;quot;birthdays do not match&amp;quot;))
  }  
  
  #find the players birthplace
  birthplace &amp;lt;- info_card %&amp;gt;%
    filter(X1 == &amp;quot;place of birth&amp;quot;)
  
  birthplace &amp;lt;- birthplace$X2 %&amp;gt;%
    gsub(&amp;quot;\\[.*&amp;quot;, &amp;quot;&amp;quot;, .)
  
  #return info as a df
  df &amp;lt;- data.frame(id = id,
                   name = name,
                   birthdate = birthdate,
                   birthplace = birthplace)
  return(df)
}

#run the function over the first 1333 players
#after this very few players are found
british_player_birthplaces &amp;lt;- rbindlist(lapply(1:1329, get_info)) %&amp;gt;%
  #bind in the manually found data
  rbind(., missing_players_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the birthplaces for each player, we need to convert these into coordinates via geocoding. For this I use googleway, but the geocode() function from ggmap could also be used.&lt;/p&gt;
&lt;p&gt;The function takes a place and a key (for the API which isn’t included in the knitted markdown) and finds the lat lon for that place. To save on API requests I only run it on unique birthplaces then merge this back into the dataset.&lt;/p&gt;
&lt;p&gt;Once we have the lat/lon of each birthplace we can convert the df of players into an sf (spatial) object. If we do this, we see that a lot of players who are eligible for British nations aren’t actually born on the islands (e.g. Raheem Sterling was born in Jamaica). so I only select those which are born within the grouped spatial object of all 5 countries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#geocodes locations using googlemaps
#requires a google maps API key (hidden here)
googleway_geocode &amp;lt;- function(place, key){
  data &amp;lt;- google_geocode(place, key = key)
  latlon &amp;lt;- data$results$geometry$location[1,] %&amp;gt;%
    mutate(birthplace = place)
  #returns coordinates in the form latitude/longitude
  return(latlon)
}

birthplace_coords &amp;lt;- rbindlist(lapply(as.character(unique(british_player_birthplaces$birthplace)),
                                      googleway_geocode, key = key))

#also melt into one spatial row for subsetting later
uk &amp;lt;- uk_counties %&amp;gt;%
  group_by(&amp;quot;UK&amp;quot;) %&amp;gt;%
  summarise()

british_player_birthplaces &amp;lt;- british_player_birthplaces %&amp;gt;%
  merge(., birthplace_coords, by = &amp;quot;birthplace&amp;quot;) %&amp;gt;%
  #convert to an sf object
  st_as_sf(coords = c(&amp;quot;lng&amp;quot;, &amp;quot;lat&amp;quot;), crs = st_crs(uk_counties)) %&amp;gt;%
  #keep only those born within the UK proper
  .[unlist(st_contains(uk, .)),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we plot the players, we see they tend to be grouped around the large cities in London, Lancashire, and Yorkshire, with realtively few in Northern Ireland, rural Wales and the Highlands&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(data = uk_counties) +
  geom_sf() +
  geom_sf(data = british_player_birthplaces, colour = &amp;quot;darkred&amp;quot;, alpha = 0.3) +
  ggtitle(&amp;quot;Players Born in Historic UK Counties&amp;quot;) +
  theme_void()

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_3_files/figure-html/plot_british_players-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To find which county each player comes from, we can take the lat/lon of their birthplace and find which county shapefile contains it. The name of that county shapefile is then returned as a new column on the df of all British players&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the historic county each player was born within
british_player_birthplaces$county &amp;lt;- unlist(lapply(seq(nrow(british_player_birthplaces)), function(player) {
  #which county is there birthplace coordinates in
  container &amp;lt;- st_contains(uk_counties, british_player_birthplaces[player,])
  if(length(unlist(container)) == 1) {
    #which county name is this
    county &amp;lt;- as.character(uk_counties$county[as.numeric(t(container))])
    } else {
      county &amp;lt;- NA
    }
  return(county)
}))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;if we table the results of the county binning, we can see that many counties contain very few players, whereas some contain many more (e.g. Lancashire has 164 available players, whereas Cambridgeshire has only 5). Later, we will only look at counties that can field at least 10 outfield players + 1 goalkeeper.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#the number of players from each historic county
table(british_player_birthplaces$county)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##                   Aberdeen                   Anglesey 
##                         12                          1 
##                      Angus                   Ayrshire 
##                          3                         12 
##               Bedfordshire                  Berkshire 
##                         10                         15 
##               Berwickshire            Buckinghamshire 
##                          1                         15 
##                  Caithness             Cambridgeshire 
##                          1                          5 
##              Cardiganshire            Carmarthenshire 
##                          1                          2 
##             Carnarvonshire                   Cheshire 
##                          2                         50 
##                   Cornwall              County Antrim 
##                          5                         13 
##              County Armagh County Derry / Londonderry 
##                          2                          7 
##                County Down           County Fermanagh 
##                          3                          2 
##              County Tyrone                 Cumberland 
##                          3                          8 
##               Denbighshire                 Derbyshire 
##                          4                         13 
##                      Devon                     Dorset 
##                         17                          3 
##              Dumfriesshire             Dunbartonshire 
##                          2                          5 
##                     Dundee                     Durham 
##                          6                         26 
##                  Edinburgh                      Essex 
##                         23                         71 
##                       Fife                 Flintshire 
##                          5                          4 
##                  Glamorgan                    Glasgow 
##                         12                         35 
##            Gloucestershire                  Hampshire 
##                         13                         28 
##              Herefordshire              Hertfordshire 
##                          5                         33 
##            Huntingdonshire            Inverness-shire 
##                          3                          3 
##                       Kent                Lanarkshire 
##                         50                         18 
##                 Lancashire             Leicestershire 
##                        164                         12 
##               Lincolnshire                  Middlesex 
##                          8                         77 
##                 Midlothian              Monmouthshire 
##                          5                          5 
##                      Nairn                    Norfolk 
##                          1                          6 
##           Northamptonshire             Northumberland 
##                         12                         14 
##            Nottinghamshire                Oxfordshire 
##                         20                          6 
##                 Perthshire               Renfrewshire 
##                          3                          3 
##               Selkirkshire                 Shropshire 
##                          1                         11 
##                   Somerset              Staffordshire 
##                         12                         46 
##              Stirlingshire                    Suffolk 
##                          4                         10 
##                     Surrey                     Sussex 
##                         63                         16 
##               Warwickshire               West Lothian 
##                         44                          1 
##               Wigtownshire                  Wiltshire 
##                          1                          6 
##             Worcestershire                  Yorkshire 
##                          6                        103&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously not all of these counties can field complete teams of 11 players, but for those who can, in the next post, we’ll start picking teams and seeing how counties and nations stack up against each other.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Could an Independent Yorkshire Win the World Cup - LASSOs and Player Positions</title>
      <link>/post/yorkshire_world_cup_2/</link>
      <pubDate>Wed, 06 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/yorkshire_world_cup_2/</guid>
      <description>&lt;p&gt;Recently, a Yorkshire national football team &lt;a href=&#34;https://www.theguardian.com/uk-news/2018/jan/28/yorkshire-football-team-makes-debut-in-world-league-of-stateless-peoples&#34;&gt;appeared in a league of national teams for stateless people&lt;/a&gt;. This got me wondering how the historic counties of the UK would do at the world cup. Could any of them compete with full international teams?&lt;/p&gt;
&lt;p&gt;This is the complete script for an short article I wrote for &lt;a href=&#34;https://www.citymetric.com/horizons/football-could-independent-yorkshire-win-world-cup-3961&#34;&gt;CityMetric&lt;/a&gt; on the topic. It’s split over 6 separate parts and is pretty hefty but contains pretty much everything you need to clone the article. Last time, we got the shapefiles for the historic counties of the UK and scraped the player data we’ll use to build the teams&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(magrittr)
library(data.table)
library(ggplot2)
#we&amp;#39;ll use glmnet to do LASSO regression to determine players positional ability
library(glmnet)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;work-out-player-position-ability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Work Out Player Position Ability&lt;/h1&gt;
&lt;p&gt;The data we’ve scraped only gives a player’s overall ‘ability’ and their abilities on specific skills (e.g. strength, long shots, dribbling…). We want to use this to work out how good each player is at each position.&lt;/p&gt;
&lt;p&gt;It’s logical to assume that a player’s overall ability is how good they are at their main position (the position listed first on their page on fifaindex.com). We can therefore use &lt;a href=&#34;https://en.wikipedia.org/wiki/Lasso_(statistics)&#34;&gt;LASSO regression&lt;/a&gt; to work out which stats are contributing to their overall ability score. For instance, we would expect that a goalkeepers overall ability score is just a function of gk_positioning, gk_diving, gk_handling and so on… and doesn’t care about (e.g.) dribbling.&lt;/p&gt;
&lt;p&gt;This positional ability score is important as we can’t just select the 11 best players for each team as we might end up playing a goalkeeper and 10 defenders (or etc.). We need to make sure we select the best palyers for each position on a realistic formation.&lt;/p&gt;
&lt;p&gt;Unfortunately, there’s going to be no real way to tell between a players ability to play on either side of the field. There’s some correlation with their footedness ($foot), but it’s not worth going too in the weeds about that. So first we want to make all position symmetrical (i.e. we do discriminate between left and right sided positions).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_players_data %&amp;lt;&amp;gt;% mutate(symmetric_position = gsub(&amp;quot;L|R&amp;quot;, &amp;quot;W&amp;quot;, main_position))

unique(as.character(all_players_data$main_position))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;CF&amp;quot;  &amp;quot;LW&amp;quot;  &amp;quot;ST&amp;quot;  &amp;quot;GK&amp;quot;  &amp;quot;CAM&amp;quot; &amp;quot;CM&amp;quot;  &amp;quot;CB&amp;quot;  &amp;quot;CDM&amp;quot; &amp;quot;RW&amp;quot;  &amp;quot;LB&amp;quot;  &amp;quot;RM&amp;quot; 
## [12] &amp;quot;LM&amp;quot;  &amp;quot;RB&amp;quot;  &amp;quot;LWB&amp;quot; &amp;quot;RWB&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(all_players_data$symmetric_position)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;CF&amp;quot;  &amp;quot;WW&amp;quot;  &amp;quot;ST&amp;quot;  &amp;quot;GK&amp;quot;  &amp;quot;CAM&amp;quot; &amp;quot;CM&amp;quot;  &amp;quot;CB&amp;quot;  &amp;quot;CDM&amp;quot; &amp;quot;WB&amp;quot;  &amp;quot;WM&amp;quot;  &amp;quot;WWB&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First we need to convert the stats that we’re going to use for this regression/prediction into matrix form&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;player_stats &amp;lt;- all_players_data %&amp;gt;%
  #select only the players stats for each skill
  select(c(12, 14:47)) %&amp;gt;%
  #transition data into matrix
  model.matrix(overall~., .)

#show the first 6 instances of the first 5 stats
head(player_stats[,c(1:5)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept) ball_control dribbling marking slide_tackle
## 1           1           96        97      13           26
## 2           1           93        91      22           23
## 3           1           95        96      21           33
## 4           1           91        86      30           38
## 5           1           48        30      10           11
## 6           1           42        18      13           13&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To show how this works I’m going to illustrate it using goalkeepers and predicting the ability of each outfield player to play in goal.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#filter out only the goalkeepers
goalkeepers &amp;lt;- all_players_data %&amp;gt;%
  filter(symmetric_position == &amp;quot;GK&amp;quot;)

#select a percentage of these to use as training data
sample_percent &amp;lt;- 10
train_samples &amp;lt;- sample(1:nrow(goalkeepers), (nrow(goalkeepers)/100)*sample_percent)

gk_stats &amp;lt;- goalkeepers %&amp;gt;%
  slice(train_samples) %&amp;gt;%
  select(c(12, 14:47))

#get the stats per skill
train_matrix &amp;lt;- model.matrix(overall~., gk_stats)
#and the overall (gk) ability
train_ability &amp;lt;- gk_stats$overall
  
#perform the regression
cv_model &amp;lt;- cv.glmnet(train_matrix, train_ability)

#look at the weightings given to important variables using lambda value that gives minimum mean cross-validated error
coef(cv_model, s = &amp;quot;lambda.min&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 36 x 1 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
##                         1
## (Intercept)    1.48054668
## (Intercept)    .         
## ball_control   .         
## dribbling      .         
## marking        .         
## slide_tackle   .         
## stand_tackle   .         
## aggression     .         
## reactions      0.11010673
## positioning    .         
## interceptions  .         
## vision         .         
## composure      .         
## crossing       .         
## short_pass     .         
## long_pass      .         
## acceleration   .         
## stamina        .         
## strength       .         
## balance        .         
## sprint_speed   .         
## agility        .         
## jumping        .         
## heading        .         
## shot_power     .         
## finishing      .         
## long_shots     .         
## curve          .         
## free_kicks     .         
## penalties      .         
## volleys        .         
## gk_positioning 0.20822323
## gk_diving      0.20474961
## gk_handling    0.20874770
## gk_kicking     0.05344379
## gk_reflexes    0.20770553&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The regression selects only variables which have a strong relationship with the outcome (overall ability in the gk position in this case). As expected, it selects only the gk_… skillset and also a players reactions, which makes sense if goalkeepers have to make point blank saves.&lt;/p&gt;
&lt;p&gt;We can validate this by predicting the overall ability of the goalkeepers that aren’t in the training set fairly simply&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find non training examples of goalkeepers
test_matrix &amp;lt;- goalkeepers %&amp;gt;%
  slice(-train_samples) %&amp;gt;%
  select(c(12, 14:47)) %&amp;gt;%
  model.matrix(overall~., .)

#get the overall ability for these players
gk_abilities &amp;lt;- goalkeepers %&amp;gt;%
  slice(-train_samples) %&amp;gt;%
  select(overall)

#predict their overall ability based on their stats
gk_abilities$predicted &amp;lt;- as.vector(predict(cv_model, newx = test_matrix, s = &amp;quot;lambda.min&amp;quot;, type=&amp;quot;response&amp;quot;))

#plot these
p &amp;lt;- ggplot(data = gk_abilities, aes(x = overall, y = predicted)) +
  geom_point() +
  xlab(&amp;quot;Overall FIFA Ability&amp;quot;) +
  ylab(&amp;quot;Predicted FIFA Ability&amp;quot;) +
  ggtitle(&amp;quot;Actual and Predicted Goalkeeping Ability of Goalkeepers in FIFA18&amp;quot;)

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_2_files/figure-html/example_LASSO_validation-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which gives a very good fit. This is expected for the dataset we’re using as the overall ability hasbeen directly calclulated from the complete set of skills using some hidden algorithm. In the real world, the actual vs. predicted results would most likely have more noise.&lt;/p&gt;
&lt;p&gt;We can use this model now to predict how well outfield players would fare in goal, given that they have (low) ratings for all of these skills&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find all outfield players and convert stats the matrix
outfield_players &amp;lt;- all_players_data %&amp;gt;%
  filter(symmetric_position != &amp;quot;GK&amp;quot;) %&amp;gt;%
  select(c(12, 14:47)) %&amp;gt;%
  model.matrix(overall~., .)

#get the names of each outfield palyer
outfield_goalkeepers &amp;lt;- all_players_data %&amp;gt;%
  filter(symmetric_position != &amp;quot;GK&amp;quot;) %&amp;gt;%
  select(name)

#predict how well each outfield player would do in goal
outfield_goalkeepers$predicted_ability &amp;lt;- as.vector(predict(cv_model, newx = outfield_players, s = &amp;quot;lambda.min&amp;quot;, type=&amp;quot;response&amp;quot;))

head(outfield_goalkeepers)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 name predicted_ability
## 1       Lionel Messi          20.84383
## 2  Cristiano Ronaldo          21.78181
## 3             Neymar          21.10118
## 4        Luis Suárez          38.68063
## 5    Kevin De Bruyne          22.22474
## 6 Robert Lewandowski          20.20816&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is of course, extremely ironic that Luis Suarez scores relatively highly as an outfield player in goal &lt;a href=&#34;https://www.youtube.com/watch?v=wn_oYeugGiw&#34;&gt;given his history&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can use this technique to predict how each player would play in each position using the following function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#function to predict how each player would play in each position
get_position_weights &amp;lt;- function(position, sample_percent) {
  #filter data
  position_df &amp;lt;- all_players_data %&amp;gt;%
    filter(symmetric_position == position)
  
  #get training data
  train_samples &amp;lt;- sample(1:nrow(position_df), (nrow(position_df)/100)*sample_percent)
  
  train_stats &amp;lt;- position_df %&amp;gt;%
    .[train_samples,] %&amp;gt;%
    select(c(12, 14:47))
  
  train_matrix &amp;lt;- model.matrix(overall~., train_stats)
  
  train_ability &amp;lt;- train_stats$overall
  
  #use LASSO regression to find weighting of significant covariates
  cv_model &amp;lt;- cv.glmnet(train_matrix, train_ability)
  
  #predict players ability in that position
  position_ability &amp;lt;- predict(cv_model, newx = player_stats, s = &amp;quot;lambda.min&amp;quot;, type=&amp;quot;response&amp;quot;)
}

#run through every mirrored position
#using a high percentage of palyers in training set (50%) as in theory should be perfect regression
position_abilities &amp;lt;- lapply(unique(all_players_data$symmetric_position), 
                             get_position_weights, sample_percent = 50) %&amp;gt;%
  do.call(cbind, .) %&amp;gt;%
  data.frame()

#name each position
names(position_abilities) &amp;lt;- unique(all_players_data$symmetric_position)

#bind this to the data we have on all the players
all_players_data &amp;lt;- cbind(all_players_data, position_abilities) %&amp;gt;%
  #convert all non-natural goalkeepers goalkeeping ability to zero
  #want to make sure no non-goalkeepers are chosen in goal
  mutate(GK = ifelse(main_position == &amp;quot;GK&amp;quot;, overall, 0)) 

#show the first 6 rows
head(select(all_players_data, c(1, 49:60)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                name symmetric_position       CF       WW       ST GK
## 1      Lionel Messi                 CF 93.60706 92.27461 89.70278  0
## 2 Cristiano Ronaldo                 WW 92.88560 91.16822 92.62438  0
## 3            Neymar                 WW 89.48736 89.30329 85.17042  0
## 4       Luis Suárez                 ST 90.48249 87.40918 88.93461  0
## 5      Manuel Neuer                 GK 43.64828 39.22244 36.55564 92
## 6            De Gea                 GK 41.70048 36.48414 34.36504 91
##        CAM       CM       CB      CDM       WB       WM      WWB
## 1 93.36468 85.23987 46.02382 60.18700 58.22984 91.18465 62.26366
## 2 90.39084 83.31839 53.60718 62.65345 62.21724 90.32306 66.13034
## 3 89.01684 80.80476 47.23479 60.16824 60.11565 88.27848 64.19203
## 4 87.59322 81.40708 59.77832 66.74543 65.13876 86.44133 67.67849
## 5 47.58404 49.47508 34.07456 42.88950 35.23432 44.42806 35.91158
## 6 44.18874 46.37230 34.44230 42.30995 36.35090 41.15741 36.12737&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have the ability of each player to perform in any position we can use it to build teams. First however, we need to sort British players into the county of their birth, which we’ll do in the next post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Could an Independent Yorkshire Win the World Cup - Data &amp; Scraping</title>
      <link>/post/yorkshire_world_cup_1/</link>
      <pubDate>Tue, 05 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/yorkshire_world_cup_1/</guid>
      <description>&lt;p&gt;Recently, a Yorkshire national football team &lt;a href=&#34;https://www.theguardian.com/uk-news/2018/jan/28/yorkshire-football-team-makes-debut-in-world-league-of-stateless-peoples&#34;&gt;appeared in a league of national teams for stateless people&lt;/a&gt;. This got me wondering how the historic counties of the UK would do at the world cup. Could any of them compete with full international teams?&lt;/p&gt;
&lt;p&gt;This is the complete script for an short article I wrote for &lt;a href=&#34;https://www.citymetric.com/horizons/football-could-independent-yorkshire-win-world-cup-3961&#34;&gt;CityMetric&lt;/a&gt; on the topic. It’s split over 6 separate parts and is pretty hefty but contains pretty much everything you need to clone the article.&lt;/p&gt;
&lt;p&gt;To start, there are 95 historic counts of Great Britain + the 6 counties of Northern Ireland which I included for completeness. These are of a wide variety of sizes and &lt;a href=&#34;https://www.citymetric.com/horizons/which-historic-english-county-has-highest-population-3386&#34;&gt;approximate population&lt;/a&gt; and demographic, so it’s not clear how each would do simply from inspection.&lt;/p&gt;
&lt;p&gt;The data for this comes from &lt;a href=&#34;https://www.ordnancesurvey.co.uk/business-and-government/products/boundary-line.html&#34;&gt;Boundary Line&lt;/a&gt; and the &lt;a href=&#34;https://www.opendatani.gov.uk/dataset?q=boundary&#34;&gt;Northern Irish Boundary Database&lt;/a&gt;, I’ve put them together already, but its simple enough to do it in sf.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(rvest)
library(data.table)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(data = uk_counties) +
  geom_sf() +
  theme_void()

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-07-yorkshire_world_cup_1_files/figure-html/historic_counties-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In order to calculate how good each county team would be, I needed a measure of the ability of all of the players they could field. For this I turned to the FIFA18 video game which rates players along a variety of scales.&lt;/p&gt;
&lt;div id=&#34;scrape-player-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Scrape Player Data&lt;/h1&gt;
&lt;p&gt;To get data on every player in the game I wrote a quick scraping function. This finds the links to every player on all 602 pages of &lt;a href=&#34;https://www.fifaindex.com/players/&#34; class=&#34;uri&#34;&gt;https://www.fifaindex.com/players/&lt;/a&gt; and then downloads all the data required on each player.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#both steps here take a fair amount of time
#about 10mins and an hour respectively

#get the links to each players page
all_player_links &amp;lt;- unlist(lapply(paste0(&amp;quot;https://www.fifaindex.com/players/&amp;quot;, 1:602), function(x) {
  player_link &amp;lt;- read_html(x) %&amp;gt;%
    html_nodes(&amp;quot;td:nth-child(4) a&amp;quot;) %&amp;gt;%
    html_attr(&amp;quot;href&amp;quot;)
  })) %&amp;gt;%
  paste0(&amp;quot;https://www.fifaindex.com/&amp;quot;, .)

#big function to scrape every piece of data we could want of each players page
get_player_data &amp;lt;- function(link) {
  #read the players web page
  read &amp;lt;- read_html(link)
  
  #basic data
  name &amp;lt;- read %&amp;gt;%html_nodes(&amp;quot;.big&amp;quot;) %&amp;gt;% html_text()
  club &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;.panel-title a+ a&amp;quot;) %&amp;gt;% html_text() %&amp;gt;% .[length(.)]
  if(length(club) == 0) {
    club &amp;lt;- NA
  }
  nationality &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;.subtitle a&amp;quot;) %&amp;gt;% html_text()
  
  #general info on the player
  height &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;.col-lg-5 p:nth-child(1) .pull-right&amp;quot;) %&amp;gt;% html_text() %&amp;gt;%
    gsub(&amp;quot; cm&amp;quot;, &amp;quot;&amp;quot;, .) %&amp;gt;% as.numeric()
  weight &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;.col-lg-5 p:nth-child(2) .pull-right&amp;quot;) %&amp;gt;% html_text() %&amp;gt;%
    gsub(&amp;quot; kg&amp;quot;, &amp;quot;&amp;quot;, .) %&amp;gt;% as.numeric()
  foot &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;.col-lg-5 p:nth-child(3) .pull-right&amp;quot;) %&amp;gt;% html_text()
  birthdate &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;.col-lg-5 p:nth-child(4) .pull-right&amp;quot;) %&amp;gt;% html_text() %&amp;gt;%
    as.Date(&amp;quot;%m/%d/%Y&amp;quot;)
  age &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;.col-lg-5 p:nth-child(5) .pull-right&amp;quot;) %&amp;gt;% html_text() %&amp;gt;%
    as.numeric()
  main_position &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;body &amp;gt; div.container.main &amp;gt; div:nth-child(3) &amp;gt; div.col-md-8 &amp;gt; div:nth-child(3) &amp;gt; div.col-lg-5.col-sm-6 &amp;gt; div &amp;gt; div.panel-body &amp;gt; p:nth-child(6) &amp;gt; span &amp;gt; a&amp;quot;) %&amp;gt;% 
    .[1] %&amp;gt;% html_attr(&amp;quot;title&amp;quot;)
  work_rate &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;.col-lg-5 p:nth-child(7) .pull-right&amp;quot;) %&amp;gt;% html_text() %&amp;gt;% 
    str_split(., &amp;quot; / &amp;quot;) %&amp;gt;% unlist()
  
  #the players rating for each skill
  ratings &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;.rating&amp;quot;) %&amp;gt;% html_text() %&amp;gt;% as.numeric() %&amp;gt;%
    as.matrix() %&amp;gt;% t() %&amp;gt;% as.data.frame()
  names(ratings) &amp;lt;- c(&amp;quot;overall&amp;quot;,&amp;quot;specific&amp;quot;,&amp;quot;ball_control&amp;quot;,&amp;quot;dribbling&amp;quot;,&amp;quot;marking&amp;quot;,&amp;quot;slide_tackle&amp;quot;,&amp;quot;stand_tackle&amp;quot;,
                      &amp;quot;aggression&amp;quot;,&amp;quot;reactions&amp;quot;,&amp;quot;positioning&amp;quot;,&amp;quot;interceptions&amp;quot;,&amp;quot;vision&amp;quot;,&amp;quot;composure&amp;quot;,&amp;quot;crossing&amp;quot;,
                      &amp;quot;short_pass&amp;quot;,&amp;quot;long_pass&amp;quot;,&amp;quot;acceleration&amp;quot;,&amp;quot;stamina&amp;quot;,&amp;quot;strength&amp;quot;,&amp;quot;balance&amp;quot;,&amp;quot;sprint_speed&amp;quot;,
                      &amp;quot;agility&amp;quot;,&amp;quot;jumping&amp;quot;,&amp;quot;heading&amp;quot;,&amp;quot;shot_power&amp;quot;,&amp;quot;finishing&amp;quot;,&amp;quot;long_shots&amp;quot;,&amp;quot;curve&amp;quot;,&amp;quot;free_kicks&amp;quot;,
                      &amp;quot;penalties&amp;quot;,&amp;quot;volleys&amp;quot;,&amp;quot;gk_positioning&amp;quot;,&amp;quot;gk_diving&amp;quot;,&amp;quot;gk_handling&amp;quot;,&amp;quot;gk_kicking&amp;quot;,&amp;quot;gk_reflexes&amp;quot;)
  
  #stick everything into a dataframe to be output
  df &amp;lt;- data.frame(name = name, club = club, nationality = nationality,
                   height = height, weight = weight, foot = foot, birthdate = birthdate, age = age,
                   main_position = main_position, work_rate1 = work_rate[1], work_rate2 = work_rate[2]) %&amp;gt;%
    cbind(ratings)
  return(df)
}

#scrape the info on all players
all_players_data &amp;lt;- rbindlist(lapply(all_player_links, get_player_data)) %&amp;gt;%
  setDT() %&amp;gt;%
  #add an id column for each player
  .[, id := 1:.N]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once that’s scraped and bound we can take a peek at the data. There’s 18k players in total and 48 variables for each so we’ll just look at a few for now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#show a selection of the some key info for each player
#the id we gave them, their name, nationality, and their overall ability
head(select(all_players_data, id, name, nationality, overall))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id              name nationality overall
## 1:  1      Lionel Messi   Argentina      94
## 2:  2 Cristiano Ronaldo    Portugal      94
## 3:  3            Neymar      Brazil      92
## 4:  4       Luis Suárez     Uruguay      92
## 5:  5      Manuel Neuer     Germany      92
## 6:  6            De Gea       Spain      91&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Over the course of the next posts, we’ll use this data to calculate a player’s ability in any position on the field. This will then be used to select optimal teams for each nation (or each historic British county). Finally we’ll take the average ability of these optimal teams and use them to simulate the World Cup to get the chance each team has to win the tournament.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Riddler 27th April 2018</title>
      <link>/post/riddler-27th-april-2018/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/riddler-27th-april-2018/</guid>
      <description>&lt;p&gt;I’ve been looking for small programming problems to practice on while running experiments. One such source is &lt;a href=&#34;https://fivethirtyeight.com/features/how-fast-can-you-type-a-million-letters/&#34;&gt;Fivethirtyeight’s Riddler&lt;/a&gt; column which posts conundrums weekly. This week one problem focus on one of life’s universal problems: how many urinals are needed in any bathroom for all patrons to use it without awkwardness.&lt;/p&gt;
&lt;p&gt;Formally this is phrased as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Some number, N, of people need to pee, and there is some number, M, of urinals in a row in a men’s room. The people always follow a rule for which urinal they select: The first person goes to one on either far end of the row, and the rest try to maximize the number of urinals between them and any other person. So the second person will go on the other far end, the third person in the middle, and so on. They continue to occupy the urinals until one person would have to go directly next to another person, at which point that person decides not to go at all.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;What’s the minimum number, M, of urinals required to accommodate all the N people at the same time?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Which is perhaps easiest explained using the ‘urinal etiquette’ meme:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/urinal_etiquette.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Luckily, this sort of problem is extremely tractable in R to get an estimate of the function for any 1:N people with a few simple loops:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#just going to use dplyr and purr
#data.table might be faster but not too worried- verbose programming anyway
library(dplyr)
library(purrr)

#a tip from colin fay
#https://tinyurl.com/colin-fay-purrr
`%not_in%` &amp;lt;- negate(`%in%`)

#start with n = 1 and with a bathroom with 1 urinal
n &amp;lt;- 1
urinal_number &amp;lt;- 1

#create a df with 1 urinal which is unoccupied
urinals_df &amp;lt;- data.frame(urinal = 1:urinal_number,
                         occupied = rep(NA, urinal_number))

#for how many n do we want to solve
while(n &amp;lt; 101) {
  #whilst not all n have a urinal to use loop through
  while(sum(urinals_df$occupied, na.rm = TRUE) &amp;lt; n) {
    #when all are unoccupied take the first urinal
    if(sum(urinals_df$occupied, na.rm = TRUE) == 0) {
      urinals_df$occupied[1] &amp;lt;- 1
    #when all but 1 are unoccupied and there are more than 2 urinals
    #take the opposite end one next
    } else if(sum(urinals_df$occupied, na.rm = TRUE) == 1 &amp;amp;
              nrow(urinals_df) &amp;gt; 2) {
      urinals_df$occupied[nrow(urinals_df)] &amp;lt;- 1
    #otherwise work out the most isolated free urinal
    } else {
      #get the distances from each urinal to all the occupied urinals
      urinal_distances &amp;lt;- abs(1:nrow(urinals_df) - 
                                rep(which(!is.na(urinals_df$occupied)), each = nrow(urinals_df))) %&amp;gt;%
        matrix(., nrow = length(!is.na(urinals_df$occupied)))
      #index
      rownames(urinal_distances) &amp;lt;- 1:nrow(urinal_distances)  
      
      #awkward urinals are ones that are either taken or next to taken urinals
      #don&amp;#39;t want to urinate there
      awkward &amp;lt;- c(which(urinal_distances == 1, arr.ind = TRUE)[,1], 
                   which(urinal_distances == 0, arr.ind = TRUE)[,1]) %&amp;gt;%
        unique()
      
      #use %not_in% to find free urinals that aren&amp;#39;t in an awkward position
      possible_urinals &amp;lt;- which(rownames(urinal_distances) %not_in% awkward)
      
      #if only one remains use this urinal
      if(length(possible_urinals) == 1) {
        taken_urinal &amp;lt;- possible_urinals
      } else if(length(possible_urinals) &amp;gt; 1) {
        #for the remaining possible urinals find how far the closest taken urinal is
        #initialise a small nameless func
        closest_distance &amp;lt;- lapply(seq(nrow(urinal_distances)), function(x){
          row &amp;lt;- urinal_distances[x,]
          min &amp;lt;- min(row)
          }) %&amp;gt;%
          unlist()
        
        #use the urinal that has the maximum distance to its closest urinal
        taken_urinal &amp;lt;- as.numeric(rownames(urinal_distances)[which.max(closest_distance)])
      } else if(length(possible_urinals) == 0) {
        #if there are no free urinals break the loop
        #and add one to the urinal number in the hypothetical bathroom
        urinal_number &amp;lt;- urinal_number + 1
        break
      }
      #occupy the chosen urinal
      urinals_df$occupied[taken_urinal] &amp;lt;- 1
    }
  }
  
  #if completed
  #i.e. if all users have found a satisfactory free urinal
  if(sum(urinals_df$occupied, na.rm = TRUE) == n) {
    if(n == 1) {
      #when n = 1 initial a df to hold the results per n
      results_df &amp;lt;- data.frame(n = 1,
                               urinals_required = urinal_number)
    } else {
      #otherwise add in a new row to results_df
      results_df &amp;lt;- rbind(results_df, data.frame(n = n, urinals_required = urinal_number))
    }
      #increase n to the next number of patrons
      n &amp;lt;- n + 1
      #start with at least n urinals in the next bathroom
      #this is the bare minimum we would need
      urinal_number &amp;lt;- n
  }
  
  #reintialise the bathroom to see if it is big enough for the n patrons
  urinals_df &amp;lt;- data.frame(urinal = 1:urinal_number,
                           occupied = rep(NA, urinal_number))
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then plot this. I decided to add a little flair to the plot using annotate_custom which is a nice little trick to spice up ggplots&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load the libraries for plotting
library(ggplot2)
library(png)
library(grid)

#a nice png of a urinal I found online
urinal_image &amp;lt;- readPNG(&amp;quot;../../static/img/urinal.png&amp;quot;) %&amp;gt;%
  rasterGrob()

#plot the number of urinals needed for any n number of patrons
urinals_plot &amp;lt;- ggplot(data = results_df, aes(x = n, y = urinals_required)) + 
  geom_point() +
  #mapply a function to paste the urinal image as an annotation to the graph
  #takes the x and y arguments from the ggplot aesthetic
  mapply(function(x, y, size) {
    annotation_custom(urinal_image,
                      xmin = x - size, xmax = x + size, 
                      ymin = y - size, ymax = y + size) },
    x = results_df$n, y = results_df$urinals_required, size = 7) +
  #labelling and etc.
  ylab(&amp;quot;Urinals Required&amp;quot;) +
  xlab(&amp;quot;Number of Patrons&amp;quot;) +
  ggtitle(&amp;quot;How many urinals are needed for any n number of socially awkward urinators&amp;quot;,
          subtitle = &amp;quot;answer to The Riddler 27/04/2018&amp;quot;) +
  theme(panel.background = element_rect(fill = &amp;#39;lightblue&amp;#39;))

urinals_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-01-riddler-27th-april-2018_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;which gives a surprisingly complex function! I had assume it would be some simple function of x but clearly something more complex is going on.&lt;/p&gt;
&lt;p&gt;Why this happens become clear if you plot out why M urinals are needed for N people. Optimally each person would be separated by 1 urinal, but as the number of urinals increases they become less efficiently packed, with 2 urinals (neither of which can be used without standing next to someone) between each urinating person. This eventually reaches a breaking point and the number of urinals necessary jumps upward.&lt;/p&gt;
&lt;p&gt;The formula is known as ‘The Pay Phone Packing Sequence’ (where users of pay phones don’t want to be overheard) and is summarised at &lt;a href=&#34;https://oeis.org/A185456&#34; class=&#34;uri&#34;&gt;https://oeis.org/A185456&lt;/a&gt; (where packing occurs/ n &amp;gt; 2). The formula itself is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(n) = n + 2 ^ {(1 + floor(log(n - 2)))}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;That’s all for this weeks riddler.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=l3V4KfeJBCQ&#34;&gt;Franz Ferdinand and Sparks - Piss Off&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello World! And A Small Chess Plotting Package</title>
      <link>/post/hello-world-a-small-chess-plotting-package/</link>
      <pubDate>Wed, 21 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/hello-world-a-small-chess-plotting-package/</guid>
      <description>&lt;p&gt;Finally gotten around to using my &lt;a href=&#34;https://cran.r-project.org/web/packages/blogdown/index.html&#34; target=&#34;_blank&#34;&gt;blogdown&lt;/a&gt; website!&lt;/p&gt;

&lt;p&gt;Feels like a fairly good place to publicise a small package to plot chess games using only the pgn using ggplot2 in R which can be found &lt;a href=&#34;https://github.com/RobWHickman/kaRpov&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To copy the readme mini-vignette provides a nice overview of the uber-function which goes from pgn -&amp;gt; gif.&lt;/p&gt;

&lt;p&gt;As in the README massive thanks and inspiration to &lt;a href=&#34;https://twitter.com/jbkunst&#34; target=&#34;_blank&#34;&gt;Joshua Kunst&lt;/a&gt; and his &lt;a href=&#34;http://jkunst.com/rchess/&#34; target=&#34;_blank&#34;&gt;rchess package&lt;/a&gt; which I failed to get going on my laptop and decided to try my arm at writing something from scratch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(kaRpov)

#the pgn for the immortal game
immortal_pgn &amp;lt;- &amp;quot;1.e4 e5 2.f4 exf4 3.Bc4 Qh4+ 4.Kf1 b5 5.Bxb5 Nf6 6.Nf3 Qh6 7.d3 Nh5 8.Nh4 Qg5 9.Nf5 c6 10.g4 Nf6 11.Rg1 cxb5 12.h4 Qg6 13.h5 Qg5 14.Qf3 Ng8 15.Bxf4 Qf6 16.Nc3 Bc5 17.Nd5 Qxb2 18.Bd6 Bxg1 19.e5 Qxa1+ 20.Ke2 Na6 21.Nxg7+ Kd8 22.Qf6+ Nxf6 23.Be7#&amp;quot;

filename &amp;lt;- &amp;quot;C:/Users/MagnusCarlsen/Desktop/immortal_game&amp;quot;

#need to fix library importing
library(tweenr)
library(animation)
library(ggplot2)
library(grid)
library(png)

#create the gif
plot_pgn (immortal_pgn, 
          light_col = &amp;quot;#f5f5dc&amp;quot;, dark_col = &amp;quot;#00688b&amp;quot;, square_labels = FALSE, plot = FALSE,
          move_cutoff = NULL, frames = 100, interpolation = 0.5,
          speed = 10, pause_end = TRUE, black_shift = NULL,
          name = filename)
          
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&amp;rsquo;s also a load of semi-arranged smaller functions used to work out the positions of the pieces and plot the board etc. which people can play around with. I&amp;rsquo;ll probably do a bit more cleaning of it and really want to try and integrate it with a chess engine API to add the relative strengths of each player to the package, but for now there&amp;rsquo;s more interesting things to do elsewhere :)&lt;/p&gt;

&lt;p&gt;Thank you for reading- and I hope you enjoy the package!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/immortal_game.gif&#34; alt=&#34;Immortal Game&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
