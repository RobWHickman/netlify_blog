<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.54.0" />
  <meta name="author" content="Robert Hickman">

  
  
  
  
    
      
    
  
  <meta name="description" content="I’m extremely biased, but to me, one of the real success* stories in neuroscience over the last (just over) two decades has been in studying reward signals. Since the seminal 1997 paper, a lot of work has gone into figuring out how the brain assigns value to outcomes.
*ugh, maybe. This isn’t a blog post about that
My PhD project looks at novel ways of eliciting valuation behaviour to study these signals, but as a key part of the modelling involved in this work, it’s important to get a handle on reinforcement learning.">

  
  <link rel="alternate" hreflang="en-us" href="/post/r-inforcement_learning_one/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Robert Hickman">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Robert Hickman">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/r-inforcement_learning_one/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@robert_squared">
  <meta property="twitter:creator" content="@robert_squared">
  
  <meta property="og:site_name" content="Robert Hickman">
  <meta property="og:url" content="/post/r-inforcement_learning_one/">
  <meta property="og:title" content="R-inforcement Learning Part One- Tic Tac Toe | Robert Hickman">
  <meta property="og:description" content="I’m extremely biased, but to me, one of the real success* stories in neuroscience over the last (just over) two decades has been in studying reward signals. Since the seminal 1997 paper, a lot of work has gone into figuring out how the brain assigns value to outcomes.
*ugh, maybe. This isn’t a blog post about that
My PhD project looks at novel ways of eliciting valuation behaviour to study these signals, but as a key part of the modelling involved in this work, it’s important to get a handle on reinforcement learning.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-11-28T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2019-11-28T00:00:00&#43;00:00">
  

  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<script async type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <title>R-inforcement Learning Part One- Tic Tac Toe | Robert Hickman</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Robert Hickman</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        

        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#publications_selected">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/files/cv/cv.pdf">
            
            <span>CV</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <div class="article-inner">
      <h1 itemprop="name">R-inforcement Learning Part One- Tic Tac Toe</h1>

      

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2019-11-28 00:00:00 &#43;0000 UTC" itemprop="datePublished dateModified">
      Nov 28, 2019
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert Hickman">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    14 min read
  </span>
  

  
  

  
  
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=R-inforcement%20Learning%20Part%20One-%20Tic%20Tac%20Toe&amp;url=%2fpost%2fr-inforcement_learning_one%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpost%2fr-inforcement_learning_one%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2fr-inforcement_learning_one%2f&amp;title=R-inforcement%20Learning%20Part%20One-%20Tic%20Tac%20Toe"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpost%2fr-inforcement_learning_one%2f&amp;title=R-inforcement%20Learning%20Part%20One-%20Tic%20Tac%20Toe"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=R-inforcement%20Learning%20Part%20One-%20Tic%20Tac%20Toe&amp;body=%2fpost%2fr-inforcement_learning_one%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


      <div class="article-style" itemprop="articleBody">
        


<p>I’m extremely biased, but to me, one of the real success* stories in neuroscience over the last (just over) two decades has been in studying reward signals. Since the <a href="https://www.ncbi.nlm.nih.gov/pubmed/9054347">seminal 1997 paper</a>, a lot of work has gone into figuring out how the brain assigns value to outcomes.</p>
<p>*ugh, maybe. This isn’t a blog post about that</p>
<p>My PhD project looks at novel ways of eliciting valuation behaviour to study these signals, but as a key part of the modelling involved in this work, it’s important to get a handle on <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>. When originally working through the <a href="http://incompleteideas.net/book/the-book.html">Sutton and Barto books</a>, I threw together some code a few years ago for the problem sets- mostly in python and MATLAB. As someone who runs a blog nominally about coding in R, however, I thought there might be some value in going through said code and refactoring into R. As R can struggle with the speed necessary for reinforcement learning (which typically relies on large numbers of iterating behaviour), it also provided a good chance to crack out some C++ code using the always excellent <a href="http://adv-r.had.co.nz/Rcpp.html">Rcpp package</a>, which is always worth practicing.</p>
<p>In this first example of Reinforcement Learning in R (and C++), we’re going to train our computers to play Noughts and Crosses (or tic tac toe for Americans) to at least/super human level.</p>
<p>Let’s get started with the libraries we’ll need. I want to stick to base for speed here, as well as obviously Rcpp. In theory you can easily generalise all the code here to any size board, but I only have tested in with 3x3 boards so YMMV.</p>
<pre class="r"><code>#will use ggplot
#everything else Ive used base of listed packages
library(ggplot2)
#Rcpp for some vroom vroom
library(Rcpp)

#in theory this stuff should work for boards of any size
#but I haven&#39;t tested that
board_cols = 3
board_rows = 3
squares &lt;- board_cols * board_rows</code></pre>
<p>The very first thing we’ll want to do is find a way to store the information in a game state and convert between this, and a human readable form.</p>
<pre class="r"><code>#function to plot boards in a human readable way
#not generalised to all board sizes but easy enough to
plot_board &lt;- function(string) {
  pieced &lt;- rep(&quot;&quot;, length(string))
  pieced[which(string == 1)] &lt;- &quot;x&quot;
  pieced[which(string == -1)] &lt;- &quot;o&quot;
  pieced[which(string == 0)] &lt;- &quot;*&quot;
  board &lt;- gsub(&quot; \\|$&quot;, &quot;&quot;, paste(pieced, &quot;|&quot;, collapse = &quot; &quot;))
  board_lines &lt;- gsub(&quot;(. \\| . \\| . )\\|( . \\| . \\| . )\\|( . \\| . \\| .)&quot;, 
                      &quot;\n \\1\n-----------\n\\2\n-----------\n\\3&quot;,
                      board
                      )
  return(writeLines(board_lines))
}</code></pre>
<p>Next we’re going to want to find every possible state we might encounter so we can test for any exceptions. I’m storing strings as a list of 9 0s (unused), 1s (Xs) and -1s (Os) representing the squares 1-&gt;9 from the top left corner.</p>
<p>It’s simple and fast enough to do this with a quick R function</p>
<pre class="r"><code>#get all possible boards
possible_boards &lt;- gtools::permutations(
  board_cols, squares,
  c(-1,0,1),
  repeats.allowed = TRUE
  )

#can only have a sum of 1 or 0
possible_boards &lt;- possible_boards[which(rowSums(possible_boards) %in% c(0,1)),]

#plot a random example
plot_board(c(1,0,0,-1,0,0,0,0,1))</code></pre>
<pre><code>## 
##  x | * | * 
## -----------
##  o | * | * 
## -----------
##  * | * | x</code></pre>
<p>Now we have the representations of any possible board, we want to find a way to store this is a more compressed format as a hash. I originally wrote a pretty quick function to do this in R and then threw up a quick one underneath compiled in Rcpp for comparison.</p>
<pre class="r"><code>#get a unique hash for each board
calc_hash &lt;- function(board) {
  hash &lt;- 0
  for(piece in seq(squares)) {
    hash &lt;- (hash*board_cols) + board[piece] + 1
  }
  return(hash)
}

#and the equivalent in Cpp
cppFunction(&#39;int calc_hashCpp(NumericVector board, int squaresize) {
    //need to init vals in C++
    int hash = 0;
    int boardsize = squaresize * squaresize;
    
    //C++ for loops have start, end, and by
    for (int i=0; i &lt;= boardsize - 1; ++i) {
      hash = (hash * squaresize) + board[i] + 1;
    }
    
    //always have to declare a return
    return hash;
}&#39;)

#get a list of all the possible hashes
hashes &lt;- lapply(purrr::array_tree(possible_boards, margin = 1),
                 calc_hashCpp, squaresize = 3)

#should all be unique
which(duplicated(hashes))</code></pre>
<pre><code>## integer(0)</code></pre>
<p>In order to play noughts and crosses, we then need some way for a game to end. An easy way to check this is when our board string (0s,1s,and-1s) add up to 3/-3 along any row, column or diagonal.</p>
<pre class="r"><code>#first we need a function to check when a game has been won
cppFunction(&#39;int check_winnerCpp(NumericVector board) {
  int winner = 0;

  int vec_length = board.size();
  int square_size = sqrt(vec_length);

  //check rows and columns for a winner
  for (int i=0; i &lt;= square_size - 1; ++i) {
    //check row i
    NumericVector row_squares = NumericVector::create(0,1,2);
    row_squares = row_squares + (square_size * i);
    NumericVector row_elements = board[row_squares];
    int row_sum = sum(row_elements);
    if(abs(row_sum) == square_size) {
      if(row_sum &gt; 0) {
        winner = 1;
      } else {
        winner = -1;
      }
    }
    
    //check col i
    NumericVector col_squares = NumericVector::create(0,3,6);
    col_squares = col_squares + i;
    NumericVector col_elements = board[col_squares];
    int col_sum = sum(col_elements);
    if(abs(col_sum) == square_size) {
      if(col_sum &gt; 0) {
        winner = 1;
      } else {
        winner = -1;
      }
    }
  }
  
  //check the diagonalsNumericVector 
  NumericVector rising_diag_squares = NumericVector::create();
  NumericVector falling_diag_squares = NumericVector::create();
  for (int i=0; i &lt;= square_size - 1; ++i) {
    int rising_diag_square = (square_size * i) + i;
    rising_diag_squares.push_back(rising_diag_square);
    
    int falling_diag_square = (square_size - 1) * (i+1);
    falling_diag_squares.push_back(falling_diag_square);
  }
  
  NumericVector rising_diag_elements = board[rising_diag_squares];
  NumericVector falling_diag_elements = board[falling_diag_squares];
  int rising_sum = sum(rising_diag_elements);
  int falling_sum = sum(falling_diag_elements);
  
  if(abs(falling_sum) == square_size) {
    if(falling_sum &gt; 0) {
      winner = 1;
    } else {
      winner = -1;
    }
  }
  if(abs(rising_sum) == square_size) {
    if(rising_sum &gt; 0) {
      winner = 1;
    } else {
      winner = -1;
    }
  }
    
  //return the winner
  //0 for no winner, 999 for draw
  return winner;
}&#39;)</code></pre>
<p>We can then apply this function to every possible board and find the ones that indicate a winning state. We also init a data frame containing all possible boards, their hash, and their ‘value’ (0 for all for now, more on this later). Finally, I plot the first one in this set just because why not?</p>
<pre class="r"><code>#find which boards are winning positions
winning &lt;- purrr::map(purrr::array_tree(possible_boards, margin = 1), check_winnerCpp)

#going to create a df to store the values of all moves
moves_df &lt;- data.frame(hash = unlist(hashes),
                       value = 0,
                       winning = unlist(winning))

#store all boards as a list
#purrr::aray_tree is a really nice way to convert matrix to lists
moves_df$board = purrr::array_tree(possible_boards, margin = 1)

#plot the first board just why not
plot_board(unlist(moves_df$board[1]))</code></pre>
<pre><code>## 
##  o | o | o 
## -----------
##  o | * | x 
## -----------
##  x | x | x</code></pre>
<p>As we can see, we still have some impossible boards here. This particular board will never occur in actual play because X wins before O can make a move to complete the top row. It doesn’t matter, but useful to keep in mind for a plot later.</p>
<p>We then need a function telling the computer how to make a move. For this post we’re going to use what’s called ‘E (epsilon)-greedy’ selection. A computer has a parameter epsilon such that</p>
<p><span class="math display">\[\begin{cases}
    v &amp;\text{if } \epsilon \leq \rho\\
    V_{max} &amp;\text{if } \epsilon &gt; \rho\\
\end{cases} \]</span></p>
<p>if epsilon is greater than a random number rho, the computer makes the most valuable choice possible. It chooses whatever it thinks (rightly or wrongly) will lead to the best outcome. This is called <em>exploitation</em>. If epsilon is less than or equal to rho, the computer randomly chooses any possible action v. This is known as <em>exploration</em> to test any possibly rewarding but unvalued paths.</p>
<p>(I may have gotten epsilon the wrong way round here. It really doesn’t matter at all.)</p>
<p>Let’s implement this in C++</p>
<pre class="r"><code>cppFunction(&#39;int choose_moveCpp(NumericVector epsilon, NumericVector values) {
  //random number to decide if computer should explore or exploit
  NumericVector random_number = runif(1);
  int move_choice = 0;
  NumericVector choices = NumericVector::create();
  
  //exploit the best move
  if(epsilon[0] &gt; random_number[0]) {
    double max = Rcpp::max(values);
    std::vector&lt; int &gt; res;
    
    int i;
    for(i = 0; i &lt; values.size(); ++i) {
      if(values[i] == max) { 
        res.push_back(i);
      }
    }
    IntegerVector max_indexes(res.begin(), res.end());
    if(max_indexes.size() &gt; 1) {
      std::random_shuffle(max_indexes.begin(), max_indexes.end());
      move_choice = max_indexes[0] + 1;
    } else {
      move_choice = max_indexes[0] + 1;
    }
   //explore all moves randomly
  } else {
    int potential_choices = values.size();
    choices = seq(1, potential_choices);
    std::random_shuffle(choices.begin(), choices.end());
    move_choice = choices[0];
  }
  
  return move_choice;
}&#39;)</code></pre>
<p>We also want a little helper func to find all the possible hashes so we can look up which moves a computer can make before choosing between them.</p>
<pre class="r"><code>#find all possible next moves
get_next_hashes &lt;- function(board, piece) {
  unused &lt;- which(board == 0)
  
  next_boards &lt;- lapply(unused, function(x, piece) {
    board[x] &lt;- piece
    return(board)
  }, piece = piece)
  #get the hashes of the next boards
  hashes &lt;- lapply(next_boards, calc_hashCpp, squaresize = 3)
}</code></pre>
<p>Finally, we need to reward the computer for making good actions, and punish it for making bad ones. We’ll do this using Temporal Difference (TD) error learning.</p>
<p>The computer looks at how good an end point was (for noughts and crosses this can be a win, lose, or draw) and then decides if that outcome is better or worse than it was expecting. It then re-evaluates its beliefs about the choices it made to lead to that end state. It can be formulated as</p>
<p><span class="math display">\[V_{state} = V_{state} + TD  error \cdot scalar \]</span>
the scalar here is the <em>learning rate</em> of the computer. Do we want it to forget everything it new about the world seconds earlier and take only the most recent information (1), or update it’s beliefs very slowly (~0). We’ll refer to this as lr in subsequent equations.</p>
<p>The TD error itself is calculated as</p>
<p><span class="math display">\[TD error = (\gamma \cdot reward - V_{state}) \]</span>
Where gamma acts to make sure we don’t overfit too far back into the past. It reduces the reward as you go further back and is set between 0 and 1. The reward here will (e.g.) be 1 if the computer has just won with it’s latest move, otherwise it will be the value of the state the computer might move into.</p>
<p>Putting these together we get</p>
<p><span class="math display">\[ V_{state} = V_{state} + lr \cdot (\gamma \cdot V_{state+1} - V_{state}) \]</span></p>
<p>Let’s implement this using Rcpp</p>
<pre class="r"><code>#function to feed reward back to the agent based on results
cppFunction(&#39;NumericVector backfeed_rewardCpp(NumericVector values, double reward, double learning_rate, double gamma) {
  int states = values.size();
  NumericVector new_values = NumericVector::create();
  
  //go from last state backwards
  for( int state = states-1; state &gt;= 0; state--) {
    double new_value = values[state] + learning_rate * ((gamma * reward) - values[state]);
    new_values.push_back(new_value);
    //recurse the reward
    reward = new_value;
  }
  return new_values;
}&#39;)</code></pre>
<p>Now we can start actually playing games! I wrote out a long function in R to play through the various bits. It surely could be refactored a little more concisely but it works for now and I was getting tired by this point.</p>
<p>We first add two functions (one to make moves/play the game, and one to update the values using the formula above) then put it all into to one uber-function</p>
<pre class="r"><code>#function to choose and implement computer moves
computer_move &lt;- function(piece, board, epsilon) {
  #get potential moves
  potential_move_hashes &lt;- get_next_hashes(board, piece)
  #get the values of the potential moves
  potential_move_vals &lt;- moves_df$value[
    unlist(lapply(potential_move_hashes, function(x) which(moves_df$hash == x)))]
  #choose move based on rewards
  player_move &lt;- choose_moveCpp(epsilon, potential_move_vals)
  #update the board with the new move
  updated_board &lt;- unlist(moves_df$board[
    moves_df$hash == unlist(potential_move_hashes)[player_move]])
  return(updated_board)
}

#function to get the values for each state based on the reward
update_move_vals &lt;- function(player1_reward, player2_reward, 
                             player1_hashes, player2_hashes,
                             learning_rate,gamma) {
  player1_newvals &lt;- backfeed_rewardCpp(moves_df$value[
    unlist(lapply(player1_hashes, function(x) which(moves_df$hash == x)))],
    player1_reward, learning_rate, gamma)
  player2_newvals &lt;- backfeed_rewardCpp(moves_df$value[
    unlist(lapply(player2_hashes, function(x) which(moves_df$hash == x)))],
    player2_reward, learning_rate, gamma)
  new_vals &lt;- list(player1_newvals, player2_newvals)
  return(new_vals)
}

#function to get two computers to play each other
play_game_computers &lt;- function(player1_epsilon, 
                                player2_epsilon,
                                learning_rate, gamma) {
  #init board
  board &lt;- rep(0, squares)
  winner &lt;- 0
  moves &lt;- 0
  #init hash storage
  player1_hashes &lt;- c()
  player2_hashes &lt;- c()
  
  #keep moving until game is over
  while(winner == 0 &amp; moves &lt; 9) {
    #iterate moves
    moves &lt;- moves + 1
    #player 1 moves
    board &lt;- computer_move(1, board, player1_epsilon)
    player1_hashes &lt;- append(calc_hashCpp(board, board_cols), player1_hashes)
    winner &lt;- check_winnerCpp(board)
    
    #same for player 2
    if(winner == 0  &amp; moves &lt; 9) {
      moves &lt;- moves + 1
      board &lt;- computer_move(-1, board, player1_epsilon)
      player2_hashes &lt;- append(calc_hashCpp(board, board_cols), player2_hashes)
      winner &lt;- check_winnerCpp(board)
    }
  }
  
  #update policies
  if(winner == 1) {
    message &lt;- &quot;x wins!&quot;
    new_vals &lt;- update_move_vals(1, 0, player1_hashes, player2_hashes,
                                 learning_rate, gamma)
  } else if(winner == -1) {
    message &lt;- &quot;o wins!&quot;
    new_vals &lt;- update_move_vals(0, 1, player1_hashes, player2_hashes, 
                                 learning_rate, gamma)
  } else {
    message &lt;- &quot;draw!&quot;
    new_vals &lt;- update_move_vals(0.1, 0.5, player1_hashes, player2_hashes, learning_rate, gamma)
  }
  #push the values back into the dictionary data frame
  moves_df$value[unlist(lapply(player1_hashes, function(x) which(moves_df$hash == x)))] &lt;&lt;- new_vals[[1]]
  moves_df$value[unlist(lapply(player2_hashes, function(x) which(moves_df$hash == x)))] &lt;&lt;- new_vals[[2]]
  return(message)
}</code></pre>
<p>So that the computer can learn the value of moves, we first want to run this on a training epoch. We’ll get the computer to play 100000 games against itself with an epsilon &lt; 1 so that it explores the game state and learns by reinforcement. We’ll then plot the values it’s learn for all moves based upon if they are winning or not.</p>
<pre class="r"><code>#test on 10000 games with a little randomness thrown in
train &lt;- purrr::rerun(100000, play_game_computers(0.8, 0.8, 0.35, 0.9))

#test how fast our function is
microbenchmark::microbenchmark(play_game_computers(0.8, 0.8, 0.35, 0.9), times = 1000)</code></pre>
<pre><code>## Unit: microseconds
##                                      expr   min      lq     mean median
##  play_game_computers(0.8, 0.8, 0.35, 0.9) 838.7 1061.05 1352.258 1222.2
##       uq    max neval
##  1361.45 4548.4  1000</code></pre>
<pre class="r"><code>#plot the updated values of moves
p1 &lt;- ggplot(moves_df, aes(x = value, group = as.character(winning))) +
  geom_density(alpha = 0.5, aes(fill = as.character(winning))) +
  scale_fill_manual(values = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;), name = &quot;winning move&quot;) +
  theme_minimal()
p1</code></pre>
<p><img src="/post/2019-11-28-Rinforcement_learning_part_1_files/figure-html/train_computer-1.png" width="672" /></p>
<p>Thankfully the computer has learned that winning moves are more valuable than non-winning moves! The reason there are peaks at 0 is because these are ‘winning’ moves that are impossible as referenced nearer the top of the post.</p>
<p>We’ll then run 2500 testing games where the computer is trying to play optimally. Noughts and crosses is a <a href="https://en.wikipedia.org/wiki/Solved_game">solved</a> game. Unless a play chooses a non-optimal move, the game should end in a draw. Let’s see what proportion actually do end in a draw by grouping every 500 games of the testing set.</p>
<pre class="r"><code>#run on an extra 2500 games with no exploration (just exploit)
test &lt;- purrr::rerun(2500, play_game_computers(1, 1, 0.35, 0.9))

#group by each 500 games
test_df &lt;- data.frame(result = unlist(test),
                      group = rep(1:5, each = 500))

#plot percentage of games that are drawn
p2 &lt;- ggplot(test_df, aes(x = group, fill = result)) +
  geom_bar(stat = &quot;count&quot;) +
  labs(x = &quot;group (every 500 games)&quot;) +
  theme_minimal()
p2</code></pre>
<p><img src="/post/2019-11-28-Rinforcement_learning_part_1_files/figure-html/test_computer-1.png" width="672" /></p>
<p>And it seems like the computer learns after a final bit of optimisation to always draw! hooray!!</p>
<p>Finally, because obviously this post wouldn’t be complete without human testing, I wrote a quick and dirty function to play a game against the now proficient computer. Enjoy below!!</p>
<pre class="r"><code>player_move &lt;- function(board){
  #find free spaces a move can be made into
  free_spaces &lt;- which(board == 0)
  cat(&quot;Please move to one of the following board spaces: [&quot;, free_spaces,  &quot;]\n&quot;)
  #user input
  submitted_move &lt;- as.integer(readline(prompt = &quot;&quot;))
  #need valid input
  while(!submitted_move %in% free_spaces) {
    if(submitted_move == 0) {
      break
    } else {
      cat(&quot;Illegal Move! Please move to one of the following board spaces: [&quot;, free_spaces,  &quot;] or press 0 to quit\n&quot;)
      submitted_move &lt;- as.integer(readline(prompt = &quot;&quot;))
    }
  }
  #return move
  return(submitted_move)
}</code></pre>
<pre class="r"><code>#only need a computer epsilon and which piece (turn order)
play_game_human &lt;- function(human_piece, computer_epsilon = 1) {
  board &lt;- rep(0, 9)
  moves &lt;- 0
  winner &lt;- 0
  
  #play the game as before but with a human player
  if (human_piece == 1) {
    while (winner == 0 &amp; moves &lt; 9) {
      moves &lt;- moves + 1
      plot_board(board)
      human_move &lt;- player_move(board)
      
      if (human_move == 0) {
        break
      } else {
        board[human_move] &lt;- human_piece
      }
      i &lt;&lt;- board
      j &lt;&lt;- board
      winner &lt;- check_winnerCpp(board)
      if (winner == 0 &amp; moves &lt; 9) {
        moves &lt;- moves + 1
        piece &lt;- human_piece * -1

        board &lt;- computer_move(-1, board, computer_epsilon)
        winner &lt;- check_winnerCpp(board)
      }
    }
  } else {
    while (winner == 0 &amp; moves &lt; 9) {
      moves &lt;- moves + 1
      piece &lt;- human_piece * -1

      board &lt;- computer_move(-1, board, player1_epsilon)
      winner &lt;- check_winnerCpp(board)

      if (winner == 0 &amp; moves &lt; 9) {
        moves &lt;- moves + 1
        plot_board(board)
        human_move &lt;- player_move(board)
        if (human_move == 0) {
          break
        } else {
          board[human_move] &lt;- human_piece
        }
        winner &lt;- check_winnerCpp(board)
      }
    }
  }
  #little ending flavour
  if (winner == human_piece) {
    print(&quot;you win!!&quot;)
  } else if(winner == -human_piece) {
    print(&quot;oh no! you lost!&quot;)
  } else {
    print(&quot;a draw..&quot;)
  }
  plot_board(board)
}

#run like:
play_game_human(1, 1)</code></pre>

      </div>

      


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/rstats">rstats</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/rcpp">rcpp</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/reinforcement_learning">reinforcement_learning</a>
  
</div>



    </div>
  </div>

</article>



<div class="article-container article-widget">
  <div class="hr-light"></div>
  <h3>Related</h3>
  <ul>
    
    <li><a href="/post/cam_rowing_1/">Predicting the Unpredictable- Analysing Rowing in Cambridge pt. 1</a></li>
    
    <li><a href="/post/guardian_knowledge_june/">The Guardian Knowledge June 2019</a></li>
    
    <li><a href="/post/dynamic_web_scraping/">Scraping Dynamic Websites with PhantomJS</a></li>
    
    <li><a href="/post/riddler-1st-feb-2019/">Riddler 1st February 2019</a></li>
    
    <li><a href="/post/counties_league_points/">Which English County Has Won the Most Points</a></li>
    
  </ul>
</div>




<div class="article-container">
  

</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

