<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>maps on Robert Hickman</title>
    <link>/tags/maps/</link>
    <description>Recent content in maps on Robert Hickman</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Thu, 09 Jan 2020 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/maps/" rel="self" type="application/rss+xml" />
    
    <item>
      <title></title>
      <link>/post/from_hackney_to_haringey/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/from_hackney_to_haringey/</guid>
      <description>


&lt;p&gt;A perculiar way in which the UK’s constituency-based electoral system shapes media coverage is that the names of certain towns/districts have an outsized effect. For instance, in the 2019 UK general election, much was made of &lt;a href=&#34;https://www.newstatesman.com/politics/uk/2019/10/which-voters-who-swing-election-2019-workington-man&#34;&gt;Workington&lt;/a&gt; &lt;a href=&#34;https://www.dailymail.co.uk/news/article-7640915/Workington-Man-backing-Boris-Johnson-Survey-finds-Tories-course-win-key-Labour-seat.html&#34;&gt;Man&lt;/a&gt;* in Cumbria- a seat that had fairly consistently returned Labour MPs in the modern era.&lt;/p&gt;
&lt;p&gt;One particular media trend made possible by the variety of names for UK seats is to alliterate between constituencies that are seen as showing a range of geography/opinion/etc. This is best summed up in a great exchange between the absolute boy, and Health Secretary, Matt Hancock, and Kay Burley:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en-gb&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
On Sky, Matt Hanock says new cancer treatments are being rolled out “from Barnsley to Bassetlaw; from Wigan to Warrington.”&lt;br&gt;&lt;br&gt;Kay Burley: “That&#39;s not very far, you know.”&lt;br&gt;&lt;br&gt;Hancock: “It&#39;s also happening in Cornwall.”
&lt;/p&gt;
— Peter Walker (&lt;span class=&#34;citation&#34;&gt;@peterwalker99&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/peterwalker99/status/1189443331237003265?ref_src=twsrc%5Etfw&#34;&gt;30 October 2019&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;Given that I had an afternoon off sick from work, and enjoy wasting my time on such things, I wanted to see what the best constituencies to use for ‘From Xx to Xy’ in British politics is. For this I’m going to use mostly data that is hosted on this website, however, where it isn’t I’ve made it pretty clear in comments where it can be downlaoded.&lt;/p&gt;
&lt;p&gt;*for a good take on this, see &lt;a href=&#34;https://www.theguardian.com/commentisfree/2019/nov/22/workington-man-voter-caricature-essex-man&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile 
## Source: &amp;quot;C:\Users\rob-getty\Desktop\cleanup desktop\geo_data\boundary\Data\GB&amp;quot;, layer: &amp;quot;westminster_const_region&amp;quot;
## with 632 features
## It has 15 fields
## Integer64 fields read as strings:  NUMBER NUMBER0 POLYGON_ID UNIT_ID&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, and easiest, let’s start with geographic distances between constituencies. For this I use the Ordnance Survey &lt;a href=&#34;https://www.ordnancesurvey.co.uk/business-government/products/boundaryline&#34;&gt;boundary line&lt;/a&gt; dataset which gives shapefile of each constituency in the UK.&lt;/p&gt;
&lt;p&gt;After some string regex to match names between datasets, I also removed all constituencies beginning with North/South/East/West (as ‘From East Surry to East Hampshire’ doesn’t really have a ring to it) and also only took seats within England or Wales (more on why later). I also took out constituency names that were longer than two words, again for stylistic reasons.&lt;/p&gt;
&lt;p&gt;We’re then left with 350 (out of 650 total) seats which we can plot, filled by the first letter of their name.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load tidyverse for munging
library(tidyverse)

#this data can be found at https://www.ordnancesurvey.co.uk/business-government/products/boundaryline
#open as:
# constituency_shapefiles &amp;lt;- readOGR(dsn = &amp;quot;where/you/downloaded&amp;quot;, 
#                                    layer = &amp;quot;westminster_const_region&amp;quot;)
#using rgdal and sf for geospatial work
library(rgdal)
library(sf)

constituency_geography &amp;lt;- constituency_shapefiles %&amp;gt;%
  st_as_sf() %&amp;gt;%
  #some munging to line up datasets
  mutate(name = gsub(&amp;quot; Co Const| Burgh Const| Boro Const&amp;quot;, &amp;quot;&amp;quot;, NAME)) %&amp;gt;%
  mutate(name = case_when(
    grepl(&amp;quot;London and Westminster&amp;quot;, name) ~ &amp;quot;Cities of London and Westminster&amp;quot;,
    grepl(&amp;quot;-.*-&amp;quot;, name) ~ gsub(&amp;quot;(-)([a-z]{1})(.*-)&amp;quot;, perl = TRUE, &amp;quot;\\1\\U\\2\\E\\3&amp;quot;, name),
    grepl(&amp;quot;St\\. &amp;quot;, name) ~ gsub(&amp;quot;St\\. &amp;quot;, &amp;quot;St &amp;quot;, name),
    grepl(&amp;quot; of &amp;quot;, name) ~ gsub(&amp;quot; of &amp;quot;, &amp;quot; Of &amp;quot;, name),
    grepl(&amp;quot;Newcastle upon &amp;quot;, name) ~ gsub(&amp;quot; upon &amp;quot;, &amp;quot; Upon &amp;quot;, name),
    TRUE ~ name
  )) %&amp;gt;%
  #get the first letter
  #removing compass directions
  filter(!grepl(&amp;quot;North |East |South |West &amp;quot;, name) &amp;amp; !grepl(&amp;quot; .* &amp;quot;, name)) %&amp;gt;%
  mutate(first_letter = gsub(&amp;quot;(.)(.*)&amp;quot;, &amp;quot;\\1&amp;quot;,  name)) %&amp;gt;%
  #only going to play with English constituencies here
  filter(grepl(&amp;quot;^E|^W&amp;quot;, CODE)) %&amp;gt;%
  #remove Chorley (speaker&amp;#39;s seat)
  filter(!grepl(&amp;quot;Chorley&amp;quot;, name)) %&amp;gt;%
  #select and rename relevant columns
  select(WSTid = CODE, WSTnm = name, first_letter)

constituency_names &amp;lt;- constituency_geography %&amp;gt;%
  `st_geometry&amp;lt;-`(NULL)

#ggthemes for map theme
library(ggthemes)

#plot remaining constituencies coloured based on first letter
first_letter_plot &amp;lt;- ggplot() +
  geom_sf(data = constituency_geography, aes(fill = first_letter)) +
  scale_fill_discrete(guide = FALSE) +
  theme_map()

plot(first_letter_plot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-09-from_hackney_to_haringey_files/figure-html/get_data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To calculate the distance between any two constituencies, I use the center location of each, calculated using sf::st_centroid(). Grouping by first letter then creating a matrix from each to each is simple enough using sf::st_distance() as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get the coordinates of the center of each constituency
geographic_centers &amp;lt;- constituency_geography %&amp;gt;%
  st_centroid() %&amp;gt;%
  split(f = .$first_letter)

#function to find distances between center points
get_distances &amp;lt;- function(letter_list) {
  constituencies &amp;lt;- letter_list$WSTnm
  first_letter &amp;lt;- unique(letter_list$first_letter)
  
  #get distance to/from every center point with same first letter
  distance_matrix &amp;lt;- st_distance(letter_list, letter_list)
  
  distances_df &amp;lt;- distance_matrix %&amp;gt;%
    as.data.frame()
  names(distances_df) &amp;lt;- constituencies
  
  melted_df &amp;lt;- distances_df %&amp;gt;%
    pivot_longer(., names(.), names_to = &amp;quot;to&amp;quot;, values_to = &amp;quot;distance&amp;quot;) %&amp;gt;%
    mutate(from = rep(unique(to), each = length(unique(to)))) %&amp;gt;%
    mutate(first_letter = first_letter)
  
  return(melted_df)
}

#run the function to get the distances between constituencies with same
#first letter
constituency_interdistances &amp;lt;- map_df(geographic_centers, get_distances)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then find the longest distance (in metres) between the centre of constituencies, grouped by the first letter of their name using some simple muning:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the longest distances 
longest_distances &amp;lt;- constituency_interdistances %&amp;gt;%
  #arrange by longest first
  arrange(-distance) %&amp;gt;%
  #take the longest per first letter
  filter(!duplicated(first_letter)) %&amp;gt;%
  filter(distance != 0)

#show the ongest 10 interdistances
head(longest_distances %&amp;gt;% arrange(-distance))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   to                 distance from               first_letter
##   &amp;lt;chr&amp;gt;                 &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt;       
## 1 St Ives             601117. Sunderland Central S           
## 2 Tynemouth           540605. Totnes             T           
## 3 Berwick-Upon-Tweed  528160. Brighton, Kemptown B           
## 4 Worthing West       490544. Wansbeck           W           
## 5 Hove                488181. Hexham             H           
## 6 Carlisle            483127. Canterbury         C&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perhaps not surprisingly, St Ives (in the far South West on England) to Sunderland Central (in the far North East) is the furthest distance (601km). We can see though that there’s a fair few first letter for which we have a pair of constituencies that are pretty far away from each other.&lt;/p&gt;
&lt;p&gt;To plot the longest distance between a pair of constituencies that alliterate is simple enough. I also load a shapefile of the outline of England and Wales to pretty up the plots and create lines between each constituency. Where constituencies are too small to be plotted on this scale, I use a red dot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#shapefile of England and Wales for plotting
eng_wal &amp;lt;- &amp;quot;C:/Users/rob-getty/Desktop/netlify_blog/static/files/constituency_distances/england_wales_shape.rds&amp;quot; %&amp;gt;%
  readRDS()

#filter the longest journey per letter
selected_constituencies &amp;lt;- constituency_geography %&amp;gt;%
  filter(WSTnm %in% pivot_longer(longest_distances, cols = c(&amp;quot;to&amp;quot;, &amp;quot;from&amp;quot;))$value) %&amp;gt;%
  left_join(., 
            longest_distances %&amp;gt;% 
              mutate(journey = paste(to, from, sep = &amp;quot; to\n&amp;quot;)) %&amp;gt;% 
              select(first_letter, journey),
            by = &amp;quot;first_letter&amp;quot;)

#get the center coordinates of constituencies
#to help plotting small constituencies
plotting_points &amp;lt;- do.call(rbind, geographic_centers) %&amp;gt;%
  filter(WSTnm %in% selected_constituencies$WSTnm) %&amp;gt;%
  left_join(., 
            longest_distances %&amp;gt;% 
              mutate(journey = paste(to, from, sep = &amp;quot; to\n&amp;quot;)) %&amp;gt;% 
              select(first_letter, journey),
            by = &amp;quot;first_letter&amp;quot;) %&amp;gt;%
  st_transform(crs = st_crs(eng_wal))

#calculate straight lines between two constituencies
plotting_lines &amp;lt;- plotting_points %&amp;gt;%
  split(f = .$journey) %&amp;gt;%
  map_df(., function(data) {
    coords &amp;lt;- rbind(st_coordinates(data[1,]), st_coordinates(data[2,]))
    line &amp;lt;- st_linestring(coords)
    df &amp;lt;- st_sfc(line, crs = st_crs(&amp;quot;+init=epsg:27700&amp;quot;)) %&amp;gt;%
      as.data.frame() %&amp;gt;%
      mutate(journey = unique(data$journey))
  }) %&amp;gt;%
  st_as_sf(crs = st_crs(plotting_points))

#plot the longest journey between constituencies with the same first letter
alliterative_journeys_plot &amp;lt;-  ggplot() +
  geom_sf(data = eng_wal, fill = &amp;quot;white&amp;quot;) +
  geom_sf(data = plotting_lines, colour = &amp;quot;darkblue&amp;quot;) +
  #some constituencies are too small to plot as shapefiles
  geom_sf(data = plotting_points, colour = &amp;quot;red&amp;quot;, size = 2.5) +
  geom_sf(data = selected_constituencies, fill = &amp;quot;red&amp;quot;) +
  theme_map() +
  #split by first letter
  facet_wrap(~journey)

plot(alliterative_journeys_plot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-09-from_hackney_to_haringey_files/figure-html/plot_interdistances-1.png&#34; width=&#34;1344&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load data on voting in the 2019 general election
#2016 brexit vote based on Hanretty work also included
votes_data &amp;lt;- readRDS(&amp;quot;../../static/files/constituency_distances/ge2019_results.rds&amp;quot;) %&amp;gt;%
  select(WSTnm = constituency_name, winner = first_party, votes = electorate, 
         con, lab, ld, brexit, green, other, brexit_hanretty) %&amp;gt;%
  #convert to vote fractions
  modify_at(c(&amp;quot;con&amp;quot;, &amp;quot;lab&amp;quot;, &amp;quot;ld&amp;quot;, &amp;quot;brexit&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;other&amp;quot;), function(x) x/.$votes) %&amp;gt;%
  #take only relevant constituencies
  filter(WSTnm %in% constituency_geography$WSTnm)

head(votes_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 WSTnm winner votes       con       lab         ld
## 1            Aberavon    Lab 50750 0.1284335 0.3351330 0.02112315
## 2           Aberconwy    Con 44699 0.3285756 0.2830712 0.04073917
## 3           Aldershot    Con 72617 0.3853092 0.1553631 0.09529449
## 4 Aldridge-Brownhills    Con 60138 0.4631015 0.1332602 0.03942599
## 5        Amber Valley    Con 69976 0.4157997 0.1744884 0.04105693
## 6               Arfon     PC 42215 0.1048916 0.2452446 0.00000000
##       brexit       green      other brexit_hanretty
## 1 0.06124138 0.008866995 0.01440394       0.6012448
## 2 0.00000000 0.000000000 0.00000000       0.5219712
## 3 0.00000000 0.024099040 0.00000000       0.5789777
## 4 0.00000000 0.012820513 0.00558715       0.6779635
## 5 0.00000000 0.019835372 0.00000000       0.6529912
## 6 0.02745470 0.000000000 0.00000000       0.3584544&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the largest gap in 2016 brexit vote between constituencies
#which same first letter
brexit_differences &amp;lt;- votes_data %&amp;gt;%
  left_join(., constituency_names, by = &amp;quot;WSTnm&amp;quot;) %&amp;gt;%
  split(f = .$first_letter) %&amp;gt;%
  map_df(., function(data) {
    difference &amp;lt;- outer(data$brexit_hanretty, data$brexit_hanretty, &amp;quot;-&amp;quot;) %&amp;gt;%
      as.data.frame() %&amp;gt;%
      mutate(from = data$WSTnm)
    names(difference)[1:(ncol(difference) - 1)] &amp;lt;- data$WSTnm
    df &amp;lt;- difference %&amp;gt;%
      pivot_longer(cols = -starts_with(&amp;quot;from&amp;quot;),
                   names_to = &amp;quot;to&amp;quot;,
                   values_to = &amp;quot;brexit_2016_difference&amp;quot;) %&amp;gt;%
      mutate(first_letter = unique(data$first_letter))
  }) %&amp;gt;%
  #arrange by greatest difference
  arrange(-brexit_2016_difference)

head(brexit_differences, n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 4
##    from                 to           brexit_2016_difference first_letter
##    &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;                         &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       
##  1 Stoke-On-Trent North Streatham                     0.516 S           
##  2 Stoke-On-Trent South Streatham                     0.506 S           
##  3 Barnsley East        Bristol West                  0.503 B           
##  4 Bolsover             Bristol West                  0.497 B           
##  5 Barnsley East        Battersea                     0.489 B           
##  6 Bolsover             Battersea                     0.483 B           
##  7 Scunthorpe           Streatham                     0.481 S           
##  8 Bassetlaw            Bristol West                  0.476 B           
##  9 Barnsley Central     Bristol West                  0.475 B           
## 10 Blackpool South      Bristol West                  0.471 B&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot the distances between constituencies of same first letter with
#greatest difference in 2016 brexit vote
brexit_distance_plot &amp;lt;- brexit_differences %&amp;gt;%
  filter(!duplicated(first_letter)) %&amp;gt;%
  left_join(constituency_interdistances, by = c(&amp;quot;to&amp;quot;, &amp;quot;from&amp;quot;)) %&amp;gt;%
  mutate(journey = paste(to, from, sep = &amp;quot;-&amp;quot;)) %&amp;gt;%
  select(journey, first_letter.x, brexit_2016_difference, distance) %&amp;gt;%
  ggplot(aes(x = brexit_2016_difference, y = distance, label = journey)) +
  geom_text() +
  theme_minimal()

brexit_distance_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-09-from_hackney_to_haringey_files/figure-html/plot_brexit_differences-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load the raw values from the census data for each output area
census_oa_data &amp;lt;- census_oa_data %&amp;gt;%
  #select only integer data (counts not percentages)
  select(OAid = GeographyCode, which(sapply(.,class)==&amp;quot;integer&amp;quot;))

#load the lookup between output areas to westminster constituency
oa_to_westminster &amp;lt;- readRDS(&amp;quot;../../static/files/constituency_distances/oa_to_westminster.rds&amp;quot;) %&amp;gt;%
  select(OAid = OA11CD, WSTid = PCON11CD, WSTnm = PCON11NM, WSTperc = OA11PERCENT) %&amp;gt;%
  #select only english constituencies
  filter(WSTid %in% constituency_names$WSTid)


#gather the census data by westminster constituency
census_data_westminster &amp;lt;- left_join(census_oa_data, oa_to_westminster, by = &amp;quot;OAid&amp;quot;) %&amp;gt;%
  filter(!is.na(WSTid)) %&amp;gt;%
  #for output areas split between constituencies guesstimate the correct amounts
  mutate_if(is.integer, funs(round(. * (WSTperc/100)))) %&amp;gt;%
  select(-WSTnm, -WSTperc, -OAid) %&amp;gt;%
  #sum the counts per constituency for each statistic
  group_by(WSTid) %&amp;gt;%
  summarise_if(is.numeric, sum, na.rm = TRUE) %&amp;gt;%
  #turn into percentages from the total number of people (KS101)
  modify_if(is.numeric, function(x) x/.$KS101EW0001) %&amp;gt;%
  #arrange by name
  arrange(WSTid)

#only preview the first few columns as we have ~400 total
head(census_data_westminster[1:8])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
##   WSTid KS101EW0001 KS101EW0002 KS101EW0003 KS101EW0004 KS101EW0005
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 E140~           1       0.499       0.501       0.980     0.0195 
## 2 E140~           1       0.489       0.511       0.994     0.00574
## 3 E140~           1       0.492       0.508       0.991     0.00885
## 4 E140~           1       0.492       0.508       0.989     0.0111 
## 5 E140~           1       0.485       0.515       0.992     0.00791
## 6 E140~           1       0.492       0.508       0.992     0.00766
## # ... with 2 more variables: KS101EW0006 &amp;lt;dbl&amp;gt;, KS102EW0001 &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load the terminology for each census statistic
census_index &amp;lt;- readRDS(&amp;quot;../../static/files/constituency_distances/census_names.rds&amp;quot;)

head(select(census_index, Code, Meaning))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Code
## 2 KS101EW0001
## 3 KS101EW0002
## 4 KS101EW0003
## 5 KS101EW0004
## 6 KS101EW0005
## 7 KS101EW0006
##                                                                           Meaning
## 2                                                             All categories: Sex
## 3                                                                           Males
## 4                                                                         Females
## 5                                                            Lives in a household
## 6                                               Lives in a communal establishment
## 7 Schoolchild or full-time student aged 4 and over at their non term-time address&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census_data_westminster &amp;lt;- census_data_westminster %&amp;gt;%
  #a few codes missing from the index
  .[-which(!names(.)[2:ncol(.)] %in% census_index$Code)] %&amp;gt;%
  .[c(1, which(!apply(.[2:ncol(.)],2,function(x) var(x,na.rm=T)==0))+1)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_correlations_tidy &amp;lt;- function(demography, dependent_var) {
  #first split up the demography data by variable so we can independently
  #correlate each against the dependent variable
  split_demog &amp;lt;- demography %&amp;gt;%
    column_to_rownames(&amp;quot;WSTid&amp;quot;) %&amp;gt;%
    t() %&amp;gt;%
    split(f = rownames(.))
  
  #run the values for each variables against the dependent_var
  correlations &amp;lt;- map_df(split_demog, function(x) {
    regression &amp;lt;- lm(dependent_var ~ x)
    adj_r_squared &amp;lt;- summary(regression)$adj.r.squared
    f_stat &amp;lt;- summary(regression)$fstatistic[1]
    df &amp;lt;- summary(regression) %&amp;gt;%
      #tidy it to bind to df
      broom::tidy() %&amp;gt;%
      filter(term != &amp;quot;(Intercept)&amp;quot;) %&amp;gt;%
      mutate(adj_r = adj_r_squared, f_stat)
  })
  
  tidy_df &amp;lt;- correlations %&amp;gt;%
    #left join in the meaning for each variable
    mutate(Code = names(demography[2:ncol(demography)])) %&amp;gt;%
    left_join(., select(census_index, Code, Meaning), by = &amp;quot;Code&amp;quot;) %&amp;gt;%
    arrange(-abs(statistic)) %&amp;gt;%
    select(-term)
  
  #return this data frame
  return(tidy_df)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lr_margin &amp;lt;- votes_data %&amp;gt;%
  left_join(constituency_names, by = &amp;quot;WSTnm&amp;quot;) %&amp;gt;%
  #must line up in order with census data
  arrange(WSTid) %&amp;gt;%
  #assuming a simple left vs right decision for voters
  mutate(left = lab + ld + green,
         right = con + brexit) %&amp;gt;%
  #take the difference between left and right sum votes for each constituency
  mutate(margin = left - right) %&amp;gt;%
  .$margin

#run in the above function
lr_correlations &amp;lt;- get_correlations_tidy(census_data_westminster, lr_margin)

head(select(lr_correlations, Code, Meaning, statistic, p.value))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   Code      Meaning                                      statistic  p.value
##   &amp;lt;chr&amp;gt;     &amp;lt;fct&amp;gt;                                            &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 KS104EW0~ Not living in a couple: Single (never marri~      26.0 1.86e-83
## 2 KS404EW0~ All categories: Car or van availability          -23.4 1.67e-73
## 3 KS104EW0~ Living in a couple: Married or in a registe~     -23.2 1.44e-72
## 4 KS105EW0~ One family only: Married or same-sex civil ~     -22.2 1.78e-68
## 5 KS103EW0~ Married                                          -22.0 4.93e-68
## 6 KS404EW0~ 2 cars or vans in household                      -22.0 5.77e-68&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lr_correlations &amp;lt;- lr_correlations %&amp;gt;%
  #lots of these stats are self-correlated
  #e.g. 3 cars in household vs 4+ cars in household
  mutate(stat_category = gsub(&amp;quot;(.{5})(.*)&amp;quot;, &amp;quot;\\1&amp;quot;, Code)) %&amp;gt;%
  group_by(stat_category) %&amp;gt;%
  #take only the strongest correlated variable from each &amp;#39;category&amp;#39;
  mutate(duplicate_n = 1:n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  filter(duplicate_n == 1 &amp;amp; abs(statistic) &amp;gt; 10 &amp;amp; !duplicated(Meaning))

right_variables &amp;lt;- lr_correlations %&amp;gt;%
  mutate(census_info = paste(Code, Meaning)) %&amp;gt;%
  filter(statistic &amp;lt; 0) %&amp;gt;%
  .$census_info

left_variables &amp;lt;- lr_correlations %&amp;gt;%
  mutate(census_info = paste(Code, Meaning)) %&amp;gt;%
  filter(statistic &amp;gt; 0) %&amp;gt;%
  .$census_info

right_variables&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;KS404EW0007 All categories: Car or van availability&amp;quot;                                   
##  [2] &amp;quot;KS105EW0005 One family only: Married or same-sex civil partnership couple: No children&amp;quot;
##  [3] &amp;quot;KS103EW0003 Married&amp;quot;                                                                   
##  [4] &amp;quot;KS501EW0004 Highest level of qualification: Level 2 qualifications&amp;quot;                    
##  [5] &amp;quot;KS102EW0013 Age 60 to 64&amp;quot;                                                              
##  [6] &amp;quot;KS401EW0008 Whole house or bungalow: Detached&amp;quot;                                         
##  [7] &amp;quot;KS603EW0002 Economically active: Employee: Part-time&amp;quot;                                  
##  [8] &amp;quot;KS609EW0006 5. Skilled trades occupations&amp;quot;                                             
##  [9] &amp;quot;KS605EW0007 F Construction&amp;quot;                                                            
## [10] &amp;quot;KS402EW0002 Owned: Owned outright&amp;quot;                                                     
## [11] &amp;quot;KS301EW0014 Provides 1 to 19 hours unpaid care a week&amp;quot;                                 
## [12] &amp;quot;KS209EW0002 Christian&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;left_variables&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;KS104EW0004 Not living in a couple: Single (never married or never registered a same-sex civil partnership)&amp;quot;
##  [2] &amp;quot;KS403EW0004 Occupancy rating (rooms) of -1 or less&amp;quot;                                                         
##  [3] &amp;quot;KS201EW0007 Mixed/multiple ethnic group: White and Black African&amp;quot;                                           
##  [4] &amp;quot;KS604EW0008 Males: Part-time: 16 to 30 hours worked&amp;quot;                                                        
##  [5] &amp;quot;KS612EW0012 L14.1 Never worked&amp;quot;                                                                             
##  [6] &amp;quot;KS613EW0014 Not classified&amp;quot;                                                                                 
##  [7] &amp;quot;KS202EW0039 Other identities only&amp;quot;                                                                          
##  [8] &amp;quot;KS206EW0005 No people in household have English as a main language (English or Welsh in Wales)&amp;quot;             
##  [9] &amp;quot;KS204EW0010 Other countries&amp;quot;                                                                                
## [10] &amp;quot;KS205EW0008 Middle East and Asia&amp;quot;                                                                           
## [11] &amp;quot;KS107EW0012 Female lone parent: Not in employment&amp;quot;                                                          
## [12] &amp;quot;KS106EW0002 No adults in employment in household: With dependent children&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;brexit_vote &amp;lt;- votes_data %&amp;gt;%
  left_join(constituency_names, by = &amp;quot;WSTnm&amp;quot;) %&amp;gt;%
  #must line up in order with census data
  arrange(WSTid) %&amp;gt;%
  .$brexit_hanretty

brexit_correlations &amp;lt;- get_correlations_tidy(census_data_westminster, brexit_vote) %&amp;gt;%
  #lots of these stats are self-correlated
  #e.g. 3 cars in household vs 4+ cars in household
  mutate(stat_category = gsub(&amp;quot;(.{5})(.*)&amp;quot;, &amp;quot;\\1&amp;quot;, Code)) %&amp;gt;%
  group_by(stat_category) %&amp;gt;%
  #take only the strongest correlated variable from each &amp;#39;category&amp;#39;
  mutate(duplicate_n = 1:n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  filter(duplicate_n == 1 &amp;amp; abs(statistic) &amp;gt; 10 &amp;amp; !duplicated(Meaning))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Column `Code` joining character vector and factor, coercing into
## character vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(brexit_correlations) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 10
##   estimate std.error statistic   p.value adj_r f_stat Code  Meaning
##      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;fct&amp;gt;  
## 1     5.27     0.171      30.8 2.32e-101 0.731   948. KS50~ Highes~
## 2     6.08     0.239      25.4 3.36e- 81 0.648   645. KS61~ 6. Sem~
## 3    -3.14     0.126     -25.0 1.52e- 79 0.641   623. KS60~ 2. Pro~
## 4     9.04     0.380      23.8 5.00e- 75 0.618   567. KS60~ 8. Pro~
## 5     7.65     0.329      23.3 6.44e- 73 0.608   542. KS61~ 5. Low~
## 6    -2.20     0.103     -21.3 5.47e- 65 0.564   453. KS30~ Very g~
## # ... with 2 more variables: stat_category &amp;lt;chr&amp;gt;, duplicate_n &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lr_correlations$Meaning[which(lr_correlations$Code %in% brexit_correlations$Code)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] Not living in a couple: Single (never married or never registered a same-sex civil partnership)
## [2] Economically active: Employee: Part-time                                                       
## [3] Occupancy rating (rooms) of -1 or less                                                         
## 793 Levels: 0 or multiple adults in household ... Widowed or surviving partner from a same-sex civil partnership&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_census &amp;lt;- census_data_westminster %&amp;gt;%
  #take only the variable that strongly correlate with 2019/brexit vote
  select(unique(c(lr_correlations$Code, brexit_correlations$Code))) %&amp;gt;%
  #scale before pca
  scale()

#run the pca
#take first 3 components
demographic_pca &amp;lt;- prcomp(pca_census)$x %&amp;gt;%
  as.data.frame() %&amp;gt;%
  .[1:3] %&amp;gt;%
  #add back in ID column
  mutate(WSTid = census_data_westminster$WSTid) %&amp;gt;%
  #join in additional dta for plotting
  left_join(., constituency_names, by = &amp;quot;WSTid&amp;quot;) %&amp;gt;%
  left_join(., select(votes_data, WSTnm, winner), by = &amp;quot;WSTnm&amp;quot;)

#plot
demographic_pca_plot &amp;lt;- demographic_pca %&amp;gt;%
  ggplot(aes(x = PC1, y = PC2, label = gsub(&amp;quot;a|e|i|o|u&amp;quot;, &amp;quot;&amp;quot;, WSTnm), colour = winner)) +
  geom_point() +
  geom_text() +
  scale_colour_manual(values = c(&amp;quot;mediumblue&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;goldenrod&amp;quot;, &amp;quot;darkgreen&amp;quot;)) +
  labs(x = &amp;quot;PC1 - Urban, Young &amp;amp; Diverse -&amp;gt;&amp;quot;,
       y = &amp;quot;PC2 - Economically &amp;#39;Left Behind&amp;#39; -&amp;gt;&amp;quot;) +
  facet_wrap(~first_letter)

demographic_pca_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-09-from_hackney_to_haringey_files/figure-html/do_census_pca-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;distances &amp;lt;- demographic_pca %&amp;gt;%
  split(f = .$first_letter) %&amp;gt;%
  map_df(., function(data) {
    distances &amp;lt;- (outer(data$PC1, data$PC1, &amp;quot;-&amp;quot;)^2 + outer(data$PC2, data$PC2, &amp;quot;-&amp;quot;)^2) %&amp;gt;%
    sqrt() %&amp;gt;%
    as.data.frame() %&amp;gt;%
    mutate(from = data$WSTnm)
    names(distances)[1:(ncol(distances)-1)] &amp;lt;- as.character(data$WSTnm)
    df &amp;lt;- pivot_longer(distances, -starts_with(&amp;quot;from&amp;quot;),
                       names_to = &amp;quot;to&amp;quot;,
                       values_to = &amp;quot;pca_distance&amp;quot;) %&amp;gt;%
      mutate(pca_distance = abs(pca_distance))
    return(df)
  }) %&amp;gt;%
  filter(!duplicated(pca_distance) &amp;amp; pca_distance != 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggrepel)
all_distances &amp;lt;- distances %&amp;gt;%
  left_join(brexit_differences, by = c(&amp;quot;from&amp;quot;, &amp;quot;to&amp;quot;)) %&amp;gt;%
  mutate(brexit_2016_difference = abs(brexit_2016_difference)) %&amp;gt;%
  select(-first_letter) %&amp;gt;%
  left_join(constituency_interdistances, by = c(&amp;quot;from&amp;quot;, &amp;quot;to&amp;quot;)) %&amp;gt;%
  mutate(label = case_when(
    distance &amp;gt; 400000 &amp;amp; pca_distance &amp;gt; 15 &amp;amp; brexit_2016_difference &amp;gt; 0.3 ~ paste(to, from, sep = &amp;quot;-&amp;quot;)
  )) %&amp;gt;%
  mutate(distance = distance / 1000)

p &amp;lt;- ggplot(all_distances, aes(x = distance, y = pca_distance, size = brexit_2016_difference)) +
  geom_point(alpha = 0.2) +
  geom_point(data = filter(all_distances, !is.na(label))) +
  geom_text_repel(aes(label = label)) +
  scale_size_continuous(name = &amp;quot;diff Brexit\n 2016 vote&amp;quot;, range = c(0.5, 5)) +
  labs(x = &amp;quot;Geographic Distances between Constituences (/km&amp;quot;,
       y = &amp;quot;&amp;#39;Distance&amp;#39; between Constituencies Demograph (2011 Census)&amp;quot;) +
  theme_minimal()

plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 4608 rows containing missing values (geom_text_repel).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-09-from_hackney_to_haringey_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Which English County Has Won the Most Points</title>
      <link>/post/counties_league_points/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/counties_league_points/</guid>
      <description>


&lt;p&gt;Every so often a question on The Guardian’s &lt;a href=&#34;https://www.theguardian.com/football/series/theknowledge&#34;&gt;The Knowledge&lt;/a&gt; football trivia section piques my interest and is amenable to analysis using R. Previously, I looked at &lt;a href=&#34;http://www.robert-hickman.eu/post/the-knowledge-4th-august-2018/&#34;&gt;club name suffixes and young World Cup winners&lt;/a&gt; last August. This week (give or take), a question posed on twitter caught my attention:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
&lt;a href=&#34;https://twitter.com/TheKnowledge_GU?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@TheKnowledge_GU&lt;/span&gt;&lt;/a&gt; was just chatting to some colleagues in the kitchen at work about why Essex doesn&#39;t have many big football clubs and it got me thinking. If you combined all the points from every league team in the ceremonial counties in England, which county would be on top?
&lt;/p&gt;
— BoxBoron (&lt;span class=&#34;citation&#34;&gt;@Rutland_Walker&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/Rutland_Walker/status/1082641231853899781?ref_src=twsrc%5Etfw&#34;&gt;January 8, 2019&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;To start with as always load the libraries needed to analyse this&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get data
library(engsoccerdata)
library(rvest)
#munging
library(tidyverse)
library(magrittr)
#spatial analysis
library(sf)
library(rgdal)
#for plotting maps
library(ggthemes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The easiest way to get a total of points is using the engsoccerdata:: packages database of every English football match from the top four divisions (this does not include data from the 2017-2018, or 2018-2019 seasons). We can work out the points easily from the goals scored for each team&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load the data
match_data &amp;lt;- engsoccerdata::england %&amp;gt;%
  #select only the necessary columns and melt
  select(season = Season, home, visitor, hgoal, vgoal, tier) %&amp;gt;%
  reshape2::melt(id.vars = c(&amp;quot;season&amp;quot;, &amp;quot;hgoal&amp;quot;, &amp;quot;vgoal&amp;quot;, &amp;quot;tier&amp;quot;),
                 variable.name = &amp;quot;location&amp;quot;,
                 value.name = &amp;quot;team&amp;quot;) %&amp;gt;%
  #will need to match this to location data so some club names need cleaning
  mutate(team_subbed = case_when(
    team == &amp;quot;Yeovil&amp;quot; ~ &amp;quot;Yeovil Town&amp;quot;,
    team == &amp;quot;AFC Bournemouth&amp;quot; ~ &amp;quot;A.F.C. Bournemouth&amp;quot;,
    team == &amp;quot;Halifax Town&amp;quot; ~ &amp;quot;F.C. Halifax Town&amp;quot;,
    team == &amp;quot;Aldershot&amp;quot; ~ &amp;quot;Aldershot Town F.C&amp;quot;,
    team == &amp;quot;Wimbledon&amp;quot; ~ &amp;quot;A.F.C. Wimbledon&amp;quot;,
    team == &amp;quot;AFC Wimbledon&amp;quot; ~ &amp;quot;A.F.C. Wimbledon&amp;quot;,
    team == &amp;quot;Macclesfield&amp;quot; ~ &amp;quot;Macclesfield Town&amp;quot;,
    team == &amp;quot;Rushden &amp;amp; Diamonds&amp;quot; ~ &amp;quot;A.F.C. Rushden &amp;amp; Diamonds&amp;quot;,
    team == &amp;quot;Milton Keynes Dons&amp;quot; ~ &amp;quot;Milton Keynes&amp;quot;,
    team == &amp;quot;Dagenham and Redbridge&amp;quot; ~ &amp;quot;Dagenham &amp;amp; Redbridge&amp;quot;,
    team == &amp;quot;Stevenage Borough&amp;quot; ~ &amp;quot;Stevenage&amp;quot;
  )) %&amp;gt;%
  #if cleaning isnt required, take original
  mutate(team_subbed = ifelse(is.na(team_subbed), team, team_subbed))

#peek at the data
head(match_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   season hgoal vgoal tier location            team     team_subbed
## 1   1888     1     1    1     home Accrington F.C. Accrington F.C.
## 2   1888     0     2    1     home Accrington F.C. Accrington F.C.
## 3   1888     2     3    1     home Accrington F.C. Accrington F.C.
## 4   1888     5     1    1     home Accrington F.C. Accrington F.C.
## 5   1888     6     2    1     home Accrington F.C. Accrington F.C.
## 6   1888     3     1    1     home Accrington F.C. Accrington F.C.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 388k (194k matches) data.frame seems daunting, but actually only results in many fewer unique teams that have played at least one match in the top 4 divisions in England&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(unique(match_data$team_subbed))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 141&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The location of each club can then be found using the wikipedia pages for them/their stadia. This matches 121 of the 141 clubs pretty nicely which is a fairly good percentage all things considered&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the links to each clubs wikipedia page
wiki &amp;lt;- read_html(&amp;quot;https://en.wikipedia.org/wiki/List_of_football_clubs_in_England&amp;quot;) %&amp;gt;%
  html_nodes(&amp;quot;td:nth-child(1)&amp;quot;) %&amp;gt;%
  .[which(grepl(&amp;quot;href&amp;quot;, .))]

#get the names for each club
wiki_clubs &amp;lt;- wiki %&amp;gt;% html_text() %&amp;gt;% gsub(&amp;quot; \\(.*\\)$&amp;quot;, &amp;quot;&amp;quot;, .)

#can match 121/141 right off the bat
(unique(match_data$team_subbed) %in% wiki_clubs) %&amp;gt;%
  which() %&amp;gt;%
  length()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 121&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can find the location of these matching clubs by finding the page for their stadia and then finding the coordinates. It’s a bit of a messy function because I was just jamming stuff together to get data out as best as possible. This takes ~1 minute to run through all 121 teams (for the blog post I actually saved an RDS of the output from this and load it just to save time/server calls)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matching_club_locations &amp;lt;- wiki %&amp;gt;% 
  #take only the matching clubs
  .[which(wiki_clubs %in% unique(match_data$team_subbed))] %&amp;gt;%
  html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;%
  #get the wiki page link
  html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
  paste0(&amp;quot;https://en.wikipedia.org&amp;quot;, .) %&amp;gt;%
  #for each club page find the stadium and its coordinates
  lapply(., function(team) {
    link &amp;lt;- read_html(team) %&amp;gt;%
      html_nodes(&amp;quot;.label a&amp;quot;) %&amp;gt;%
      .[1] %&amp;gt;%
      html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% 
      paste0(&amp;quot;https://en.wikipedia.org&amp;quot;,. )
    coords &amp;lt;- link %&amp;gt;%
      read_html() %&amp;gt;% 
      html_nodes(&amp;quot;#coordinates a&amp;quot;) %&amp;gt;%
      html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
      .[2]
    #if coords not found use NA
    if(is.na(coords)) {
      coord_df &amp;lt;- data.frame(lat = NA,
                             lon = NA)
    } else {
      coords &amp;lt;- coords %&amp;gt;%
        paste0(&amp;quot;https:&amp;quot;, .) %&amp;gt;%
        read_html() %&amp;gt;%
        html_nodes(&amp;quot;.geo&amp;quot;) %&amp;gt;%
        html_text() %&amp;gt;%
        strsplit(., split = &amp;quot;, &amp;quot;)
      coord_df &amp;lt;- data.frame(lat = as.numeric(coords[[1]][1]),
                             lon = as.numeric(coords[[1]][2]))
    }
    return(coord_df)
  })  %&amp;gt;%
  #bind everything together
  do.call(rbind, .) %&amp;gt;%
  #add the club name as a new column
  mutate(team = wiki_clubs[
    which(wiki_clubs %in% unique(match_data$team_subbed))
    ]) %&amp;gt;%
  #filter out missing data
  filter(!is.na(lat) &amp;amp; !is.na(lon))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which gives us the location of 114 of our 141 clubs. Most of the remaining ones are now-defunct clubs (e.g. Middlesbrough Ironopolis, Leeds City etc.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;missing_teams &amp;lt;- unique(match_data$team_subbed)[which(!unique(match_data$team_subbed) %in% matching_club_locations$team)]
missing_teams&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Accrington F.C.&amp;quot;           &amp;quot;Darwen&amp;quot;                   
##  [3] &amp;quot;Burton Swifts&amp;quot;             &amp;quot;Port Vale&amp;quot;                
##  [5] &amp;quot;Middlesbrough Ironopolis&amp;quot;  &amp;quot;Rotherham Town&amp;quot;           
##  [7] &amp;quot;Burton Wanderers&amp;quot;          &amp;quot;Loughborough&amp;quot;             
##  [9] &amp;quot;Blackpool&amp;quot;                 &amp;quot;New Brighton Tower&amp;quot;       
## [11] &amp;quot;Burton United&amp;quot;             &amp;quot;Leeds City&amp;quot;               
## [13] &amp;quot;Rotherham County&amp;quot;          &amp;quot;Bristol Rovers&amp;quot;           
## [15] &amp;quot;Darlington&amp;quot;                &amp;quot;Wigan Borough&amp;quot;            
## [17] &amp;quot;Aberdare Athletic&amp;quot;         &amp;quot;New Brighton&amp;quot;             
## [19] &amp;quot;Thames&amp;quot;                    &amp;quot;Aldershot Town F.C&amp;quot;       
## [21] &amp;quot;Hereford United&amp;quot;           &amp;quot;Scarborough&amp;quot;              
## [23] &amp;quot;Cheltenham&amp;quot;                &amp;quot;A.F.C. Rushden &amp;amp; Diamonds&amp;quot;
## [25] &amp;quot;Accrington&amp;quot;                &amp;quot;Crawley Town&amp;quot;             
## [27] &amp;quot;Fleetwood Town&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given it was a Saturday morning where I had nothing better to do, I simply located these clubs home grounds manually and created a data.frame for their locations. It’s not really great practice but whatever.&lt;/p&gt;
&lt;p&gt;These are then all bound together and converted to an sf spatial object with the correct projection&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#add in the missing locations
missing_locations &amp;lt;- data.frame(
  lat = c(53.7646, 53.711772, 52.799, 53.049722, 54.5641, 53.42644, 52.8146,
          52.7743, 53.804722, 53.4359, 52.799, 53.7778, 53.428367, 51.48622,
          54.508425, 53.554914, 51.7127, 53.4292, 51.514431, 51.248386,
          52.060719, 54.265478, 51.906158, 52.328033, 53.7646, 51.405083, 53.9165),
  lon = c(-2.358, -2.477292, -1.6354, -2.1925, -1.2456, -1.34377, -1.6335, -1.1992,
          -3.048056, -3.0377, -1.6354, -1.5722, -1.370231, -2.583134, -1.534394,
          -2.650661, -3.4374, -3.0407, 0.034739, -0.754789, -2.717711, -0.418247,
          -2.060211, -0.5999, -2.358, -0.281944, -3.0247),
    team = as.character(missing_teams)
)

#bind together and convert to sf
all_locations &amp;lt;- rbind(matching_club_locations,
                       missing_locations) %&amp;gt;%
  st_as_sf(coords = c(&amp;quot;lon&amp;quot;, &amp;quot;lat&amp;quot;), crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;)) %T&amp;gt;%
  #make a quick plot of locations for sanity check
  plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-21-The_Knowledge_2_files/figure-html/bind_missing_locations-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that we have all the teams, we need the English historical county boundaries to group them by. I’d actually already used these for football analysis, looknig at &lt;a href=&#34;https://www.citymetric.com/horizons/football-could-independent-yorkshire-win-world-cup-3961&#34;&gt;if an independent Yorkshire could win the World Cup&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Load the data (the boundary file can be download from the &lt;a href=&#34;https://www.ordnancesurvey.co.uk/business-and-government/products/boundaryline.html&#34;&gt;Ordnance Survey&lt;/a&gt;) and make a quick plot of the boundaries and teams&lt;/p&gt;
&lt;p&gt;(I also created an sf object engwal which is just the counties from England and Wales selected out for background plotting)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile 
## Source: &amp;quot;C:\Users\Alaa\Desktop\geo_data\boundary\Data\Supplementary_Ceremonial&amp;quot;, layer: &amp;quot;Boundary-line-ceremonial-counties_region&amp;quot;
## with 91 features
## It has 2 fields&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load the boundary file
counties &amp;lt;- readOGR(dsn = &amp;quot;path/to/file&amp;quot;,
                    layer = &amp;quot;county_boundaries&amp;quot;) %&amp;gt;%
  #convert to sf and project as northing/easting
  st_as_sf(., crs = st_crs(&amp;quot;+init=epsg:27700&amp;quot;)) %&amp;gt;%
  #only interested in the county name
  select(county = NAME) %&amp;gt;%
  #transform the projection to match that of the club locations
  st_transform(., crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;))

engwal &amp;lt;- counties %&amp;gt;%
  .[c(1:54, 88, 90),]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#make a quick plot of counties and teams
ggplot() +
  geom_sf(data = counties, fill = NA) +
  geom_sf(data = all_locations) +
  ggtitle(&amp;quot;Location of Teams to have Played Top\n 4 English Football Divisions&amp;quot;) +
  theme_minimal() +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-21-The_Knowledge_2_files/figure-html/plot_team_locations-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(by the way the artifacts around Bristol and the Wirral are from the OS dataset- it’s very annoying)&lt;/p&gt;
&lt;p&gt;Then we need to determine which teams are within which counties. The easiest way to do this is to use a spatial join of the team names in all_locations by which county they fall into (using st_contains from the sf package)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#bind the team names to each county
counties %&amp;lt;&amp;gt;%
  st_join(., all_locations, join = st_contains) %&amp;gt;%
  #remove counties that contain zero teams
  filter(!is.na(team)) %&amp;gt;%
  mutate(county = as.character(county))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## although coordinates are longitude/latitude, st_contains assumes that they are planar&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#quick plot of number of teams per county (missing = 0)
counties %&amp;gt;%
  group_by(county) %&amp;gt;%
  summarise(n_clubs = n()) %&amp;gt;%
  ggplot(data = .) +
  geom_sf(data = engwal) +
  geom_sf(aes(fill = n_clubs), colour = &amp;quot;black&amp;quot;) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;, name = &amp;quot;# clubs&amp;quot;) +
  ggtitle(&amp;quot;Number of Top 4 Division Playing\n Teams in each Ceremonial County&amp;quot;) +
  theme_minimal() +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-21-The_Knowledge_2_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which shows that most English historic counties (and a few Welsh ones due to teams like Cardiff City/ Swansea City etc.) have at least 1 team that has competed in the top 4 flights of English football at some point (those that do not are: Isle of Wight, Rutland, Surrey, Warwickshire, West Sussex and Cornwall).&lt;/p&gt;
&lt;p&gt;To finally get the total number of points won by these teams, the county data needs to be joined back onto the match data from the top. First I clean it up a bit then make the left_join by team name. Finally the number of points per match is calculated using case_when and points are grouped by county and summed&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county_match_data &amp;lt;- match_data %&amp;gt;% 
  mutate(team = team_subbed) %&amp;gt;%
  select(-team_subbed) %&amp;gt;%
  left_join(., counties, by = &amp;quot;team&amp;quot;) %&amp;gt;%
  mutate(points = case_when(
    location == &amp;quot;home&amp;quot; &amp;amp; hgoal &amp;gt; vgoal ~ 3,
    location == &amp;quot;visitor&amp;quot; &amp;amp; vgoal &amp;gt; hgoal ~ 3,
    location == &amp;quot;home&amp;quot; &amp;amp; hgoal &amp;lt; vgoal ~ 0,
    location == &amp;quot;visitor&amp;quot; &amp;amp; vgoal &amp;lt; hgoal ~ 0,
    hgoal == vgoal ~ 1
  ))

county_points &amp;lt;- county_match_data %&amp;gt;%
  group_by(county) %&amp;gt;%
  summarise(total_points = sum(points))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perhaps unsurprisingly, the county with the most points is Greater London, with Greater Manchester following and other footballing hotspots/ large counties in the West Midlands, Lancashire and around Yorkshire in the trailing group&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(arrange(county_points, -total_points))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   county             total_points
##   &amp;lt;chr&amp;gt;                     &amp;lt;dbl&amp;gt;
## 1 Greater London            67189
## 2 Greater Manchester        47203
## 3 West Midlands             37413
## 4 Lancashire                30808
## 5 South Yorkshire           30061
## 6 West Yorkshire            24947&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By contrast, Worcestshire and Northumberland barely have any points, with a few Welsh counties also struggling&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(arrange(county_points, total_points))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   county          total_points
##   &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;
## 1 Worcestershire           275
## 2 Northumberland           398
## 3 Mid Glamorgan            744
## 4 Somerset                 813
## 5 Gloucestershire          994
## 6 Herefordshire           1739&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we group by tier as well as county, it’s possible to see how well each county has done at specific tiers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county_match_data %&amp;gt;%
  group_by(county, tier) %&amp;gt;%
  summarise(total_points = sum(points)) %&amp;gt;%
  left_join(.,
            select(counties, county),
            by = &amp;quot;county&amp;quot;) %&amp;gt;%
  ggplot(data = .) +
  geom_sf(data = engwal) +
  geom_sf(aes(fill = total_points), colour = &amp;quot;black&amp;quot;) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;, name = &amp;quot;total points&amp;quot;) +
  ggtitle(&amp;quot;Number of Points Won by each County\n per Tier of English Football&amp;quot;) +
  facet_wrap(~tier) +
  theme_minimal() +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-21-The_Knowledge_2_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And for the Premier League era this clears up to&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county_match_data %&amp;gt;%
  filter(season &amp;gt; 1991) %&amp;gt;%
  group_by(county, tier) %&amp;gt;%
  summarise(total_points = sum(points)) %&amp;gt;%
  left_join(.,
            select(counties, county),
            by = &amp;quot;county&amp;quot;) %&amp;gt;%
  ggplot(data = .) +
  geom_sf(data = engwal) +
  geom_sf(aes(fill = total_points), colour = &amp;quot;black&amp;quot;, name = &amp;quot;total points&amp;quot;) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;) +
  ggtitle(&amp;quot;Number of Points Won by each County\n per Tier of English Football&amp;quot;,
          subtitle = &amp;quot;From Begining of 1992/1993 Season&amp;quot;) +
  facet_wrap(~tier) +
  theme_minimal() +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Ignoring unknown parameters: name&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-21-The_Knowledge_2_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which shows just how dominant London has been in the top division of English football (especially as it is only competitive at lower levels).&lt;/p&gt;
&lt;p&gt;I had wanted to weight points by the average ELO of that league and see which county has the most weight-adjusted points but got bored for this small blog post.&lt;/p&gt;
&lt;p&gt;Best,&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Planning a Pub Crawl Using R</title>
      <link>/post/cambridge_pub_crawl/</link>
      <pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/cambridge_pub_crawl/</guid>
      <description>


&lt;p&gt;A few weeks ago I went on the first pub crawl I’d been on in years around my city of Cambridge. Around the same time I had also been visiting &lt;a href=&#34;https://www.google.co.uk/maps/@52.2046202,0.1289874,18z&#34;&gt;4 very good pubs within ~200m of each other&lt;/a&gt; tucked away in a quiet neighbourhood of the town. Together, I wondered if it was possible with freely avaiable data to plan an optimal pub crawl around any town/area of the UK, and also, if it would be feasbile to visit every pub within the city in a single day if travelling optimally.&lt;/p&gt;
&lt;p&gt;Once again, I found that the simple features library (sf) for R basically can do all of this pretty simply, with igraph picking up most of the networking slack. In fact, overall it was much much simpler than I thought it would be. In total, I only needed 9 libraries (though granted tidyverse is one).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#data munging
library(tidyverse)
library(magrittr)

#scrape pub data
library(rvest)
library(googleway)

#spatial manipulation
#almost all done in sf
library(sf)
library(rgdal)

#networking the pubs
library(igraph)
library(TSP)
library(geosphere)

rm(list=ls())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the data, I was able to get by using &lt;a href=&#34;https://www.ordnancesurvey.co.uk/opendatadownload/products.html#BDLINE&#34;&gt;Ordnance Survey data&lt;/a&gt; data for the spatial work. I used Boundary Line for the city boundaries (taking the Cambridge Boro Westminster constituency limits as the city boundaries), and OS OpenRoads for all the road work. These are freely avaiable via email using the link above.&lt;/p&gt;
&lt;p&gt;For reproducibility, these are both presented as if saved in path/to/os/data with folders called ./roads and ./boundary containing the extracted files from both of these.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;file_path &amp;lt;- &amp;quot;path/to/os/data&amp;quot;

#load the westminster constituency boundaries
cambridge &amp;lt;- readOGR(dsn = file.path(file_path, &amp;quot;boundary/Data/GB&amp;quot;), 
                     layer = &amp;quot;westminster_const_region&amp;quot;) %&amp;gt;%
  #convert to sf
  st_as_sf() %&amp;gt;%
  #select only the cambridge constituency
  filter(NAME == &amp;quot;Cambridge Boro Const&amp;quot;) %&amp;gt;%
  #get rid of associated data for cleanness
  select()

#load the road link and node data
#uses the uk national grid to partion road files
#https://en.wikivoyage.org/wiki/National_Grid_(Britain)
#cambridge is in grid TL
roads &amp;lt;- readOGR(dsn = file.path(file_path, &amp;quot;roads/data&amp;quot;), 
                     layer = &amp;quot;TL_RoadLink&amp;quot;) %&amp;gt;%
  st_as_sf() %&amp;gt;%
  #transform to the crs of the city boundary
  st_transform(st_crs(cambridge)) %&amp;gt;%
  #take only the roads which cross into the city
  .[unlist(st_intersects(cambridge, .)),]

nodes &amp;lt;- readOGR(dsn = file.path(file_path, &amp;quot;roads/data&amp;quot;), 
                     layer = &amp;quot;TL_RoadNode&amp;quot;)
#converting straight to sf gives an error so munge data manually
nodes &amp;lt;- cbind(nodes@data, nodes@coords) %&amp;gt;%
  st_as_sf(coords = c(&amp;quot;coords.x1&amp;quot;, &amp;quot;coords.x2&amp;quot;),
           crs = st_crs(&amp;quot;+init=epsg:27700&amp;quot;)) %&amp;gt;%
  st_transform(st_crs(cambridge)) %&amp;gt;%
  #take only nodes which are related to the roads we previously selected
  .[which(.$identifier %in% c(as.character(roads$startNode), 
                              as.character(roads$endNode))),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have these we can make a quick plot of the layout of the city&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#quickly plot the roads and nodes data
(p1 &amp;lt;- ggplot(cambridge) +
   geom_sf() +
   geom_sf(data = roads, colour = &amp;quot;black&amp;quot;) +
   geom_sf(data = nodes, colour = &amp;quot;red&amp;quot;, alpha = 0.5, size = 0.5) +
   theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/roads_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next we need point locations for all the pubs in Cambridge. Fortunately &lt;a href=&#34;www.pubsgalore.co.uk&#34;&gt;pubsgalore.co.uk&lt;/a&gt; has us covered with a pretty extensive list. It doesn’t contain the college bars of the University which is a bit of a shame, but is still a pretty good sample of 199 pubs in/around Cambridge.&lt;/p&gt;
&lt;p&gt;We want the name and address of every open pub which this will scrape.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#page with every pub in cambridge
pub_page &amp;lt;- &amp;quot;https://www.pubsgalore.co.uk/towns/cambridge/cambridgeshire/&amp;quot; %&amp;gt;%
  read_html()


open_pubs &amp;lt;- pub_page %&amp;gt;%
  html_nodes(&amp;quot;.pubicons .pubclosed&amp;quot;) %&amp;gt;%
  html_attr(&amp;quot;src&amp;quot;) %&amp;gt;%
  grep(&amp;quot;grey&amp;quot;, .)

pub_info &amp;lt;- pub_page %&amp;gt;%
  html_nodes(&amp;quot;#pagelist a&amp;quot;) %&amp;gt;%
  html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
  .[open_pubs] %&amp;gt;%
  paste0(&amp;quot;https://www.pubsgalore.co.uk&amp;quot;, .) %&amp;gt;%
  lapply(., function(single_pub) {
    pub_page_read &amp;lt;- single_pub %&amp;gt;%
      read_html()
    
    #get the name of the pub
    pub_name &amp;lt;- pub_page_read %&amp;gt;%
      html_nodes(&amp;quot;.pubname&amp;quot;) %&amp;gt;%
      html_text()
    
    #get the address of the pub 
    line1 &amp;lt;- pub_page_read %&amp;gt;% html_nodes(&amp;quot;.address&amp;quot;) %&amp;gt;% html_text()
    line2 &amp;lt;- pub_page_read %&amp;gt;% html_nodes(&amp;quot;.town&amp;quot;) %&amp;gt;% html_text()
    line3 &amp;lt;- pub_page_read %&amp;gt;% html_nodes(&amp;quot;.postcode&amp;quot;) %&amp;gt;% html_text()
    
    pub_address &amp;lt;- paste0(line1, &amp;quot;, &amp;quot;, line2, &amp;quot;, &amp;quot;, line3)
    
    #put together into data.frame
    pub_data &amp;lt;- data.frame(name = pub_name, address = pub_address)
    return(pub_data)
  }) %&amp;gt;%
  #rbind the lapply results
  do.call(rbind, .) %&amp;gt;%
  #remove duplicated pub addresses
  filter(!duplicated(address))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then need to geocode these adresses into coordinates. Because ggmap has been playing up for me, I tend to use the googleway package with a Google API key which you can get for free &lt;a href=&#34;https://developers.google.com/maps/documentation/javascript/get-api-key&#34;&gt;here&lt;/a&gt;. My key isn’t in the code published here for obvious reasons.&lt;/p&gt;
&lt;p&gt;These coordinates are then bound back onto the pub df and we filter out only the pubs which are located within the city limits (103).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pubs &amp;lt;- pub_info$address %&amp;gt;%
  #convert to character
  as.character() %&amp;gt;%
  #find the coords of every pub address using googleway
  lapply(., function(address) {
    #get the coords
    coords &amp;lt;- google_geocode(address, key = key) %&amp;gt;%
      .$results %&amp;gt;%
      .$geometry %&amp;gt;%
      .$location %&amp;gt;%
      #covert to df and add the address back
      data.frame() %&amp;gt;%
      mutate(address = address)
  }) %&amp;gt;%
  #rbind the results
  do.call(rbind, .) %&amp;gt;%
  #merge back in the pub names
  merge(pub_info, by = &amp;quot;address&amp;quot;) %&amp;gt;%
  #convert to sf
  st_as_sf(coords = c(&amp;quot;lng&amp;quot;, &amp;quot;lat&amp;quot;),
           crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;),
           remove = FALSE) %&amp;gt;%
  #convert to the same crs as the city shapefile
  st_transform(crs = st_crs(cambridge)) %&amp;gt;%
  #only take those which fall within the city shapefile
  #the postal district is a little large and extends into Cambridgeshire
  .[unlist(st_contains(cambridge, .)),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we plot these we can see that most are in the very centre of the city, with some sparsely distributed out in Trumpington (south), Cherry Hinton (east), and Arbury (north).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#quickly plot the roads and nodes data
(p2 &amp;lt;- ggplot(cambridge) +
   geom_sf() +
   geom_sf(data = roads, colour = &amp;quot;black&amp;quot;) +
   #pubs in blue
   geom_sf(data = pubs, colour = &amp;quot;blue&amp;quot;, size = 1.5) +
   theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/pubs_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We could just take the nearest roads to each pub and then easily create a node graph using the lookup between the roads and nodes in the OS data. However, I wanted to play around with manipulating the spatial data (this is a learning exercise after all) and so decided to see how ‘accurate’ I could get the distance on the optimal pub crawl path. In reality, the point locations given by google are probably slightly off anyway, but I’m going to ignore that.&lt;/p&gt;
&lt;p&gt;In order to include ‘half-roads’ (i.e. when the pub is halfway down a road you don’t want to walk the full length of the road), I need to first find the nearest point on &lt;em&gt;any&lt;/em&gt; road to each pub. dist2Line from the geosphere package does this nicely, though it does require turning our sf objects back into SpatialDataFrames.&lt;/p&gt;
&lt;p&gt;(this is by far the longest step in the script btw)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#convert to spatial for dist2Line
pubs_spatial &amp;lt;- st_transform(pubs, crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;)) %&amp;gt;%
  as_Spatial()
roads_spatial &amp;lt;- st_transform(roads, crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;)) %&amp;gt;%
  as_Spatial()

#finds the distance to each nearest line and that point
road_distances &amp;lt;- suppressWarnings(dist2Line(pubs_spatial, roads_spatial)) %&amp;gt;%
  as.data.frame() %&amp;gt;%
  #convert to sf
  st_as_sf(coords = c(&amp;quot;lon&amp;quot;, &amp;quot;lat&amp;quot;),
           crs = st_crs(&amp;quot;+init=epsg:4326&amp;quot;),
           remove = FALSE) %&amp;gt;%
  st_transform(crs = st_crs(cambridge))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Taking a peek at these reveals the distance of each pub the nearest road, and the ID of that road and the point on the road nearest the pub.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#display the first few of these
head(road_distances)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Simple feature collection with 6 features and 4 fields
## geometry type:  POINT
## dimension:      XY
## bbox:           xmin: 544907.5 ymin: 256223.8 xmax: 548593.7 ymax: 258690.9
## epsg (SRID):    NA
## proj4string:    +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.999601272 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.1502,0.247,0.8421,-20.4894 +units=m +no_defs
##     distance       lon      lat   ID                  geometry
## 1  5.8371470 0.1422209 52.20374 2696   POINT (546488.8 258331)
## 2  0.1801197 0.1391954 52.19889 2989   POINT (546298 257784.9)
## 3 29.6403166 0.1720762 52.18425 4059 POINT (548593.7 256223.8)
## 4 10.4782939 0.1223197 52.20734 1765 POINT (545117.3 258690.9)
## 5 32.6370002 0.1203945 52.20662 1757 POINT (544988.1 258606.7)
## 6 10.9321895 0.1191027 52.20427 1751 POINT (544907.5 258343.3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then want to break up the roads that these point lie on. To illustrate this, I’ll use the 3rd pub in the dataset, which is The Robin Hood in Cherry Hinton (as the plots look better and make more sense than the first two in my opinion).&lt;/p&gt;
&lt;p&gt;First we take the pub, and all roads and their nodes within 100m of the pub:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#take the first pub
x_pub &amp;lt;- pubs[3,]
#get all roads within 100m and their nodes
x_roads &amp;lt;- roads %&amp;gt;%
  .[unlist(st_intersects(st_buffer(x_pub, 100), .)),]
x_nodes &amp;lt;- nodes %&amp;gt;%
  .[which(.$nodeid %in% c(as.character(x_roads$start), 
                          as.character(x_roads$end))),]

#plot the roads local to this pub
(p3 &amp;lt;- ggplot() +
  geom_sf(data = x_roads) +
  geom_sf(data = x_nodes, colour = &amp;quot;red&amp;quot;, alpha = 0.5) +
  geom_sf(data = x_pub, colour = &amp;quot;blue&amp;quot;) +
  theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/splitting_roads1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We then use the point on any road nearest to this pub (green) as the ‘entrance’ of the pub (this may not strictly be the case and it might be possible to instead match road names to the address, but whatever).&lt;/p&gt;
&lt;p&gt;Using this point, we split up the road it lies on into two new separate roads (in orange and purple). To get to this pub you would have to travel down one of these to the green point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the nearest point that lies on a road
x_nearest_road_point &amp;lt;- road_distances[3,]

#split that road into two over that point
x_split_roads &amp;lt;- roads %&amp;gt;%
  .[which(.$id == x_nearest_road_point$ID),] %&amp;gt;%
  st_difference(., st_buffer(x_nearest_road_point, 0.2)) %&amp;gt;%
    st_cast(&amp;quot;LINESTRING&amp;quot;) 

#add to the plot
(p3 &amp;lt;- p3 + 
    geom_sf(data = x_split_roads[1,], colour = &amp;quot;purple&amp;quot;, size = 1.5, alpha = 0.5) +
    geom_sf(data = x_split_roads[2,], colour = &amp;quot;goldenrod&amp;quot;, size = 1.5, alpha = 0.5) +
    geom_sf(data = x_nearest_road_point, colour = &amp;quot;darkgreen&amp;quot;, size = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/splitting_roads2-1.png&#34; width=&#34;672&#34; /&gt; I could have used the green ‘entrance’ nodes for the travelling salesman part of the problem, but decided also to create roads from this ‘entrance’ to the geocoded location of the pub (blue). This is probably the equivalent of travelling from the pavement to the bar of each pub and worthy of consideration*.&lt;/p&gt;
&lt;p&gt;These roads are created by binding the green and blue points together from each pub, grouping them, and then casting a line between them.&lt;/p&gt;
&lt;p&gt;*another reason to take this into account is that some pubs may appear far away from roads. One example that I visit fairly often is &lt;a href=&#34;https://www.google.co.uk/maps/place/Fort+St+George/@52.2123057,0.1272911,18.75z/data=!4m5!3m4!1s0x47d870ecb4e8556d:0x3bfb0ff82c243075!8m2!3d52.2124367!4d0.1278073&#34;&gt;Fort St George&lt;/a&gt; which is on a river footpath and does not have direct road access.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#combine the green point on the roads with the point for the pub
x_pub_entrance &amp;lt;- select(x_nearest_road_point) %&amp;gt;%
  rbind(., select(x_pub)) %&amp;gt;%
  group_by(&amp;quot;pub road&amp;quot;) %&amp;gt;%
  summarise() %&amp;gt;%
  #cast to a line (for a new road)
  st_cast(&amp;#39;LINESTRING&amp;#39;)

#plot the pub entrance
(p3 &amp;lt;- p3 +
    geom_sf(data = x_pub_entrance, colour = &amp;quot;lightblue&amp;quot;, size = 1.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/splitting_roads3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#remove all the extra objects we created in the example
rm(list=ls()[grep(&amp;quot;x_&amp;quot;, ls())])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First we create all the new nodes (the green pub entrances, and the blue pub locations)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#add information to the pubs df
pubs &amp;lt;- pubs %&amp;gt;%
  mutate(pub = 1:nrow(.), 
         id = (max(nodes$id)+1):(max(nodes$id) + nrow(.)),
         nodeid = NA,
         class = &amp;quot;pubnode&amp;quot;)

#bind the pubs to the nodes data frame
nodes &amp;lt;- rbind(nodes, select(pubs, pub, nodeid, id, class))

#add the nodes found as the nearest road point to each pub to the nodes df
new_nodes &amp;lt;- road_distances %&amp;gt;%
  select() %&amp;gt;%
  mutate(pub = pubs$pub, 
         id = (max(nodes$id)+1):(max(nodes$id)+nrow(.)),
         nodeid = NA,
         class = &amp;quot;entrancenode&amp;quot;)
nodes &amp;lt;- rbind(nodes, new_nodes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we create all of the split roads in a for loop and all of the roads from the green to the blue points. These are bound back into the original roads data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#split up the roads that have a new node for the netrance of a pub
roads_2_split &amp;lt;- roads %&amp;gt;%
  slice(unique(road_distances$ID))
#leave the rest alone
roads &amp;lt;- roads %&amp;gt;%
  slice(-unique(road_distances$ID))

#for each new node split up the road that it bisects it 
#as we did in the example
for(node in seq(nrow(new_nodes))) {
  #find the road that the pub is nearest to
  split_road &amp;lt;- st_intersects(st_buffer(new_nodes[node,], .2),
                              roads_2_split) %&amp;gt;%
    unlist() %&amp;gt;%
    roads_2_split[.,]
  
  #split this road up
  split_roads &amp;lt;- st_difference(split_road, st_buffer(new_nodes[node,], .2)) %&amp;gt;%
    st_cast(&amp;quot;LINESTRING&amp;quot;) %&amp;gt;%
    select(start_id, end_id, id)

  #keep hold of the old id
  old_id &amp;lt;- unique(split_roads$id)
  
  #get rid of this road from the df
  roads_2_split &amp;lt;- roads_2_split %&amp;gt;%
    slice(-which(roads_2_split$id == old_id))

  #get the nodes for the old road
  old_nodes &amp;lt;- filter(nodes, id %in% c(unique(split_roads$start_id),
                                       unique(split_roads$end_id)))
  
  #add the correct nodes to the newly split road
  split_roads$start_id &amp;lt;- old_nodes$id[
      unlist(st_contains(st_buffer(split_roads, .2), old_nodes))
    ]
  split_roads$end_id &amp;lt;- new_nodes$id[node]
  
  #add in new information for the new road
  split_roads %&amp;lt;&amp;gt;%
    mutate(id = max(roads_2_split$id) + seq(nrow(split_roads)),
           class = &amp;quot;split road&amp;quot;,
           start = NA, 
           end = NA)
  #bind back to the original df
  roads_2_split &amp;lt;- rbind(roads_2_split, split_roads)
}

#bind the split roads to the original roads df
roads &amp;lt;- rbind(roads, roads_2_split)

#generate paths from the nearest point on a road to the pub gecoded location
#i.e. walking from the pavement to the bar itself
pub_roads &amp;lt;- select(road_distances) %&amp;gt;%
  #add in information and bind to equivalent pub points
  mutate(name = pubs$name, start_id = pubs$id, end_id = new_nodes$id) %&amp;gt;%
  rbind(., mutate(select(pubs, name, start_id = id), end_id = new_nodes$id)) %&amp;gt;%
  #group each pub together
  group_by(name, start_id, end_id) %&amp;gt;%
  summarise() %&amp;gt;%
  #cast to a line
  st_cast(&amp;#39;LINESTRING&amp;#39;) %&amp;gt;%
  ungroup() %&amp;gt;%
  #munge required information
  mutate(id = max(roads$id)+1 + seq(nrow(.)),
         start = NA,
         end = NA,
         class = &amp;quot;pub road&amp;quot;) %&amp;gt;%
  select(class, id, start_id, end_id, start, end)

#bind these into the original road df
roads &amp;lt;- rbind(roads, pub_roads)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that I have all the pubs and roads I want to traverse, I can move onto the &lt;a href=&#34;https://en.wikipedia.org/wiki/Travelling_salesman_problem&#34;&gt;travelling salesman&lt;/a&gt; portion of the problem- what is the shortest journey between all of them.&lt;/p&gt;
&lt;p&gt;For this, I need to use the igraph package and convert my df of roads (which contains the node at each end of every road) into a weighted node graph. Once I have this, I iterate through every combination of pubs and find the shortest path between the two and the vertices that comprise it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#add in the length between each pair of nodes using st_length
nodes_df &amp;lt;- roads %&amp;gt;%
  mutate(length = as.numeric(st_length(.))) %&amp;gt;%
  #select only the node ids and length between them
  select(start_id, end_id, length)
#get rid of the geometry
st_geometry(nodes_df) = NULL

#create a node graph from this df
#uses graph.data.frame from the igraph package
node_graph &amp;lt;- graph.data.frame(nodes_df, directed=FALSE)

#to get the shortest distance between every pair of pubs
#need to create each combination of pub id number
combinations &amp;lt;- combn(1:nrow(pubs), 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The shortest path between any two pubs is found using igraph::get.shortest.paths() and then extracting the path of nodes. Each vertex of the path is then found by using pairwise combinations of the nodes, and the travelled vertices for each pub-&amp;gt;pub journey are saved into a (large) df.&lt;/p&gt;
&lt;p&gt;The whole thing is pretty quick but obviously the number of combinations grows quickly. For 103 pubs in Cambridge, it takes ~20mins on my machine.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#go through each of these pairs
journeys &amp;lt;- lapply(seq(length(combinations)/2), function(combination) {
  #the two pubs we&amp;#39;ll test
  pub1 &amp;lt;- combinations[1,][combination]
  pub2 &amp;lt;- combinations[2,][combination]
  
  #get the shortest path (node-node) between these two pubs
  travel_nodes &amp;lt;- get.shortest.paths(node_graph,
                                     from = as.character(pubs$id[pub1]),
                                     to = as.character(pubs$id[pub2]),
                                     weights = E(node_graph)$length) %&amp;gt;%
    .$vpath %&amp;gt;%
    unlist() %&amp;gt;%
    names() %&amp;gt;%
    as.numeric()
  
  #find the vertices that connect between these nodes
  connecting_vertices &amp;lt;- lapply(seq(length(travel_nodes)-1), function(node_pair) {
    between_nodes &amp;lt;- travel_nodes[c(node_pair:(node_pair+1))]
    connecting_vertex &amp;lt;- which(roads$start_id %in% between_nodes &amp;amp;
                                 roads$end_id %in% between_nodes)
    #if more than one potential vertex, take the shorter one
    if(length(connecting_vertex) &amp;gt; 1) {
      connecting_vertex &amp;lt;- connecting_vertex[
        which.min(st_length(roads[connecting_vertex,]))
      ]
    }
    
  return(connecting_vertex)
  }) %&amp;gt;%
    unlist() %&amp;gt;%
    roads[.,] %&amp;gt;%
    #id this journey between pubs
    mutate(journey = paste0(pub1, &amp;quot;-&amp;quot;, pub2))
  
  return(connecting_vertices)
}) %&amp;gt;%
  #rbind it all together
  do.call(rbind, .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s possible to check a journey between two pubs easily using this df, and show that it does seem like igraph is finding the shortest route between the two&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get only the journey between two random pubs
set.seed(3459)
x_random_numbers &amp;lt;- sample(nrow(pubs), 2) %&amp;gt;%
  sort()
x_journey &amp;lt;- journeys %&amp;gt;%
  filter(journey == paste0(x_random_numbers[1], &amp;quot;-&amp;quot;, x_random_numbers[2]))

#get the nodes of this journey
x_journey_nodes &amp;lt;- nodes %&amp;gt;%
  filter(id %in% c(as.character(x_journey$start_id),
                   as.character(x_journey$end_id)))

#find the pubs at the start and end of this journey
#random pubs as defined earlier
x_journey_pubs &amp;lt;- pubs[x_random_numbers,]

#get all roads within a kilometer of each pub
x_local_roads &amp;lt;- roads %&amp;gt;%
  .[unlist(st_intersects(
    #select all roads that at least will be between the two pubs
    st_buffer(x_journey_pubs, max(st_distance(x_journey_pubs)/1.5)
  ), .)),]

#plot the shortest path between these two pubs
(p4 &amp;lt;- ggplot() +
    geom_sf(data = x_local_roads, colour = &amp;quot;grey&amp;quot;) +
    geom_sf(data = x_journey_nodes, colour = &amp;quot;black&amp;quot;) +
    geom_sf(data = x_journey, colour = &amp;quot;black&amp;quot;) +
    geom_sf(data = x_journey_pubs, colour = &amp;quot;blue&amp;quot;, alpha = 0.5, size = 3) +
    theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/check_shortest_paths-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#remove all the extra objects we created in the example
rm(list=ls()[grep(&amp;quot;x_&amp;quot;, ls())])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I have to find the shortest combination of these journeys which visits every pub at least once. For this I find the shortest distances between each pub and every other pub and bind it into a matrix.&lt;/p&gt;
&lt;p&gt;If you just wanted to find the distance of a pub crawl (and not the path) you could just skip to here and save a lot of time.&lt;/p&gt;
&lt;p&gt;This matrix of 103x103 is then converted into TSP object using TSP::TSP() and converted to a Hamtiltonian path problem by inserting a dummy variable. The TSP is then solved given the order of nodes (in this case, just pubs) to visit.&lt;/p&gt;
&lt;p&gt;A lot of help in doing this came from the StackOverflow answer &lt;a href=&#34;https://stackoverflow.com/questions/27363653/find-shortest-path-from-x-y-coordinates-with-start-%E2%89%A0-end&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get the distances 
distances &amp;lt;- lapply(seq(nrow(pubs)), function(node) {
  distance &amp;lt;- shortest.paths(node_graph, 
                             v = as.character(pubs$id[node]),
                             to = as.character(pubs$id[seq(nrow(pubs))]), 
                             weights = E(node_graph)$length)
}) %&amp;gt;%
  do.call(rbind, .) %&amp;gt;%
  as.dist() %&amp;gt;%
  TSP()

#insert a dummy city to turn this into a Hamiltonian path question
#i.e. we do not need to return to the start at the end
tsp &amp;lt;- insert_dummy(distances, label = &amp;quot;cut&amp;quot;)

#solve the shortest Hamiltonian tour of each pub in Cambridge
#get the total distance ()
tour &amp;lt;- solve_TSP(tsp, method=&amp;quot;2-opt&amp;quot;, control=list(rep=10))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: executing %dopar% sequentially: no parallel backend registered&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tour&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## object of class &amp;#39;TOUR&amp;#39; 
## result of method &amp;#39;2-opt_rep_10&amp;#39; for 104 cities
## tour length: 41289.44&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find the order of pubs to visit in an optimal Hamtilonian path
tour &amp;lt;-  unname(cut_tour(tour, &amp;quot;cut&amp;quot;))
tour&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1]  38  76  70  14   8  49  86  32  75  63  10   9   2  41   1  27  40
##  [18]  54  20  82  90  80  72   4  61  98  83  77   5  89  47   6  60  88
##  [35]  59  46  57  11 101  78  84   7  36  65  44  71  21  39 103  18 102
##  [52]  93  19  85  62  23  16  22  29 100  42  51  50  48  64  56  79  92
##  [69]  30  28  87  96  55  35  67  69  68  91  13  17  52  94  12  97  58
##  [86]  26  73  66  25  24  31  81  37  33  53  15  74  45  43  99  34   3
## [103]  95&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So to visit every pub, you’d expect to walk just under 41km, which fits with eye-testing the size of Cambridge (approx 10km diameter).&lt;/p&gt;
&lt;p&gt;In order to plot this, the pub order is converted into strings in the format we’ve used for journeys between pubs (e.g. “1-2”) and each journey is then extracted from the df of shortest journeys between all pubs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#it might be possible that doing some journeys in &amp;#39;reverse&amp;#39; is faster
#when considering the tour of all pubs
#e.g. from pub2 -&amp;gt; pub1 rather than the other way round
rev_journeys &amp;lt;- journeys
rev_journeys$journey &amp;lt;- strsplit(journeys$journey, &amp;quot;-&amp;quot;) %&amp;gt;%
  #reverse the journey tag
  lapply(., rev) %&amp;gt;%
  lapply(., paste, collapse = &amp;quot;-&amp;quot;)

#bind these together in a df of all journeys
journeys &amp;lt;- rbind(journeys, rev_journeys) %&amp;gt;%
  mutate(journey = as.character(journey))

#take the nodes from the shortest tour and arrange them as in
#the journeys tag for each path between two pubs
tour_strings &amp;lt;- paste0(tour, &amp;quot;-&amp;quot;, lead(tour)) %&amp;gt;%
  .[-grep(&amp;quot;NA&amp;quot;, .)] %&amp;gt;%
  data.frame(journey = .,
             journey_order = 1:length(.))

#use this to find each vertex that needs traversing in order to complete
#the shortest tour of all pubs
#subset this from the df of all shortest journeys between any two pubs
shortest_tour &amp;lt;- journeys[which(journeys$journey %in% tour_strings$journey),] %&amp;gt;%
  merge(., tour_strings,  by = &amp;quot;journey&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All that’s left to do is plot this shortest pub crawl&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(p5 &amp;lt;- ggplot(data = shortest_tour) +
  geom_sf(data = cambridge) +
  geom_sf(data = roads, size = 0.5) + 
  geom_sf(aes(colour = journey_order), size = 1.5, alpha = 0.7) +
  scale_colour_gradient(high = &amp;quot;darkred&amp;quot;, low = &amp;quot;orange&amp;quot;, name = &amp;quot;journey order&amp;quot;) +
  geom_sf(data = pubs, colour = &amp;quot;blue&amp;quot;, size = 1.5) +
  theme_minimal() +
  ggtitle(&amp;quot;Hamiltonian Path of Every Pub in the City of Cambridge&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-22-cambridge_pub_crawl_files/figure-html/plot_shortest_tour-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As it turns out, you’d want to start in Trumpington at The Lord Byron Inn and then get into central Cambridge where you dart around a lot before heading out to the north and finally the east at The Red Lion in Cherry Hinton.&lt;/p&gt;
&lt;p&gt;A bigger image of the above can be found at imgure &lt;a href=&#34;https://i.imgur.com/r18SG2B.png&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All together this script calculates a distances of ~41km (give or take probably quite a bit) to visit every pub, which is actually kind of doable in a single day (if you forgo [at least some of] the drinking).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How I Learned To Stop Worrying and Love Heatmaps</title>
      <link>/post/getis-ord-heatmaps-tutorial/</link>
      <pubDate>Thu, 04 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/getis-ord-heatmaps-tutorial/</guid>
      <description>


&lt;p&gt;Whilst &lt;del&gt;getting some work done&lt;/del&gt; browsing twitter at work today, I came across &lt;a href=&#34;https://twitter.com/jburnmurdoch/status/1047470445459644416&#34;&gt;this tweet&lt;/a&gt; from the always excellent John Burn-Murdoch on the scourge of heatmaps. What’s most frustrating about these maps is that ggplot2 (which is underrated as mapping software, especially when combined with packages like sf in R) makes it super easy to create this bland, uninformative maps.&lt;/p&gt;
&lt;p&gt;For instance, lets load some mapping libraries&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(sf)
library(rgdal)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this blog I’m going to use data of bus stops in London, because there’s an absolute ton of them and because I love the London Datastore and it was the first public, heavy, point data file I came across.&lt;/p&gt;
&lt;p&gt;Let’s grab some data to use as exemplars&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#get the shapefile data of london from GADM
#downloads into a temp file
gadm_url &amp;lt;- &amp;quot;https://biogeo.ucdavis.edu/data/gadm3.6/Rsf/gadm36_GBR_2_sf.rds&amp;quot;
temp_dir &amp;lt;- tempdir()
download.file(gadm_url, destfile = file.path(temp_dir, &amp;quot;london_shapefile.rds&amp;quot;), 
              mode = &amp;quot;wb&amp;quot;, quiet = TRUE)
london &amp;lt;- sf::st_as_sf(readRDS(file.path(temp_dir, &amp;quot;london_shapefile.rds&amp;quot;))) %&amp;gt;%
  filter(grepl(&amp;quot;London&amp;quot;, NAME_2))

#get the bus stop data
#https://data.london.gov.uk/dataset/tfl-bus-stop-locations-and-routes
bus &amp;lt;- read.csv(&amp;quot;https://files.datapress.com/london/dataset/tfl-bus-stop-locations-and-routes/bus-stops-10-06-15.csv&amp;quot;,
                stringsAsFactors = FALSE) %&amp;gt;%
  filter(!is.na(Location_Easting)) %&amp;gt;%
  #convert to simple features
  st_as_sf(coords = c(&amp;quot;Location_Easting&amp;quot;, &amp;quot;Location_Northing&amp;quot;), crs = st_crs(&amp;quot;+init=epsg:27700&amp;quot;)) %&amp;gt;%
  #transform projection to match the boundary data
  st_transform(crs = st_crs(london)) %&amp;gt;%
  #remove bus stops outside of the limits of the london shapefile
  .[unlist(st_intersects(london, .)),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the data, let’s have a first pass at plotting the points on a map of London using ggplot2&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot the bus stop locations
p1 &amp;lt;- ggplot(bus) +
  geom_sf(data = london, fill = &amp;quot;grey&amp;quot;) +
  geom_sf(colour = &amp;quot;blue&amp;quot;, alpha = 0.2) +
  ggtitle(&amp;quot;Bus Stops in London&amp;quot;) +
  theme_void()

plot(p1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-04-getis-ord-tutorial_files/figure-html/plot_bus_stops-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The points here do kiiiind of work alone- it’s possible to see the route along which buses travel. However, it’s still pretty heavy and takes some cognitive effort. It’s also worth remembering this is just the first example of data I came across- if working with stuff like common incident data, even a heavy alpha on the points is going to cover the map and leave it unreadable, as in the tweet up top.&lt;/p&gt;
&lt;p&gt;But let’s say we plot it as a heatmap to try and sumamrise where the points are. In my opinion, this is even worse. For one- it stretches utside the boundaries of the shapes, even though we have filtered data to include no bus stops outside London. In this example that’s not a huge deal- but if London was (e.g.) an Island, the graph is now suggesting commutes into the sea. It’s also incredibly taxing to keep track of the various peaks and troughs- even in a simple model where bus stops generally increase as you go near to the centre&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#bind the coordinates as numeric
bus &amp;lt;- bus %&amp;gt;%
  cbind(., st_coordinates(bus))

#plot the bus stops as a density map
p2 &amp;lt;- ggplot() +
  geom_sf(data = london) +
  stat_density_2d(data = bus, aes(X, Y)) +
  ggtitle(&amp;quot;Bus Stops in London&amp;quot;) +
  theme_void()

plot(p2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-04-getis-ord-tutorial_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Instead, we can bin the point data into equi-sized containers. I’m extremely partial to this, even though it’s not super popular. To do this with hexagonal bins (the closest to circular that still tessellates perfectly), we just have to create a grid of points and connect them up&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#merge the london wards into one boundary file
london_union &amp;lt;- london %&amp;gt;%
  group_by(&amp;quot;group&amp;quot;) %&amp;gt;%
  summarise()

#generate a grid of points separated hexagonally
#no way to do this purely in sf yet afaik
hex_points &amp;lt;- spsample(as_Spatial(london_union), type = &amp;quot;hexagonal&amp;quot;, cellsize = 0.01)

#generate hexgaon polygons from these points
hex_polygons &amp;lt;- HexPoints2SpatialPolygons(hex_points) %&amp;gt;%
  st_as_sf(crs = st_crs(london_union)) %&amp;gt;%
  #clip to the london shapefile
  st_intersection(., london_union)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then find out which bin every bus stop is located in using st_intersects&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#find how many bus stops are within each hexagon
hex_polygons$planning_no &amp;lt;- lengths(st_intersects(hex_polygons, bus))

#plot the number of bus stops per bin
p2 &amp;lt;- ggplot(hex_polygons) +
  geom_sf(aes(fill = planning_no)) +
  scale_fill_viridis_c(option = &amp;quot;magma&amp;quot;, &amp;quot;# Bus Stops&amp;quot;) +
  theme_void() +
  ggtitle(&amp;quot;Binned London Bus Stops&amp;quot;)

plot(p2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-04-getis-ord-tutorial_files/figure-html/bin_stops-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, this is still pretty confusing. It seems like bus stops are fairly randomly distributed as by chance one hexagon may contain multiple stops at its edges, whereas a neighbour may be juuuust missing out on these.&lt;/p&gt;
&lt;p&gt;To mitigate this effect, we can study the spatial autocorrelation of each hexagon to it’s neighbours. There are multiple ways to do this, but the one I was first introduced to and have used most is the Getis-Ord local statistic. In this example I will include.self() which means we are using the Gi* variant of the statistic.&lt;/p&gt;
&lt;p&gt;Basically- we tell R to find all the nearest neighbours of any bin (hexagon- though not necessarily so, we could e.g. use wards, but I think it looks messier). It then calculates the ratio of values within the bin and it’s neighbours, to the total number of points (bus stops). The reported Gi* value is a z statistic- it can be positive (more clustering) or negative (less) and used to find significant clusters. I’m not going to do any of that here- just accept for now that a high Getis_Ord Gi* value means a greater cluster of bus stops in that region.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(spdep)

#find the centroid of each hexagon and convert to a matrix of points
hex_points &amp;lt;- do.call(rbind, st_geometry(st_centroid(hex_polygons))) %&amp;gt;%
  unlist() %&amp;gt;%
  as.matrix.data.frame()

#use a k-nearest-neighbour algorithm to find which shape neighbour which
#super easy for the hexagons analytically obvs but important for e.g. using the ward boundaries instead
neighbouring_hexes &amp;lt;- knn2nb(knearneigh(hex_points, k = 6), 
                             row.names = rownames(hex_points)) %&amp;gt;%
  include.self()

#calculate the local G for a given variable (#bus stops) using the neihbours found previously
localGvalues &amp;lt;- localG(x = as.numeric(hex_polygons$planning_no),
                       listw = nb2listw(neighbouring_hexes, style = &amp;quot;B&amp;quot;),
                       zero.policy = TRUE)

#bind this back to the sf as a numeric variable column
hex_polygons$smooth_planning_no &amp;lt;- as.numeric(localGvalues)

#plot the statistic
#+ve indicates more than would be expected
p3 &amp;lt;- ggplot(hex_polygons) +
  geom_sf(aes(fill = smooth_planning_no)) +
  scale_fill_viridis_c(option = &amp;quot;magma&amp;quot;, name = &amp;quot;Gi* Statistic&amp;quot;) +
  theme_void() +
  ggtitle(&amp;quot;Getis-Ord Binned London Bus Stops Statistic&amp;quot;)

plot(p3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-04-getis-ord-tutorial_files/figure-html/getis_ord-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;which generates a nice plot showing smoothed autocorrelation of dense public-transportation access. I like how you can clearly see the darker regions for the Lea Valley and Richmond Park, and in contrast the hubs of Kingston and Croydon, but in a way to is much more manageable than the contour map, or the point data itself.&lt;/p&gt;
&lt;p&gt;It’s also worth bearing in mind, that this data is fairly organised (bus routes are to some extent, logically planned). When I first looked into spatial autocorrelation I was dealing with a huge number of dense points randomly dispersed over a significant proportion of England. At that level, techniques such as the finding the Getis-Ord statistic allow you to make sense of the data, AS WELL AS statistically test it. Though I haven’t ever worked with epidemiology data, apparently it’s a powerful technique to find clusters of disease outbreaks, and indeed, the data packaged with spdep is for SIDS data in North Carolina.&lt;/p&gt;
&lt;p&gt;For more on this, I first learned about Getis-Ord from an excellent &lt;a href=&#34;https://pudding.cool/process/regional_smoothing/&#34;&gt;The Pudding&lt;/a&gt; post (from which some of my code is stolen), &lt;a href=&#34;http://pro.arcgis.com/en/pro-app/tool-reference/spatial-statistics/h-how-hot-spot-analysis-getis-ord-gi-spatial-stati.htm&#34;&gt;ARCGIS has&lt;/a&gt; a pretty good write up, and of course there is the CRAN page for the &lt;a href=&#34;https://cran.r-project.org/web/packages/spdep/index.html&#34;&gt;spdep library&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Best,&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>sf.chlorodot mini-package</title>
      <link>/post/sf.schlorodot/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/sf.schlorodot/</guid>
      <description>


&lt;p&gt;Recently, I’d seen two tweets with stunning examples of maps by Paul Campbell &lt;a href=&#34;https://twitter.com/PaulCampbell91/status/992043182996193280&#34;&gt;here&lt;/a&gt; and (taken inspiration from the first) by Imer Muhović &lt;a href=&#34;https://twitter.com/ImerM1/status/1037358973807210498&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The basic idea of the dot chloropleths is to visualise not only the location clustering of each variable but the number of observations (something traditional ‘filled’ chloropleths don’t do). More importantly than this, the maps also just look really really cool.&lt;/p&gt;
&lt;p&gt;I had a spare few minutes during work on Friday which I tidied up into a package to calculate the random position of dots for such maps which can be found &lt;a href=&#34;https://github.com/RobWHickman/sf.chlorodot&#34;&gt;on my github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Below, I’ll outline the code for the South African example used in the package README. Data comes from Adrian Frith’s &lt;a href=&#34;https://census2011.adrianfrith.com/&#34;&gt;very good 2011 census site&lt;/a&gt; and &lt;a href=&#34;https://gadm.org/download_country_v3.html&#34;&gt;gadm&lt;/a&gt; for the shapefiles.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sf)
library(ggplot2)
library(tidyverse)
library(data.table)
library(rvest)

devtools::install_github(&amp;#39;RobWHickman/sf.chlorodot&amp;#39;)
library(sf.chlordot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, download and scrape the data for the map&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#download the South African shapefile fom gadm
admin_url &amp;lt;- &amp;quot;https://biogeo.ucdavis.edu/data/gadm3.6/Rsf/gadm36_ZAF_3_sf.rds&amp;quot;
download.file(admin_url, destfile = &amp;quot;shapefiles.rds&amp;quot;, mode = &amp;quot;wb&amp;quot;)
south_africa &amp;lt;- readRDS(&amp;quot;shapefiles.rds&amp;quot;) %&amp;gt;%
  #convert to sf
  st_as_sf() %&amp;gt;%
  select(region = NAME_3) %&amp;gt;%
  #merge geometries that have two rows
  group_by(region) %&amp;gt;%
  summarise()

#get the links to the data from Adrian Frith&amp;#39;s site
sa_data_url &amp;lt;- &amp;quot;https://census2011.adrianfrith.com&amp;quot;
south_africa_data &amp;lt;- sa_data_url %&amp;gt;%
  read_html() %&amp;gt;% html_nodes(&amp;quot;.namecell a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% paste0(sa_data_url, .) %&amp;gt;%
  lapply(., function(x) read_html(x) %&amp;gt;% html_nodes(&amp;quot;.namecell a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% paste0(sa_data_url, .)) %&amp;gt;% unlist() %&amp;gt;%
   lapply(., function(x) read_html(x) %&amp;gt;% html_nodes(&amp;quot;.namecell a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% paste0(sa_data_url, .)) %&amp;gt;% unlist()

#scrape the data on primary language from the 2011 South African census
language_data &amp;lt;- rbindlist(lapply(south_africa_data, function(x) {
  read &amp;lt;- read_html(x)
  language_nos &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;.datacell&amp;quot;) %&amp;gt;% html_text()
  start &amp;lt;- grep(&amp;quot;Percentage&amp;quot;, language_nos)[3] + 1
  stop &amp;lt;- grep(&amp;quot;Population&amp;quot;, language_nos) - 1
  #some areas have no data
  if(!is.na(start) &amp;amp; !is.na(stop)) {
    language_nos &amp;lt;- language_nos[start:stop]
    language_nos &amp;lt;- language_nos[seq(1, length(language_nos), 2)]
  } else {
    language_nos &amp;lt;- NA
  }
  
  languages &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;tr &amp;gt; :nth-child(1)&amp;quot;) %&amp;gt;% html_text()
  start &amp;lt;- grep(&amp;quot;First language&amp;quot;, languages) + 1
  stop &amp;lt;- grep(&amp;quot;Name&amp;quot;, languages) - 1
  if(length(start) &amp;gt; 0 &amp;amp; !is.na(stop)) {
    languages &amp;lt;- languages[start:stop]
  } else {
    languages &amp;lt;- NA
  }
  
  region_names &amp;lt;- read %&amp;gt;% html_nodes(&amp;quot;.topname&amp;quot;) %&amp;gt;% html_text()
  
  #combine into a df
  df &amp;lt;- data.frame(language = languages, primary_speakers = language_nos, region = region_names)
  return(df)
}))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the lanaguage data then needs to be transformed before the dot position is calculated. It must be in ‘short’ format with variables as column names. At the same time we can do some cleaning in order to match the shape areas with the region names from the census and remove data we don’t want to plot&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;language_data %&amp;lt;&amp;gt;%
  #convert number of speakers to numeric
  mutate(primary_speakers = as.numeric(as.character(primary_speakers))) %&amp;gt;%
  #matching of area names with South African shapefile
  mutate(region = gsub(&amp;quot; NU&amp;quot;, &amp;quot;&amp;quot;, region)) %&amp;gt;%
  mutate(region = gsub(&amp;quot;Tshwane&amp;quot;, &amp;quot;City of Tshwane&amp;quot;, region)) %&amp;gt;%
  #filter only the data we want to merge
  filter(region %in% south_africa$region) %&amp;gt;%
  filter(!is.na(language)) %&amp;gt;%
  filter(language != &amp;quot;Not applicable&amp;quot;) %&amp;gt;%
  #spread the data
  dcast(., region ~ language, value.var = &amp;quot;primary_speakers&amp;quot;, fun.aggregate = sum) %&amp;gt;%
  #join in the spatial geometry
  left_join(., south_africa) %&amp;gt;%
  #convert to sf
  st_as_sf()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then we can calculate the random dot position using calc_dots() from the sf.chlorodot package. This takes three arguments. The first is the df to take the data from (language_data). The second is which variables to calculate positions for. The easiest way to do this is to use names(df) and select from there, though a character vector can also be passed. Finally, n_per_dot is the number of observations (speakers of language x) for each dot on the map. This will affect the look of the map, but also the processing time (lower n_per_dot = greater time) so play around with it a bit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#calculate the dot positions using calc_dots from the sf.chlorodot package
sf_dots &amp;lt;- calc_dots(df = language_data, col_names = names(language_data)[2:14], n_per_dot = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can plot the output of this&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#stolen the background colour scheme from Paul Campbell&amp;#39;s blog
#original inspiration for this package
p &amp;lt;- ggplot() +
  geom_sf(data = south_africa, fill = &amp;quot;transparent&amp;quot;,colour = &amp;quot;white&amp;quot;) +
  geom_point(data = sf_dots, aes(lon, lat, colour = variable), size = 0.1) +
  scale_colour_discrete(name = &amp;quot;Primary Language&amp;quot;) +
  ggtitle(&amp;quot;Language Diversity in South Africa&amp;quot;) +
  theme_void() +
  guides(colour = guide_legend(override.aes = list(size = 10))) +
  theme(plot.background = element_rect(fill = &amp;quot;#212121&amp;quot;, color = NA), 
        panel.background = element_rect(fill = &amp;quot;#212121&amp;quot;, color = NA),
        legend.background = element_rect(fill = &amp;quot;#212121&amp;quot;, color = NA),
        text =  element_text(color = &amp;quot;white&amp;quot;),
        title =  element_text(color = &amp;quot;white&amp;quot;),
        legend.text=element_text(size=12),
        plot.title = element_text(size = 20),
        plot.margin = margin(1, 1, 1, 1, &amp;quot;cm&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../img/south_africa.png&#34; alt=&#34;chlorodot map of South African languages&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Enjoy!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
